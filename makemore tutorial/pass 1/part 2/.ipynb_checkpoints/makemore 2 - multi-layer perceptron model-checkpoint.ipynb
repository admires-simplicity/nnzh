{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552aa5a8-1c05-4ab5-9653-656cb4c89428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name generation model\n",
    "# based on this paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f694b08d-9534-4b04-ab57-5ad37b89c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e918d40a-6dd5-4d61-a6f5-ea7d05a999df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read  in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe49c448-a633-4076-9353-5d244ea8a0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b384bf-c57e-4dce-ae24-6c9f51f7b0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5004cf44-226d-4c23-a423-280db68ee8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf87248-14c8-4fc2-ac4e-d46d3c345409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c3003e-1b6c-40db-a50c-438b6bfd9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9904fee-f22b-4395-8502-074bb204d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed input in lower dimensional space\n",
    "# original paper embeds 17,000 words in 30 dimensional space\n",
    "# we have 27 possible input characters. let's try a 2 dimensional space.\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab79473-5ba9-49f4-885d-dce3acfae1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1804,  0.6492],\n",
       "        [ 0.2129, -1.3559],\n",
       "        [ 0.6829,  1.0071]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7])] # using list or tensor as index instead of number gives us a tensor of the respective values in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a294cf2-6958-42ed-b7ec-98fb78f11ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [-0.1804,  0.6492]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [-0.1804,  0.6492],\n",
       "         [-1.1548, -2.0889]],\n",
       "\n",
       "        [[-0.1804,  0.6492],\n",
       "         [-1.1548, -2.0889],\n",
       "         [-1.1548, -2.0889]],\n",
       "\n",
       "        [[-1.1548, -2.0889],\n",
       "         [-1.1548, -2.0889],\n",
       "         [-0.2512, -0.3175]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [ 1.4300,  1.6110]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 1.4300,  1.6110],\n",
       "         [ 0.2020, -1.1088]],\n",
       "\n",
       "        [[ 1.4300,  1.6110],\n",
       "         [ 0.2020, -1.1088],\n",
       "         [-0.0381,  0.9655]],\n",
       "\n",
       "        [[ 0.2020, -1.1088],\n",
       "         [-0.0381,  0.9655],\n",
       "         [ 0.7182, -0.2430]],\n",
       "\n",
       "        [[-0.0381,  0.9655],\n",
       "         [ 0.7182, -0.2430],\n",
       "         [-0.0381,  0.9655]],\n",
       "\n",
       "        [[ 0.7182, -0.2430],\n",
       "         [-0.0381,  0.9655],\n",
       "         [-0.2512, -0.3175]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [-0.2512, -0.3175]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [-0.2512, -0.3175],\n",
       "         [ 0.7182, -0.2430]],\n",
       "\n",
       "        [[-0.2512, -0.3175],\n",
       "         [ 0.7182, -0.2430],\n",
       "         [-0.2512, -0.3175]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [-0.0381,  0.9655]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [-0.0381,  0.9655],\n",
       "         [ 1.5629, -0.8671]],\n",
       "\n",
       "        [[-0.0381,  0.9655],\n",
       "         [ 1.5629, -0.8671],\n",
       "         [-0.2512, -0.3175]],\n",
       "\n",
       "        [[ 1.5629, -0.8671],\n",
       "         [-0.2512, -0.3175],\n",
       "         [ 0.6707, -0.3115]],\n",
       "\n",
       "        [[-0.2512, -0.3175],\n",
       "         [ 0.6707, -0.3115],\n",
       "         [-0.1804,  0.6492]],\n",
       "\n",
       "        [[ 0.6707, -0.3115],\n",
       "         [-0.1804,  0.6492],\n",
       "         [ 0.2020, -1.1088]],\n",
       "\n",
       "        [[-0.1804,  0.6492],\n",
       "         [ 0.2020, -1.1088],\n",
       "         [ 0.2020, -1.1088]],\n",
       "\n",
       "        [[ 0.2020, -1.1088],\n",
       "         [ 0.2020, -1.1088],\n",
       "         [-0.2512, -0.3175]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 2.1902,  0.7492],\n",
       "         [ 1.5629, -0.8671]],\n",
       "\n",
       "        [[ 2.1902,  0.7492],\n",
       "         [ 1.5629, -0.8671],\n",
       "         [ 1.4300,  1.6110]],\n",
       "\n",
       "        [[ 1.5629, -0.8671],\n",
       "         [ 1.4300,  1.6110],\n",
       "         [ 2.5534, -0.8210]],\n",
       "\n",
       "        [[ 1.4300,  1.6110],\n",
       "         [ 2.5534, -0.8210],\n",
       "         [ 1.6473,  0.0947]],\n",
       "\n",
       "        [[ 2.5534, -0.8210],\n",
       "         [ 1.6473,  0.0947],\n",
       "         [-0.0381,  0.9655]],\n",
       "\n",
       "        [[ 1.6473,  0.0947],\n",
       "         [-0.0381,  0.9655],\n",
       "         [-0.2512, -0.3175]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # we can also index with multidimensional integers\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fbd16b4-0eca-492f-844c-1184a67e2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76af0273-b13a-4991-ac7f-4f86f62fa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "W1 = torch.randn((6, 100)) # weights\n",
    "# 6 x 100 because:\n",
    "# (number of inputs == 6 == embedding dimensions (2) x n-embeddings per input (3))\n",
    "#   x \n",
    "# (number of neurons in this layer == some arbitrary amount of neurons (100))\n",
    "b1 = torch.randn(100) # biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad715e39-e0bb-4802-8e11-e8bd74fcea65",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25973/1082888545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;31m# what we want to do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1 + b1 # what we want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf27005-cc6d-4d0b-8341-cbb6800957a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but it doesn't work because each element in emb has size (3x2) instead of size 6.\n",
    "# we need to smush these together somehow.\n",
    "# there are multiple ways to acheive this depending on exact requirements\n",
    "# we'll use torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4244c02-0362-4a0b-87a2-fe6741ede611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.1804,  0.6492],\n",
       "        [ 2.1902,  0.7492, -0.1804,  0.6492, -1.1548, -2.0889],\n",
       "        [-0.1804,  0.6492, -1.1548, -2.0889, -1.1548, -2.0889],\n",
       "        [-1.1548, -2.0889, -1.1548, -2.0889, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  1.4300,  1.6110],\n",
       "        [ 2.1902,  0.7492,  1.4300,  1.6110,  0.2020, -1.1088],\n",
       "        [ 1.4300,  1.6110,  0.2020, -1.1088, -0.0381,  0.9655],\n",
       "        [ 0.2020, -1.1088, -0.0381,  0.9655,  0.7182, -0.2430],\n",
       "        [-0.0381,  0.9655,  0.7182, -0.2430, -0.0381,  0.9655],\n",
       "        [ 0.7182, -0.2430, -0.0381,  0.9655, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492, -0.2512, -0.3175,  0.7182, -0.2430],\n",
       "        [-0.2512, -0.3175,  0.7182, -0.2430, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.0381,  0.9655],\n",
       "        [ 2.1902,  0.7492, -0.0381,  0.9655,  1.5629, -0.8671],\n",
       "        [-0.0381,  0.9655,  1.5629, -0.8671, -0.2512, -0.3175],\n",
       "        [ 1.5629, -0.8671, -0.2512, -0.3175,  0.6707, -0.3115],\n",
       "        [-0.2512, -0.3175,  0.6707, -0.3115, -0.1804,  0.6492],\n",
       "        [ 0.6707, -0.3115, -0.1804,  0.6492,  0.2020, -1.1088],\n",
       "        [-0.1804,  0.6492,  0.2020, -1.1088,  0.2020, -1.1088],\n",
       "        [ 0.2020, -1.1088,  0.2020, -1.1088, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  1.5629, -0.8671],\n",
       "        [ 2.1902,  0.7492,  1.5629, -0.8671,  1.4300,  1.6110],\n",
       "        [ 1.5629, -0.8671,  1.4300,  1.6110,  2.5534, -0.8210],\n",
       "        [ 1.4300,  1.6110,  2.5534, -0.8210,  1.6473,  0.0947],\n",
       "        [ 2.5534, -0.8210,  1.6473,  0.0947, -0.0381,  0.9655],\n",
       "        [ 1.6473,  0.0947, -0.0381,  0.9655, -0.2512, -0.3175]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
    "# this will do it, but it's ugly because it's hard-coded.\n",
    "# what if we wanted to change the dimensions of emb from e.g. (_, 3, _) to (_, M, _) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8297fa8-0e83-46db-b304-72030fe44e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.1804,  0.6492],\n",
       "        [ 2.1902,  0.7492, -0.1804,  0.6492, -1.1548, -2.0889],\n",
       "        [-0.1804,  0.6492, -1.1548, -2.0889, -1.1548, -2.0889],\n",
       "        [-1.1548, -2.0889, -1.1548, -2.0889, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  1.4300,  1.6110],\n",
       "        [ 2.1902,  0.7492,  1.4300,  1.6110,  0.2020, -1.1088],\n",
       "        [ 1.4300,  1.6110,  0.2020, -1.1088, -0.0381,  0.9655],\n",
       "        [ 0.2020, -1.1088, -0.0381,  0.9655,  0.7182, -0.2430],\n",
       "        [-0.0381,  0.9655,  0.7182, -0.2430, -0.0381,  0.9655],\n",
       "        [ 0.7182, -0.2430, -0.0381,  0.9655, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492, -0.2512, -0.3175,  0.7182, -0.2430],\n",
       "        [-0.2512, -0.3175,  0.7182, -0.2430, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.0381,  0.9655],\n",
       "        [ 2.1902,  0.7492, -0.0381,  0.9655,  1.5629, -0.8671],\n",
       "        [-0.0381,  0.9655,  1.5629, -0.8671, -0.2512, -0.3175],\n",
       "        [ 1.5629, -0.8671, -0.2512, -0.3175,  0.6707, -0.3115],\n",
       "        [-0.2512, -0.3175,  0.6707, -0.3115, -0.1804,  0.6492],\n",
       "        [ 0.6707, -0.3115, -0.1804,  0.6492,  0.2020, -1.1088],\n",
       "        [-0.1804,  0.6492,  0.2020, -1.1088,  0.2020, -1.1088],\n",
       "        [ 0.2020, -1.1088,  0.2020, -1.1088, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  1.5629, -0.8671],\n",
       "        [ 2.1902,  0.7492,  1.5629, -0.8671,  1.4300,  1.6110],\n",
       "        [ 1.5629, -0.8671,  1.4300,  1.6110,  2.5534, -0.8210],\n",
       "        [ 1.4300,  1.6110,  2.5534, -0.8210,  1.6473,  0.0947],\n",
       "        [ 2.5534, -0.8210,  1.6473,  0.0947, -0.0381,  0.9655],\n",
       "        [ 1.6473,  0.0947, -0.0381,  0.9655, -0.2512, -0.3175]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ans: use torch.unbind\n",
    "#torch.unbind(emb, 1) # == [emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]]\n",
    "torch.cat(torch.unbind(emb, 1), 1)\n",
    "# unfortunately, this is SUPA INEFFICIENT, because torch.cat will copy everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774e0659-255f-4c07-a471-0c1166417e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.1804,  0.6492],\n",
       "        [ 2.1902,  0.7492, -0.1804,  0.6492, -1.1548, -2.0889],\n",
       "        [-0.1804,  0.6492, -1.1548, -2.0889, -1.1548, -2.0889],\n",
       "        [-1.1548, -2.0889, -1.1548, -2.0889, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  1.4300,  1.6110],\n",
       "        [ 2.1902,  0.7492,  1.4300,  1.6110,  0.2020, -1.1088],\n",
       "        [ 1.4300,  1.6110,  0.2020, -1.1088, -0.0381,  0.9655],\n",
       "        [ 0.2020, -1.1088, -0.0381,  0.9655,  0.7182, -0.2430],\n",
       "        [-0.0381,  0.9655,  0.7182, -0.2430, -0.0381,  0.9655],\n",
       "        [ 0.7182, -0.2430, -0.0381,  0.9655, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492, -0.2512, -0.3175,  0.7182, -0.2430],\n",
       "        [-0.2512, -0.3175,  0.7182, -0.2430, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492, -0.0381,  0.9655],\n",
       "        [ 2.1902,  0.7492, -0.0381,  0.9655,  1.5629, -0.8671],\n",
       "        [-0.0381,  0.9655,  1.5629, -0.8671, -0.2512, -0.3175],\n",
       "        [ 1.5629, -0.8671, -0.2512, -0.3175,  0.6707, -0.3115],\n",
       "        [-0.2512, -0.3175,  0.6707, -0.3115, -0.1804,  0.6492],\n",
       "        [ 0.6707, -0.3115, -0.1804,  0.6492,  0.2020, -1.1088],\n",
       "        [-0.1804,  0.6492,  0.2020, -1.1088,  0.2020, -1.1088],\n",
       "        [ 0.2020, -1.1088,  0.2020, -1.1088, -0.2512, -0.3175],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  2.1902,  0.7492],\n",
       "        [ 2.1902,  0.7492,  2.1902,  0.7492,  1.5629, -0.8671],\n",
       "        [ 2.1902,  0.7492,  1.5629, -0.8671,  1.4300,  1.6110],\n",
       "        [ 1.5629, -0.8671,  1.4300,  1.6110,  2.5534, -0.8210],\n",
       "        [ 1.4300,  1.6110,  2.5534, -0.8210,  1.6473,  0.0947],\n",
       "        [ 2.5534, -0.8210,  1.6473,  0.0947, -0.0381,  0.9655],\n",
       "        [ 1.6473,  0.0947, -0.0381,  0.9655, -0.2512, -0.3175]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# even BETTER + EFFICIENT ans: use tensor.view!\n",
    "# changes how tensor is INDEXED instead of STORED. efficient!\n",
    "# changes storage offset, strides, and shapes\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e162c283-a869-4ea7-9f80-c76bae5755c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1)\n",
    "# activations of inputs emb (h for \"hidden states\")\n",
    "# could do emb.view(-1, 6) and pytorch would figure out what the first dimension needs to be in order to make the size work\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddb4a76e-31d6-4bda-baed-82542573ba08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2271, -0.9872,  0.9992,  ...,  0.9424,  0.6495,  0.9120],\n",
       "        [ 0.9052, -0.9908,  0.9565,  ..., -0.5805, -0.2713,  0.9988],\n",
       "        [ 0.9987,  0.8139, -0.9620,  ...,  0.9704,  0.9011,  0.9859],\n",
       "        ...,\n",
       "        [-0.9223, -0.9523,  1.0000,  ...,  0.9634,  0.4371,  0.9947],\n",
       "        [ 0.9985, -0.9965,  0.0660,  ..., -0.8663, -0.9834,  0.9957],\n",
       "        [ 0.9809, -0.1779, -0.8992,  ...,  0.3267,  0.3806,  0.5076]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fefe14e3-9712-433c-990c-44dbc901d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make sure the expression \"emb.view(emb.shape[0], 6) @ W1 + b1\"\n",
    "# is broadcasting correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c134812-2567-4e1d-a324-ac789d7b2ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(emb.shape[0], 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d2e6050-2795-418d-a55c-5fdd19e33c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76ff1e7b-ca4e-428d-b876-559dc8e240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align on right, make missing dimensions size 1, then copy all dimensions of size 1\n",
    "# 32 100    32 100    32 100\n",
    "#    100 ->  1 100 -> 32 100\n",
    "# so it's correct (note: in case this looks wrong because we know \n",
    "#                        matrix multiplication needs (N, M) x (M, O) dimensions,\n",
    "#                        remember we're not doing a multiplication on these\n",
    "#                        two values, we're doing an addition, so we actually\n",
    "#                        do want identical sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8512fe1c-42bb-4274-8962-83ca09ec6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer\n",
    "W2 = torch.randn((100, 27)) # input size -> 27 characters\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5428ae0-f5ed-48c5-8b2a-cd5d1805ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd1f5f4a-3f12-4fa0-9b08-c6f3c0b298e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a158efa-6a0f-4ba6-a4be-5ffee43f5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c61aff17-6da2-4c86-8e05-c6129bb05068",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d79288c9-6da3-4e77-ad83-f60f716986e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70f85842-098b-47a0-8fb5-144857217217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65748a42-446c-4dec-af52-5fab6b9469eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1226e-19, 2.4630e-11, 1.1134e-09, 2.0260e-11, 1.2618e-05, 3.6701e-08,\n",
       "         9.9933e-01, 4.9256e-06, 9.2675e-06, 2.0936e-14, 6.9778e-05, 7.0656e-06,\n",
       "         1.4782e-09, 7.3348e-06, 3.2403e-08, 3.9221e-13, 5.6213e-16, 1.9290e-10,\n",
       "         6.1216e-14, 1.7845e-05, 5.5750e-09, 4.0090e-06, 2.8528e-15, 1.3739e-08,\n",
       "         5.4155e-04, 3.4953e-10, 1.2426e-08],\n",
       "        [4.3649e-12, 2.0196e-06, 1.0765e-05, 1.4215e-08, 8.2898e-08, 1.6428e-04,\n",
       "         8.5881e-01, 1.7386e-07, 8.1458e-05, 4.1131e-03, 1.5571e-07, 3.5307e-05,\n",
       "         8.9014e-06, 5.8001e-02, 3.4546e-02, 2.9718e-12, 2.3471e-14, 1.2855e-04,\n",
       "         5.0861e-13, 3.6915e-02, 1.4087e-06, 7.1624e-03, 7.0508e-16, 2.8332e-09,\n",
       "         5.3432e-07, 4.0080e-07, 1.3309e-05],\n",
       "        [7.4983e-10, 1.4923e-12, 4.5945e-07, 1.7290e-08, 5.4277e-05, 7.4846e-10,\n",
       "         3.2063e-08, 4.8525e-13, 1.4732e-07, 6.2930e-06, 9.9993e-01, 5.6163e-06,\n",
       "         1.2280e-12, 5.4515e-12, 6.1430e-07, 5.3902e-15, 3.1269e-12, 9.4441e-11,\n",
       "         1.7918e-09, 7.6165e-16, 1.4422e-16, 5.0913e-12, 4.9665e-09, 1.4864e-09,\n",
       "         3.7497e-15, 3.5242e-07, 1.4940e-09],\n",
       "        [1.8950e-06, 1.0190e-15, 5.9192e-14, 1.7677e-10, 2.6651e-03, 9.2895e-09,\n",
       "         2.0008e-11, 2.6291e-15, 4.2082e-12, 1.9239e-03, 9.9533e-01, 6.8488e-14,\n",
       "         2.0662e-07, 7.3914e-14, 4.2770e-15, 4.8361e-11, 4.8829e-12, 1.5454e-12,\n",
       "         1.9852e-06, 5.7636e-15, 1.7872e-12, 5.2675e-12, 1.4522e-05, 1.6550e-14,\n",
       "         2.9159e-09, 5.4155e-07, 6.6467e-05],\n",
       "        [1.7326e-03, 2.2592e-07, 4.2561e-09, 1.3785e-07, 6.3856e-11, 6.2844e-08,\n",
       "         2.4957e-11, 6.9562e-14, 4.7968e-13, 8.7454e-07, 1.7878e-08, 1.0347e-17,\n",
       "         2.2023e-04, 1.6956e-09, 1.2316e-14, 2.7371e-12, 8.5558e-01, 1.0712e-06,\n",
       "         1.2952e-01, 7.6899e-11, 1.2609e-07, 1.6993e-08, 1.2915e-02, 5.2705e-12,\n",
       "         6.5507e-08, 1.2995e-06, 2.6689e-05],\n",
       "        [1.1226e-19, 2.4630e-11, 1.1134e-09, 2.0260e-11, 1.2618e-05, 3.6701e-08,\n",
       "         9.9933e-01, 4.9256e-06, 9.2675e-06, 2.0936e-14, 6.9778e-05, 7.0656e-06,\n",
       "         1.4782e-09, 7.3348e-06, 3.2403e-08, 3.9221e-13, 5.6213e-16, 1.9290e-10,\n",
       "         6.1216e-14, 1.7845e-05, 5.5750e-09, 4.0090e-06, 2.8528e-15, 1.3739e-08,\n",
       "         5.4155e-04, 3.4953e-10, 1.2426e-08],\n",
       "        [1.0924e-13, 3.3205e-07, 1.2637e-07, 3.2936e-08, 3.5487e-04, 2.3919e-08,\n",
       "         2.2568e-01, 6.1609e-05, 1.3303e-05, 9.8892e-11, 7.0284e-05, 2.1543e-05,\n",
       "         3.9469e-05, 2.5303e-02, 1.3969e-03, 5.5429e-12, 6.9509e-09, 4.9148e-08,\n",
       "         1.6505e-13, 7.4474e-01, 3.7448e-07, 6.8220e-04, 1.5609e-12, 3.5829e-06,\n",
       "         1.0852e-03, 7.6442e-09, 5.4365e-04],\n",
       "        [3.2740e-17, 2.6578e-10, 7.8736e-08, 9.3728e-10, 7.9170e-10, 1.9099e-10,\n",
       "         9.7612e-01, 3.2809e-05, 1.6210e-03, 2.9331e-08, 4.4692e-04, 2.1773e-02,\n",
       "         8.8216e-14, 4.1721e-07, 1.7824e-07, 1.0404e-14, 5.1986e-19, 2.4684e-11,\n",
       "         1.3862e-12, 3.9676e-06, 1.8723e-12, 8.6905e-10, 3.8910e-17, 7.7678e-09,\n",
       "         5.2656e-14, 1.7527e-09, 1.4700e-09],\n",
       "        [1.6644e-06, 1.1581e-07, 6.5383e-07, 1.9901e-08, 1.6872e-07, 2.0307e-09,\n",
       "         4.8684e-04, 1.2973e-06, 5.4650e-15, 9.5918e-01, 3.8498e-09, 3.7870e-08,\n",
       "         3.9115e-02, 8.5083e-12, 1.1227e-07, 1.7444e-09, 5.4742e-06, 3.6838e-04,\n",
       "         1.0680e-09, 2.3458e-10, 9.1631e-07, 1.0624e-10, 1.0012e-06, 6.3564e-14,\n",
       "         8.3858e-04, 2.4053e-07, 2.4048e-06],\n",
       "        [3.6291e-10, 4.0681e-09, 1.1352e-07, 7.5637e-10, 1.3964e-10, 1.0525e-05,\n",
       "         2.3117e-02, 1.2343e-05, 9.2005e-01, 1.7265e-12, 4.8019e-06, 2.7806e-10,\n",
       "         1.3647e-10, 5.3125e-02, 1.1267e-09, 1.3496e-09, 6.6855e-15, 1.2123e-10,\n",
       "         6.3112e-08, 3.6373e-03, 7.0897e-12, 8.1639e-06, 4.2290e-12, 1.4151e-05,\n",
       "         8.5241e-11, 2.2445e-05, 4.9609e-10],\n",
       "        [1.3260e-07, 4.0934e-05, 1.2691e-01, 9.8872e-09, 8.1827e-12, 2.7143e-08,\n",
       "         5.9000e-05, 5.1394e-06, 5.1536e-06, 5.2594e-01, 3.8790e-09, 1.0915e-04,\n",
       "         4.4060e-02, 3.1964e-06, 8.5782e-06, 3.7166e-06, 3.9417e-05, 1.9495e-04,\n",
       "         1.4373e-06, 9.7797e-07, 3.0255e-01, 1.0843e-08, 1.2273e-10, 7.1280e-14,\n",
       "         2.0314e-06, 6.6655e-05, 2.9801e-07],\n",
       "        [6.5928e-08, 1.1109e-08, 2.7927e-03, 2.1745e-08, 8.6049e-09, 5.0729e-07,\n",
       "         1.4578e-01, 4.0922e-08, 8.4872e-01, 5.2322e-04, 2.5126e-04, 1.5649e-04,\n",
       "         2.9133e-06, 4.9342e-04, 6.9382e-05, 9.2927e-12, 4.3979e-13, 4.9925e-10,\n",
       "         1.0473e-07, 8.6130e-06, 2.8112e-13, 2.0192e-06, 4.8285e-11, 1.8717e-06,\n",
       "         6.0994e-13, 1.1970e-03, 2.7576e-11],\n",
       "        [1.1226e-19, 2.4630e-11, 1.1134e-09, 2.0260e-11, 1.2618e-05, 3.6701e-08,\n",
       "         9.9933e-01, 4.9256e-06, 9.2675e-06, 2.0936e-14, 6.9778e-05, 7.0656e-06,\n",
       "         1.4782e-09, 7.3348e-06, 3.2403e-08, 3.9221e-13, 5.6213e-16, 1.9290e-10,\n",
       "         6.1216e-14, 1.7845e-05, 5.5750e-09, 4.0090e-06, 2.8528e-15, 1.3739e-08,\n",
       "         5.4155e-04, 3.4953e-10, 1.2426e-08],\n",
       "        [2.2942e-15, 7.5089e-08, 4.5471e-07, 2.7373e-07, 3.3639e-09, 1.1108e-06,\n",
       "         8.9871e-01, 1.2209e-05, 1.0893e-05, 9.3215e-04, 4.6440e-05, 2.3690e-03,\n",
       "         2.6896e-09, 3.9588e-05, 1.7262e-03, 1.0675e-12, 1.1706e-18, 1.9091e-07,\n",
       "         3.7013e-13, 9.6130e-02, 1.5068e-08, 1.6702e-06, 1.4998e-16, 5.6948e-09,\n",
       "         1.5736e-11, 2.6627e-08, 1.4811e-05],\n",
       "        [3.8968e-11, 1.5055e-08, 2.2962e-08, 1.6482e-07, 8.9816e-03, 1.0309e-09,\n",
       "         4.3912e-03, 3.0813e-07, 4.3007e-11, 9.5911e-06, 9.8660e-01, 1.4079e-06,\n",
       "         1.8534e-07, 3.4638e-10, 6.4138e-07, 4.8437e-15, 5.1820e-10, 1.7123e-08,\n",
       "         2.3323e-11, 8.3843e-10, 1.2096e-12, 5.4341e-15, 8.7295e-09, 1.0343e-05,\n",
       "         5.0015e-06, 1.6129e-09, 1.5654e-07],\n",
       "        [8.8236e-08, 2.4330e-08, 5.8500e-04, 1.3228e-06, 7.3084e-06, 2.1396e-01,\n",
       "         1.6411e-01, 2.2005e-09, 4.8218e-01, 1.5294e-03, 1.3846e-02, 5.6609e-11,\n",
       "         5.6755e-05, 4.1932e-04, 4.4974e-09, 2.3668e-07, 6.5915e-10, 1.1849e-05,\n",
       "         1.4647e-03, 7.2113e-05, 1.0917e-01, 1.1741e-02, 1.1553e-06, 1.2127e-12,\n",
       "         9.2832e-07, 2.3045e-04, 6.0861e-04],\n",
       "        [1.1226e-19, 2.4630e-11, 1.1134e-09, 2.0260e-11, 1.2618e-05, 3.6701e-08,\n",
       "         9.9933e-01, 4.9256e-06, 9.2675e-06, 2.0936e-14, 6.9778e-05, 7.0656e-06,\n",
       "         1.4782e-09, 7.3348e-06, 3.2403e-08, 3.9221e-13, 5.6213e-16, 1.9290e-10,\n",
       "         6.1216e-14, 1.7845e-05, 5.5750e-09, 4.0090e-06, 2.8528e-15, 1.3739e-08,\n",
       "         5.4155e-04, 3.4953e-10, 1.2426e-08],\n",
       "        [4.0635e-11, 3.3675e-06, 9.3387e-06, 2.0904e-08, 3.6661e-07, 1.7690e-04,\n",
       "         7.3764e-01, 1.6237e-07, 1.3238e-04, 1.1980e-03, 1.7139e-08, 1.4828e-05,\n",
       "         1.4541e-04, 1.1597e-01, 2.1107e-02, 2.0690e-12, 8.8486e-12, 2.0156e-04,\n",
       "         1.2071e-13, 4.9955e-02, 3.5494e-06, 7.3421e-02, 3.3440e-15, 5.3947e-09,\n",
       "         4.7474e-06, 3.9065e-07, 1.5431e-05],\n",
       "        [4.0344e-18, 3.0557e-14, 2.2756e-11, 8.8251e-12, 3.8853e-07, 1.4751e-11,\n",
       "         1.0749e-03, 4.3817e-06, 7.6464e-08, 6.5359e-11, 9.9876e-01, 1.6013e-04,\n",
       "         1.4030e-10, 2.3056e-13, 8.6397e-13, 7.4162e-14, 1.1379e-18, 2.8569e-13,\n",
       "         1.9157e-15, 5.0907e-12, 8.5966e-17, 2.0245e-17, 1.1552e-13, 3.5873e-09,\n",
       "         1.1367e-12, 2.0435e-12, 1.4351e-10],\n",
       "        [1.2923e-10, 5.5704e-07, 1.8796e-06, 3.4647e-06, 2.5012e-04, 8.2606e-05,\n",
       "         2.6096e-02, 3.2317e-06, 2.0012e-03, 1.4763e-03, 9.5784e-01, 1.2350e-04,\n",
       "         5.9791e-03, 3.0208e-08, 7.0530e-06, 1.8462e-07, 1.7784e-10, 2.9960e-08,\n",
       "         4.4500e-04, 6.3749e-08, 5.6005e-03, 8.1437e-07, 5.1428e-10, 3.1730e-13,\n",
       "         1.6815e-05, 6.9757e-05, 1.2674e-06],\n",
       "        [1.7326e-05, 1.8518e-05, 5.0768e-07, 5.0181e-04, 7.7457e-01, 4.8606e-03,\n",
       "         1.0177e-01, 1.4919e-06, 4.1404e-06, 6.2747e-08, 4.9769e-02, 6.5081e-11,\n",
       "         6.0597e-06, 4.1506e-05, 5.3617e-10, 2.1114e-14, 2.9426e-04, 4.6768e-02,\n",
       "         1.5779e-07, 1.2696e-02, 8.7196e-11, 5.8833e-08, 2.3254e-03, 2.0315e-05,\n",
       "         9.4321e-05, 1.8190e-07, 6.2410e-03],\n",
       "        [1.8001e-05, 2.0168e-05, 4.5332e-03, 1.2805e-09, 2.2299e-12, 1.8799e-05,\n",
       "         6.1620e-06, 3.5973e-09, 1.3954e-04, 1.2116e-05, 1.4540e-09, 3.0494e-12,\n",
       "         2.7263e-03, 1.8517e-03, 9.4625e-08, 8.1434e-08, 1.4877e-05, 1.5853e-03,\n",
       "         2.9831e-06, 2.9322e-03, 9.8541e-01, 7.1862e-04, 2.0902e-09, 5.8035e-14,\n",
       "         1.2573e-06, 1.3098e-06, 2.6732e-06],\n",
       "        [6.0689e-09, 2.5327e-12, 1.6187e-07, 2.1645e-08, 4.8291e-03, 5.0559e-08,\n",
       "         5.9622e-05, 1.3795e-11, 6.0762e-02, 4.3185e-06, 9.3434e-01, 2.1336e-08,\n",
       "         1.8618e-10, 1.1269e-08, 5.6089e-10, 8.7820e-12, 2.2399e-16, 6.0959e-13,\n",
       "         2.7466e-10, 2.5113e-11, 1.1183e-15, 2.9222e-09, 3.7969e-08, 3.7847e-07,\n",
       "         6.9798e-14, 3.6889e-06, 1.4486e-09],\n",
       "        [4.7839e-14, 3.7078e-13, 3.8476e-13, 2.7277e-10, 2.1652e-01, 1.6042e-11,\n",
       "         1.6073e-08, 9.9891e-14, 4.3415e-09, 6.7978e-09, 7.8348e-01, 2.2253e-10,\n",
       "         1.2981e-08, 1.6504e-11, 2.3615e-15, 3.7538e-12, 2.3594e-13, 3.2458e-14,\n",
       "         2.0007e-07, 2.3276e-15, 8.3631e-10, 2.2205e-14, 9.3346e-10, 1.5711e-16,\n",
       "         6.8855e-08, 2.5736e-08, 3.0437e-09],\n",
       "        [1.0672e-02, 1.4336e-04, 7.7367e-06, 1.9327e-04, 1.6998e-03, 2.7278e-02,\n",
       "         1.2006e-04, 5.9023e-08, 5.7096e-06, 1.7904e-02, 1.0873e-03, 7.0475e-13,\n",
       "         2.8959e-05, 3.9833e-04, 2.7656e-11, 8.4032e-12, 1.0345e-03, 2.9003e-01,\n",
       "         2.9314e-03, 3.2557e-04, 2.4926e-03, 5.4179e-03, 1.3492e-04, 1.8625e-09,\n",
       "         1.2367e-04, 4.3473e-05, 6.3793e-01],\n",
       "        [1.1226e-19, 2.4630e-11, 1.1134e-09, 2.0260e-11, 1.2618e-05, 3.6701e-08,\n",
       "         9.9933e-01, 4.9256e-06, 9.2675e-06, 2.0936e-14, 6.9778e-05, 7.0656e-06,\n",
       "         1.4782e-09, 7.3348e-06, 3.2403e-08, 3.9221e-13, 5.6213e-16, 1.9290e-10,\n",
       "         6.1216e-14, 1.7845e-05, 5.5750e-09, 4.0090e-06, 2.8528e-15, 1.3739e-08,\n",
       "         5.4155e-04, 3.4953e-10, 1.2426e-08],\n",
       "        [3.1885e-23, 5.6222e-11, 1.7466e-08, 2.0086e-11, 1.6093e-08, 5.5180e-10,\n",
       "         9.9998e-01, 2.5516e-07, 1.2096e-07, 1.7936e-12, 1.8554e-05, 4.1865e-06,\n",
       "         8.3834e-15, 1.1356e-09, 1.3062e-10, 2.4144e-15, 4.9457e-21, 3.3121e-11,\n",
       "         2.2630e-15, 2.4342e-08, 8.3693e-13, 9.5857e-11, 1.0430e-19, 4.0572e-11,\n",
       "         4.2605e-11, 5.9524e-12, 5.9621e-10],\n",
       "        [6.4540e-13, 5.4513e-07, 6.5505e-09, 1.3075e-06, 1.8940e-06, 1.5581e-05,\n",
       "         1.7508e-01, 4.6697e-04, 6.0240e-10, 1.3821e-06, 1.5411e-07, 6.1479e-09,\n",
       "         1.1052e-05, 7.6340e-05, 5.2222e-08, 4.6490e-10, 1.2435e-07, 2.7088e-03,\n",
       "         2.8746e-14, 5.7337e-03, 2.5571e-04, 3.3878e-05, 4.0708e-09, 2.4303e-09,\n",
       "         9.7240e-02, 7.1674e-09, 7.1837e-01],\n",
       "        [9.7070e-20, 2.2244e-13, 2.8058e-09, 1.2574e-09, 4.7920e-06, 2.2904e-07,\n",
       "         4.6084e-01, 2.5804e-03, 5.2952e-01, 4.3220e-16, 1.2692e-04, 1.7484e-09,\n",
       "         2.1545e-12, 6.8572e-03, 1.0923e-11, 9.5914e-09, 1.8545e-19, 1.0154e-08,\n",
       "         1.7380e-14, 5.7244e-05, 7.4284e-13, 6.6838e-06, 2.3454e-15, 1.4143e-08,\n",
       "         8.9912e-12, 2.5166e-09, 4.3701e-10],\n",
       "        [2.9267e-19, 1.5562e-07, 4.3406e-08, 5.6720e-06, 2.5076e-07, 5.5257e-10,\n",
       "         9.9620e-01, 7.6109e-07, 9.5521e-08, 4.0274e-10, 1.2614e-04, 1.2081e-04,\n",
       "         7.3386e-09, 1.4455e-10, 3.1355e-07, 2.8009e-09, 1.1039e-17, 1.6436e-11,\n",
       "         9.0437e-12, 4.0181e-08, 9.8340e-08, 6.3625e-10, 4.6855e-16, 1.9106e-12,\n",
       "         3.5405e-03, 5.3208e-09, 6.7131e-07],\n",
       "        [2.6299e-10, 2.6571e-04, 2.1486e-06, 4.7708e-08, 1.1657e-07, 4.4915e-06,\n",
       "         3.2259e-01, 6.2849e-05, 1.9184e-07, 1.2963e-04, 4.1771e-09, 7.9929e-06,\n",
       "         5.1949e-05, 6.0044e-02, 5.2465e-04, 1.7381e-11, 3.7663e-07, 1.7711e-02,\n",
       "         1.3639e-10, 4.7487e-01, 2.0506e-05, 1.1599e-01, 1.0816e-08, 5.7572e-06,\n",
       "         3.4058e-08, 1.2099e-05, 7.7047e-03],\n",
       "        [2.3943e-09, 5.2399e-08, 1.3363e-03, 4.1386e-07, 7.5859e-08, 2.2089e-07,\n",
       "         9.6894e-01, 9.2952e-07, 1.0128e-02, 1.2754e-03, 1.6716e-02, 6.8475e-04,\n",
       "         2.5788e-06, 4.9857e-05, 6.5921e-04, 1.8059e-12, 8.4686e-13, 5.0797e-08,\n",
       "         1.4171e-09, 4.3164e-07, 6.9405e-13, 1.2405e-07, 5.0931e-09, 1.2523e-05,\n",
       "         1.1677e-11, 1.9435e-04, 1.1759e-09]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d75948ba-eba6-4ad8-bf6f-4a06dd8ecb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6701e-08, 5.8001e-02, 5.4515e-12, 1.0190e-15, 1.7326e-03, 3.9221e-13,\n",
       "        3.9469e-05, 2.9331e-08, 1.0012e-06, 1.7265e-12, 4.0934e-05, 6.5928e-08,\n",
       "        2.4630e-11, 1.4998e-16, 1.5055e-08, 8.8236e-08, 2.0936e-14, 4.9955e-02,\n",
       "        3.0557e-14, 1.8796e-06, 4.8606e-03, 2.7263e-03, 1.8618e-10, 3.7078e-13,\n",
       "        1.0672e-02, 1.7845e-05, 2.4144e-15, 1.2435e-07, 5.2952e-01, 4.0274e-10,\n",
       "        2.6571e-04, 2.3943e-09])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at how likely the neural net thought the actual outputs were\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88929f87-05db-4e2d-b244-07bdc9f5b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's pretty bad, but it's ok because we haven't trained it at all yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4385fb57-2617-4559-b50b-8e528fdeafa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3de0adc1-d02f-40aa-a312-d46df4338fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22637fa3-d652-4305-924b-f0f4450a9a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.5602)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc5b4b09-f921-46af-b80d-71b1c1b36f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# TIDIED UP! (more respectable)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3103df59-90de-4c88-b710-8965e57948a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "990babc3-6a5b-4d0f-9f08-ed6e55999a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca7feef2-2a80-48d7-bbaf-06466a058cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32930ab5-b564-4554-81a5-7ebbbb8185bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # [32, 3, 2]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "299a18f9-5db5-45d9-a684-4ba02af2246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# TIDIED UP, 2! (even MORE respectable)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3ae2ff8-bb6a-4c55-ab20-09ea56320db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d12bf65-f375-4f22-b939-b86765d43488",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a22a244-1810-413d-a29b-a4a357b38141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "159eb64a-8aab-4dbd-b6a9-848de81fb0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # [32, 3, 2]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss\n",
    "# andrej explains a bunch of reasons to use F.cross_entropy instead of making your\n",
    "# own loss function\n",
    "# 1. efficiency -- pytorch can skip creating intermediate tensors which waste memory\n",
    "#    and can group operations together or something for more computational\n",
    "#    efficiency ??\n",
    "# 2. backward pass is more efficient because F.cross_entropy knows how to do\n",
    "#    backpropogation better or something ??\n",
    "# 3. better numerical behaviour -- you skip the bug where calling logits.exp() ends\n",
    "#    up giving you floating point infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "798aec57-8125-47bd-92dd-8c6c25c6ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# ITERATION 3, MAKE IT ITERATE! (now we're LEARNING with MACHINES)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f6c396c-61fc-43c9-a470-d8a93cd9499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2941b891-68f7-4e24-a4ed-79110a4d181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b48f3f67-0b2e-4d31-8160-7033e4b513ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "343ebbfb-369c-47ea-9f1e-ebcccf91c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8dbb9b13-391f-4389-bc02-5dd9507b527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.76971435546875\n",
      "13.656402587890625\n",
      "11.298770904541016\n",
      "9.452459335327148\n",
      "7.984263896942139\n",
      "6.891321659088135\n",
      "6.100014686584473\n",
      "5.452036380767822\n",
      "4.898152828216553\n",
      "4.4146647453308105\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    emb = C[X] # [32, 3, 2]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "148bd446-8dc6-4696-ab55-7cb9ffa70223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([10.7865, 12.2558, 17.3982, 13.2739, 10.6965, 10.7865,  9.5145,  9.0495,\n",
       "        14.0280, 11.8378,  9.9038, 15.4187, 10.7865, 10.1476,  9.8372, 11.7660,\n",
       "        10.7865, 10.0029,  9.2940,  9.6824, 11.4241,  9.4885,  8.1164,  9.5176,\n",
       "        12.6383, 10.7865, 10.6021, 11.0822,  6.3617, 17.3157, 12.4544,  8.1669],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 1,  8,  9,  0, 15,  1, 17,  2,  9,  9,  2,  0,  1, 15,  1,  0,  1, 19,\n",
       "         1,  1, 16, 10, 26,  9,  0,  1, 15, 16,  3,  9, 19,  1]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are the actual highest likelihood predictions\n",
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f14a5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 4 -- FULL DATASET TIME!\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ed724eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "851abb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e038938",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c79e410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73b845f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cdb6435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.398847579956055\n",
      "17.929603576660156\n",
      "15.25473690032959\n",
      "16.650882720947266\n",
      "13.82009220123291\n",
      "16.016050338745117\n",
      "9.395875930786133\n",
      "11.865763664245605\n",
      "11.321473121643066\n",
      "11.886265754699707\n",
      "11.60723876953125\n",
      "9.621662139892578\n",
      "10.382100105285645\n",
      "8.230337142944336\n",
      "10.58810043334961\n",
      "9.596362113952637\n",
      "8.012267112731934\n",
      "9.953437805175781\n",
      "10.471138000488281\n",
      "6.532413959503174\n",
      "9.95022201538086\n",
      "13.090639114379883\n",
      "7.876365661621094\n",
      "9.005633354187012\n",
      "6.01797342300415\n",
      "7.416234493255615\n",
      "7.078078269958496\n",
      "6.765199661254883\n",
      "8.042718887329102\n",
      "6.724619388580322\n",
      "7.217287540435791\n",
      "6.606802940368652\n",
      "6.842647552490234\n",
      "5.9526143074035645\n",
      "5.865553379058838\n",
      "7.023158550262451\n",
      "6.305763244628906\n",
      "6.25828742980957\n",
      "7.309614181518555\n",
      "7.676468372344971\n",
      "6.542578220367432\n",
      "4.751112461090088\n",
      "5.433254241943359\n",
      "7.1201252937316895\n",
      "4.6549882888793945\n",
      "4.627724647521973\n",
      "4.940521240234375\n",
      "6.734306335449219\n",
      "6.272601127624512\n",
      "5.342100143432617\n",
      "5.039976119995117\n",
      "7.209207534790039\n",
      "4.952581882476807\n",
      "4.79857063293457\n",
      "4.571561813354492\n",
      "5.695641994476318\n",
      "4.102962493896484\n",
      "6.110617160797119\n",
      "4.889636993408203\n",
      "6.12545919418335\n",
      "4.287574768066406\n",
      "4.480322360992432\n",
      "3.842975378036499\n",
      "3.619086742401123\n",
      "5.233297348022461\n",
      "3.8657407760620117\n",
      "4.4346537590026855\n",
      "4.829010963439941\n",
      "4.456973552703857\n",
      "5.36604118347168\n",
      "5.517421245574951\n",
      "3.902050495147705\n",
      "3.3900747299194336\n",
      "4.499850273132324\n",
      "3.648836851119995\n",
      "3.8135602474212646\n",
      "4.355334281921387\n",
      "3.323270082473755\n",
      "5.150407791137695\n",
      "4.040286064147949\n",
      "5.3328351974487305\n",
      "3.6962268352508545\n",
      "3.925429344177246\n",
      "4.157334327697754\n",
      "3.6255099773406982\n",
      "3.5787830352783203\n",
      "4.326396465301514\n",
      "2.966515302658081\n",
      "4.012258529663086\n",
      "4.677416801452637\n",
      "3.459599018096924\n",
      "4.788365364074707\n",
      "3.2811684608459473\n",
      "3.1679372787475586\n",
      "3.7699155807495117\n",
      "3.7555699348449707\n",
      "3.8918774127960205\n",
      "3.9707579612731934\n",
      "2.981034278869629\n",
      "3.3921844959259033\n",
      "3.5133631229400635\n",
      "4.081798553466797\n",
      "3.695183038711548\n",
      "3.795534610748291\n",
      "4.056897163391113\n",
      "4.0727338790893555\n",
      "3.097987651824951\n",
      "3.693645477294922\n",
      "3.825716972351074\n",
      "4.002623081207275\n",
      "3.9469761848449707\n",
      "3.8602423667907715\n",
      "3.793153762817383\n",
      "3.719752073287964\n",
      "3.4883267879486084\n",
      "3.6780550479888916\n",
      "3.7267119884490967\n",
      "3.451594114303589\n",
      "3.189434051513672\n",
      "3.8321428298950195\n",
      "3.5619373321533203\n",
      "3.2049691677093506\n",
      "3.2189252376556396\n",
      "3.6616358757019043\n",
      "2.9588847160339355\n",
      "3.270704984664917\n",
      "3.288054943084717\n",
      "3.24515438079834\n",
      "3.74800443649292\n",
      "3.6229796409606934\n",
      "3.6291394233703613\n",
      "3.7624781131744385\n",
      "3.240867853164673\n",
      "3.1677939891815186\n",
      "3.8770854473114014\n",
      "3.7352609634399414\n",
      "4.392421722412109\n",
      "3.0793912410736084\n",
      "3.803842067718506\n",
      "3.3088648319244385\n",
      "3.80228853225708\n",
      "2.7672152519226074\n",
      "3.309138774871826\n",
      "4.288457870483398\n",
      "3.1863350868225098\n",
      "3.795262575149536\n",
      "3.1815710067749023\n",
      "3.295774221420288\n",
      "3.0778841972351074\n",
      "3.600443124771118\n",
      "4.001805305480957\n",
      "3.472668409347534\n",
      "3.1870980262756348\n",
      "2.9003405570983887\n",
      "3.412940502166748\n",
      "4.007542133331299\n",
      "3.1648969650268555\n",
      "3.3938095569610596\n",
      "3.2290425300598145\n",
      "2.866349697113037\n",
      "3.3019866943359375\n",
      "3.404191017150879\n",
      "3.2271945476531982\n",
      "4.631747722625732\n",
      "3.108140468597412\n",
      "2.956778049468994\n",
      "3.287564277648926\n",
      "3.9104974269866943\n",
      "2.9426612854003906\n",
      "3.1754302978515625\n",
      "3.8696398735046387\n",
      "2.5536282062530518\n",
      "3.816035032272339\n",
      "2.669787645339966\n",
      "3.9137074947357178\n",
      "3.671555280685425\n",
      "3.0182318687438965\n",
      "3.5156850814819336\n",
      "2.8677492141723633\n",
      "3.0526862144470215\n",
      "3.3437860012054443\n",
      "2.6500096321105957\n",
      "2.6028504371643066\n",
      "3.494661808013916\n",
      "3.8521807193756104\n",
      "3.259542226791382\n",
      "3.265738010406494\n",
      "3.0824480056762695\n",
      "3.0834953784942627\n",
      "3.1947476863861084\n",
      "3.7386982440948486\n",
      "2.700695514678955\n",
      "3.5730907917022705\n",
      "2.8386390209198\n",
      "3.715977907180786\n",
      "3.2371044158935547\n",
      "3.372908115386963\n",
      "2.564903497695923\n",
      "2.880950450897217\n",
      "4.339513301849365\n",
      "3.2463901042938232\n",
      "3.766718864440918\n",
      "2.966287612915039\n",
      "3.167680501937866\n",
      "2.5656816959381104\n",
      "2.776207685470581\n",
      "3.289773464202881\n",
      "3.3077902793884277\n",
      "2.9013752937316895\n",
      "2.8336057662963867\n",
      "3.732773780822754\n",
      "3.1389572620391846\n",
      "2.73746919631958\n",
      "2.462763547897339\n",
      "3.80324387550354\n",
      "3.288376808166504\n",
      "2.6892147064208984\n",
      "3.0081870555877686\n",
      "2.806710958480835\n",
      "3.0631494522094727\n",
      "3.200265645980835\n",
      "2.979781150817871\n",
      "3.5097408294677734\n",
      "3.6102025508880615\n",
      "3.21329927444458\n",
      "2.905714273452759\n",
      "3.0158298015594482\n",
      "3.2103993892669678\n",
      "3.252894401550293\n",
      "3.0793533325195312\n",
      "3.0042459964752197\n",
      "3.2953150272369385\n",
      "3.1081056594848633\n",
      "3.354459285736084\n",
      "2.58878755569458\n",
      "2.8733742237091064\n",
      "3.0317108631134033\n",
      "3.037975311279297\n",
      "2.7145042419433594\n",
      "2.937027931213379\n",
      "2.7269608974456787\n",
      "2.9319417476654053\n",
      "3.221229314804077\n",
      "2.933929443359375\n",
      "3.213348388671875\n",
      "2.6106789112091064\n",
      "2.685290575027466\n",
      "3.223191022872925\n",
      "2.3506295680999756\n",
      "2.8955671787261963\n",
      "2.764575958251953\n",
      "2.8429336547851562\n",
      "3.0657260417938232\n",
      "3.030200719833374\n",
      "3.0956451892852783\n",
      "3.2639853954315186\n",
      "3.0837042331695557\n",
      "3.282292127609253\n",
      "3.024563789367676\n",
      "2.8884053230285645\n",
      "2.7653095722198486\n",
      "3.221158027648926\n",
      "2.653183937072754\n",
      "3.3884665966033936\n",
      "3.3401103019714355\n",
      "3.282653331756592\n",
      "3.3955960273742676\n",
      "2.70984148979187\n",
      "2.954087734222412\n",
      "2.880511522293091\n",
      "3.001802682876587\n",
      "3.060025691986084\n",
      "2.7453503608703613\n",
      "2.720383405685425\n",
      "2.5905792713165283\n",
      "2.9365501403808594\n",
      "2.753883123397827\n",
      "2.7735021114349365\n",
      "3.0379185676574707\n",
      "2.627500057220459\n",
      "2.6388168334960938\n",
      "3.098304510116577\n",
      "3.1250734329223633\n",
      "2.784115791320801\n",
      "3.04481840133667\n",
      "2.498310089111328\n",
      "3.0685343742370605\n",
      "3.2777957916259766\n",
      "2.7805066108703613\n",
      "3.0712058544158936\n",
      "3.172116994857788\n",
      "3.3746485710144043\n",
      "2.918243408203125\n",
      "2.485710382461548\n",
      "3.1975769996643066\n",
      "2.910822868347168\n",
      "3.041351556777954\n",
      "3.0726077556610107\n",
      "2.584592819213867\n",
      "3.452867269515991\n",
      "2.8034770488739014\n",
      "2.628016948699951\n",
      "2.9831011295318604\n",
      "2.933104991912842\n",
      "3.017232656478882\n",
      "3.521594285964966\n",
      "3.580712080001831\n",
      "2.8550806045532227\n",
      "2.867194414138794\n",
      "2.7223029136657715\n",
      "2.498514175415039\n",
      "3.33010196685791\n",
      "2.555943250656128\n",
      "3.2888996601104736\n",
      "3.355778694152832\n",
      "2.6233839988708496\n",
      "2.7847893238067627\n",
      "2.7853355407714844\n",
      "2.5925040245056152\n",
      "2.607780933380127\n",
      "2.8916802406311035\n",
      "2.9317426681518555\n",
      "3.0153298377990723\n",
      "2.9768950939178467\n",
      "2.684556722640991\n",
      "2.412285566329956\n",
      "2.507615804672241\n",
      "3.1150436401367188\n",
      "3.0752880573272705\n",
      "2.934741497039795\n",
      "2.7222983837127686\n",
      "2.5896434783935547\n",
      "3.2222812175750732\n",
      "3.4906458854675293\n",
      "3.668170928955078\n",
      "2.907670259475708\n",
      "2.7369344234466553\n",
      "3.053572654724121\n",
      "2.743234634399414\n",
      "3.488537549972534\n",
      "3.0931341648101807\n",
      "2.725893020629883\n",
      "3.043083429336548\n",
      "2.9878036975860596\n",
      "2.8551177978515625\n",
      "2.8673930168151855\n",
      "3.03039288520813\n",
      "2.6020350456237793\n",
      "2.464700222015381\n",
      "3.256006956100464\n",
      "2.783308982849121\n",
      "2.567359685897827\n",
      "2.2434706687927246\n",
      "2.6222875118255615\n",
      "3.12392258644104\n",
      "2.8031089305877686\n",
      "3.215616464614868\n",
      "3.4623045921325684\n",
      "2.7547190189361572\n",
      "3.043161153793335\n",
      "2.830979585647583\n",
      "3.038548469543457\n",
      "2.680410861968994\n",
      "2.9637601375579834\n",
      "2.9724881649017334\n",
      "3.1142284870147705\n",
      "2.8434858322143555\n",
      "3.1341042518615723\n",
      "3.1325035095214844\n",
      "2.95409893989563\n",
      "2.603541851043701\n",
      "2.6548914909362793\n",
      "3.0171871185302734\n",
      "2.833042860031128\n",
      "2.5146286487579346\n",
      "2.3361928462982178\n",
      "2.8603808879852295\n",
      "2.8221871852874756\n",
      "2.6497232913970947\n",
      "2.662567377090454\n",
      "2.7677249908447266\n",
      "2.8751473426818848\n",
      "2.572910785675049\n",
      "2.6096315383911133\n",
      "3.370774745941162\n",
      "2.7467527389526367\n",
      "2.7861547470092773\n",
      "3.2153854370117188\n",
      "2.7329607009887695\n",
      "2.9942190647125244\n",
      "3.247277021408081\n",
      "3.0280590057373047\n",
      "2.972656011581421\n",
      "2.730557680130005\n",
      "3.121445655822754\n",
      "2.829364061355591\n",
      "2.8796615600585938\n",
      "3.2282583713531494\n",
      "2.6762332916259766\n",
      "3.3279287815093994\n",
      "3.1192314624786377\n",
      "2.7671446800231934\n",
      "2.9454801082611084\n",
      "2.8229753971099854\n",
      "2.959357261657715\n",
      "2.918973922729492\n",
      "3.292752265930176\n",
      "2.840521812438965\n",
      "2.802809238433838\n",
      "2.490445375442505\n",
      "2.660064220428467\n",
      "3.282050132751465\n",
      "3.239380359649658\n",
      "3.20535945892334\n",
      "2.6325604915618896\n",
      "2.78865647315979\n",
      "2.669193744659424\n",
      "2.611419677734375\n",
      "2.7955050468444824\n",
      "2.8743836879730225\n",
      "2.680417060852051\n",
      "3.1080374717712402\n",
      "2.9220809936523438\n",
      "2.4416890144348145\n",
      "2.864323139190674\n",
      "3.6075551509857178\n",
      "3.236786365509033\n",
      "2.5538711547851562\n",
      "2.6734726428985596\n",
      "2.803502321243286\n",
      "3.716571807861328\n",
      "3.2956740856170654\n",
      "2.7748496532440186\n",
      "2.508206605911255\n",
      "3.775117874145508\n",
      "3.54168438911438\n",
      "3.420781373977661\n",
      "2.435091733932495\n",
      "3.10614013671875\n",
      "2.9798786640167236\n",
      "2.785825729370117\n",
      "3.193814277648926\n",
      "3.061802387237549\n",
      "2.7901268005371094\n",
      "2.720353841781616\n",
      "2.655902147293091\n",
      "2.8151512145996094\n",
      "2.4126946926116943\n",
      "3.0528862476348877\n",
      "2.6983633041381836\n",
      "3.102555751800537\n",
      "2.7123236656188965\n",
      "2.5771522521972656\n",
      "2.7327702045440674\n",
      "2.448798179626465\n",
      "2.949974775314331\n",
      "2.987628221511841\n",
      "2.7420291900634766\n",
      "2.9673056602478027\n",
      "2.690438747406006\n",
      "2.6725099086761475\n",
      "2.97220516204834\n",
      "2.757335901260376\n",
      "2.635735034942627\n",
      "3.1844334602355957\n",
      "2.5978188514709473\n",
      "2.549424886703491\n",
      "3.047088384628296\n",
      "2.4077959060668945\n",
      "2.8693554401397705\n",
      "2.9286365509033203\n",
      "2.2890729904174805\n",
      "3.017063856124878\n",
      "3.2162833213806152\n",
      "2.960193157196045\n",
      "2.697935104370117\n",
      "2.561791181564331\n",
      "2.5386950969696045\n",
      "2.8924307823181152\n",
      "3.066464424133301\n",
      "3.327871084213257\n",
      "2.5106585025787354\n",
      "3.0400052070617676\n",
      "2.9502370357513428\n",
      "2.788881778717041\n",
      "2.878354072570801\n",
      "3.4592936038970947\n",
      "2.63103985786438\n",
      "2.4840190410614014\n",
      "2.7650294303894043\n",
      "2.600748062133789\n",
      "2.8811800479888916\n",
      "2.807112216949463\n",
      "2.733081817626953\n",
      "2.9004318714141846\n",
      "3.1230335235595703\n",
      "2.6376137733459473\n",
      "2.966660261154175\n",
      "2.62841796875\n",
      "2.9772965908050537\n",
      "2.7314634323120117\n",
      "2.7489218711853027\n",
      "2.713491916656494\n",
      "3.2271347045898438\n",
      "2.6501669883728027\n",
      "3.3039710521698\n",
      "3.010385274887085\n",
      "2.928438901901245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6969809532165527\n",
      "2.4446747303009033\n",
      "3.0221519470214844\n",
      "2.842484951019287\n",
      "2.9106571674346924\n",
      "2.9745025634765625\n",
      "2.750222682952881\n",
      "2.7295970916748047\n",
      "2.8994240760803223\n",
      "2.9767253398895264\n",
      "2.8005175590515137\n",
      "2.982074737548828\n",
      "2.6691524982452393\n",
      "2.8432228565216064\n",
      "2.8519015312194824\n",
      "2.843674659729004\n",
      "2.975151300430298\n",
      "2.8254151344299316\n",
      "2.9762988090515137\n",
      "2.837631940841675\n",
      "3.0267014503479004\n",
      "2.8226044178009033\n",
      "2.6386425495147705\n",
      "2.3677756786346436\n",
      "2.590506076812744\n",
      "2.7285056114196777\n",
      "2.452359676361084\n",
      "3.227295398712158\n",
      "2.6370015144348145\n",
      "3.2204267978668213\n",
      "3.0019724369049072\n",
      "2.653672218322754\n",
      "2.7506139278411865\n",
      "2.906033515930176\n",
      "2.605267286300659\n",
      "2.7501070499420166\n",
      "2.58504056930542\n",
      "3.1646621227264404\n",
      "2.8332440853118896\n",
      "2.7880377769470215\n",
      "2.935258626937866\n",
      "3.0907111167907715\n",
      "2.945432424545288\n",
      "2.9970524311065674\n",
      "2.922455072402954\n",
      "2.892784595489502\n",
      "2.647291421890259\n",
      "2.836977958679199\n",
      "2.418673276901245\n",
      "2.6650075912475586\n",
      "3.189648389816284\n",
      "2.9550623893737793\n",
      "2.6591615676879883\n",
      "2.905155897140503\n",
      "2.708789825439453\n",
      "2.5512259006500244\n",
      "2.5298023223876953\n",
      "2.6636765003204346\n",
      "2.873807430267334\n",
      "2.380990505218506\n",
      "2.38787579536438\n",
      "2.757768154144287\n",
      "2.903538465499878\n",
      "2.5707361698150635\n",
      "2.559136390686035\n",
      "3.2661497592926025\n",
      "2.8278095722198486\n",
      "2.700071334838867\n",
      "2.8489537239074707\n",
      "2.8754611015319824\n",
      "2.9993643760681152\n",
      "2.7919797897338867\n",
      "2.6925556659698486\n",
      "2.5811848640441895\n",
      "2.593611240386963\n",
      "2.7374372482299805\n",
      "2.7387588024139404\n",
      "2.879986047744751\n",
      "2.8189289569854736\n",
      "3.023522138595581\n",
      "2.9709503650665283\n",
      "2.757154703140259\n",
      "2.9108059406280518\n",
      "2.510646343231201\n",
      "2.624994993209839\n",
      "2.942875623703003\n",
      "2.5627644062042236\n",
      "2.616534471511841\n",
      "2.4715118408203125\n",
      "2.9427785873413086\n",
      "2.8586606979370117\n",
      "2.813375949859619\n",
      "2.41511869430542\n",
      "2.5490529537200928\n",
      "2.8082587718963623\n",
      "3.0267820358276367\n",
      "2.66287899017334\n",
      "2.719397783279419\n",
      "2.4587953090667725\n",
      "2.6630518436431885\n",
      "2.9427430629730225\n",
      "2.4008233547210693\n",
      "2.742713212966919\n",
      "2.706653594970703\n",
      "2.7611138820648193\n",
      "2.651202440261841\n",
      "2.677941083908081\n",
      "2.9979465007781982\n",
      "3.168285608291626\n",
      "2.7594680786132812\n",
      "2.4252426624298096\n",
      "2.5829684734344482\n",
      "2.61966609954834\n",
      "2.465660333633423\n",
      "2.5542118549346924\n",
      "2.736095666885376\n",
      "2.7680134773254395\n",
      "2.417894124984741\n",
      "2.434910774230957\n",
      "3.240143060684204\n",
      "2.8288521766662598\n",
      "2.5910403728485107\n",
      "2.679161787033081\n",
      "2.764160633087158\n",
      "2.7386674880981445\n",
      "3.364494562149048\n",
      "2.9073896408081055\n",
      "2.7410993576049805\n",
      "2.8579792976379395\n",
      "2.7871623039245605\n",
      "2.8312606811523438\n",
      "2.5912859439849854\n",
      "2.8447041511535645\n",
      "2.7405800819396973\n",
      "2.6557343006134033\n",
      "2.8930647373199463\n",
      "2.793773889541626\n",
      "2.723442792892456\n",
      "2.6461737155914307\n",
      "2.730400323867798\n",
      "2.541121482849121\n",
      "2.546384572982788\n",
      "2.881300210952759\n",
      "3.2819323539733887\n",
      "2.6186776161193848\n",
      "2.489201545715332\n",
      "2.6293821334838867\n",
      "2.625218152999878\n",
      "2.839271306991577\n",
      "2.6449196338653564\n",
      "3.180537700653076\n",
      "2.468418836593628\n",
      "2.776224374771118\n",
      "2.482837677001953\n",
      "2.6507256031036377\n",
      "2.8834683895111084\n",
      "2.9408833980560303\n",
      "2.618605136871338\n",
      "2.702380418777466\n",
      "2.9048283100128174\n",
      "2.5578393936157227\n",
      "2.735806465148926\n",
      "2.763296127319336\n",
      "2.9214961528778076\n",
      "2.4118216037750244\n",
      "3.356426239013672\n",
      "2.7237043380737305\n",
      "2.5292224884033203\n",
      "3.1520133018493652\n",
      "2.620596170425415\n",
      "3.293872117996216\n",
      "2.5883731842041016\n",
      "3.081793785095215\n",
      "3.0003299713134766\n",
      "2.6938047409057617\n",
      "2.8351147174835205\n",
      "2.9701850414276123\n",
      "2.940284490585327\n",
      "2.9618496894836426\n",
      "2.6694841384887695\n",
      "2.6830999851226807\n",
      "2.923403263092041\n",
      "2.9486045837402344\n",
      "2.5424747467041016\n",
      "2.7601380348205566\n",
      "2.797445297241211\n",
      "2.707228422164917\n",
      "2.873055934906006\n",
      "2.691619873046875\n",
      "2.6146953105926514\n",
      "2.6279358863830566\n",
      "2.687643527984619\n",
      "2.9014410972595215\n",
      "3.196011781692505\n",
      "2.494357109069824\n",
      "2.924468517303467\n",
      "3.005923271179199\n",
      "2.510786294937134\n",
      "3.111013650894165\n",
      "2.5137648582458496\n",
      "2.881608009338379\n",
      "2.766291379928589\n",
      "2.4871530532836914\n",
      "2.4797005653381348\n",
      "3.0118408203125\n",
      "2.509153366088867\n",
      "2.7741901874542236\n",
      "2.542158603668213\n",
      "2.7570486068725586\n",
      "2.8231208324432373\n",
      "2.763867139816284\n",
      "3.0685575008392334\n",
      "2.846966505050659\n",
      "3.114159345626831\n",
      "2.781327962875366\n",
      "2.753131866455078\n",
      "3.103943109512329\n",
      "2.7828636169433594\n",
      "3.13506817817688\n",
      "2.7063100337982178\n",
      "3.07938289642334\n",
      "2.6734321117401123\n",
      "3.0150012969970703\n",
      "2.597247838973999\n",
      "2.7958765029907227\n",
      "2.743246078491211\n",
      "2.531421661376953\n",
      "2.677963972091675\n",
      "2.964355945587158\n",
      "2.6399857997894287\n",
      "2.6734328269958496\n",
      "2.7228851318359375\n",
      "2.6943233013153076\n",
      "3.0675952434539795\n",
      "2.761199951171875\n",
      "2.8252692222595215\n",
      "3.3105714321136475\n",
      "2.7374954223632812\n",
      "2.617218255996704\n",
      "2.640836000442505\n",
      "2.539121389389038\n",
      "2.7210726737976074\n",
      "2.729886054992676\n",
      "2.6770007610321045\n",
      "2.573265552520752\n",
      "2.8483662605285645\n",
      "2.777547597885132\n",
      "2.4674887657165527\n",
      "2.8053958415985107\n",
      "2.605429172515869\n",
      "3.0492000579833984\n",
      "2.408111333847046\n",
      "3.0079195499420166\n",
      "2.6179022789001465\n",
      "2.6261258125305176\n",
      "2.8031558990478516\n",
      "2.9595282077789307\n",
      "2.9917984008789062\n",
      "2.820465564727783\n",
      "2.8135011196136475\n",
      "2.8356454372406006\n",
      "3.0259273052215576\n",
      "2.5491342544555664\n",
      "2.6310527324676514\n",
      "2.617035388946533\n",
      "2.5100672245025635\n",
      "2.494688034057617\n",
      "2.9747021198272705\n",
      "2.471900701522827\n",
      "2.3201146125793457\n",
      "2.8838393688201904\n",
      "2.6901514530181885\n",
      "2.6837849617004395\n",
      "2.741135597229004\n",
      "3.0229363441467285\n",
      "3.074706792831421\n",
      "2.8867759704589844\n",
      "2.695551872253418\n",
      "2.7229135036468506\n",
      "2.7047383785247803\n",
      "2.5460851192474365\n",
      "2.855755090713501\n",
      "2.547264575958252\n",
      "2.8652217388153076\n",
      "3.026893377304077\n",
      "2.7781519889831543\n",
      "2.487222909927368\n",
      "2.577953338623047\n",
      "2.4381661415100098\n",
      "2.8275973796844482\n",
      "2.7380144596099854\n",
      "2.978940725326538\n",
      "2.4250400066375732\n",
      "2.3856494426727295\n",
      "2.986513137817383\n",
      "2.7036819458007812\n",
      "2.999464511871338\n",
      "2.7385430335998535\n",
      "2.3593153953552246\n",
      "2.3508996963500977\n",
      "2.5641047954559326\n",
      "2.456204414367676\n",
      "3.0100371837615967\n",
      "2.8695478439331055\n",
      "3.15631103515625\n",
      "2.6819519996643066\n",
      "2.7243711948394775\n",
      "2.969616651535034\n",
      "2.5496323108673096\n",
      "2.860356092453003\n",
      "2.8998148441314697\n",
      "3.415323257446289\n",
      "3.025967836380005\n",
      "2.848339796066284\n",
      "2.846850633621216\n",
      "2.7675461769104004\n",
      "2.387603282928467\n",
      "2.5473451614379883\n",
      "2.312847852706909\n",
      "2.5746965408325195\n",
      "2.4912302494049072\n",
      "3.008782148361206\n",
      "2.435166120529175\n",
      "2.651857376098633\n",
      "2.8706836700439453\n",
      "2.641653060913086\n",
      "2.541260242462158\n",
      "2.4275546073913574\n",
      "2.8511438369750977\n",
      "2.759674310684204\n",
      "2.6700398921966553\n",
      "2.993776559829712\n",
      "2.345339775085449\n",
      "2.6127429008483887\n",
      "3.0593550205230713\n",
      "2.1982460021972656\n",
      "3.124507188796997\n",
      "2.9563090801239014\n",
      "2.6513216495513916\n",
      "2.412799119949341\n",
      "2.384446144104004\n",
      "2.9216885566711426\n",
      "2.8488752841949463\n",
      "2.887728691101074\n",
      "2.633664846420288\n",
      "2.957409143447876\n",
      "2.778261661529541\n",
      "2.6293814182281494\n",
      "2.7069711685180664\n",
      "2.412008285522461\n",
      "2.530388116836548\n",
      "2.4797213077545166\n",
      "2.821077346801758\n",
      "2.6988348960876465\n",
      "2.557351589202881\n",
      "2.7255899906158447\n",
      "2.8133304119110107\n",
      "2.29821515083313\n",
      "2.4993419647216797\n",
      "2.6728944778442383\n",
      "2.2563982009887695\n",
      "2.380096435546875\n",
      "2.756291151046753\n",
      "2.8839612007141113\n",
      "2.6117727756500244\n",
      "2.6134121417999268\n",
      "2.639403820037842\n",
      "2.5602591037750244\n",
      "2.795581817626953\n",
      "2.777045965194702\n",
      "2.558558464050293\n",
      "2.2889363765716553\n",
      "2.45320987701416\n",
      "3.0409603118896484\n",
      "2.8032705783843994\n",
      "2.6621196269989014\n",
      "3.334209680557251\n",
      "2.553556203842163\n",
      "2.9853241443634033\n",
      "2.5726661682128906\n",
      "2.91129732131958\n",
      "2.424337387084961\n",
      "2.5420145988464355\n",
      "2.490190267562866\n",
      "2.847872257232666\n",
      "3.1223573684692383\n",
      "2.736933708190918\n",
      "2.5788910388946533\n",
      "2.951721429824829\n",
      "3.4373621940612793\n",
      "2.605233907699585\n",
      "2.6302523612976074\n",
      "2.7069594860076904\n",
      "2.6613917350769043\n",
      "3.084540605545044\n",
      "3.280505895614624\n",
      "2.7219882011413574\n",
      "2.851426601409912\n",
      "2.522771120071411\n",
      "2.8791022300720215\n",
      "2.4832568168640137\n",
      "2.5022284984588623\n",
      "3.039672613143921\n",
      "2.9410455226898193\n",
      "2.6966230869293213\n",
      "2.556781530380249\n",
      "2.596384048461914\n",
      "2.414707660675049\n",
      "3.0648040771484375\n",
      "2.5637099742889404\n",
      "2.6534621715545654\n",
      "2.7982051372528076\n",
      "2.1711082458496094\n",
      "2.1378495693206787\n",
      "2.807401657104492\n",
      "2.700882911682129\n",
      "2.582432270050049\n",
      "2.6229922771453857\n",
      "2.7123186588287354\n",
      "2.574305534362793\n",
      "2.6154463291168213\n",
      "2.613128662109375\n",
      "2.497546434402466\n",
      "2.6767849922180176\n",
      "2.7002758979797363\n",
      "2.734726905822754\n",
      "2.4569272994995117\n",
      "2.735779285430908\n",
      "2.314411163330078\n",
      "2.4295365810394287\n",
      "2.78145170211792\n",
      "2.6511991024017334\n",
      "2.588475227355957\n",
      "2.5279717445373535\n",
      "2.4623405933380127\n",
      "2.456446647644043\n",
      "2.7065799236297607\n",
      "2.7509214878082275\n",
      "3.0933706760406494\n",
      "2.5264155864715576\n",
      "2.6705565452575684\n",
      "2.4813804626464844\n",
      "2.322798013687134\n",
      "2.3552067279815674\n",
      "2.7810633182525635\n",
      "2.5281734466552734\n",
      "2.4613444805145264\n",
      "2.9828813076019287\n",
      "2.5806884765625\n",
      "2.693840503692627\n",
      "2.883110523223877\n",
      "2.6075944900512695\n",
      "2.679234266281128\n",
      "2.5422303676605225\n",
      "2.739825963973999\n",
      "2.3193185329437256\n",
      "3.111560106277466\n",
      "2.5293538570404053\n",
      "2.519026756286621\n",
      "2.424501419067383\n",
      "2.3356804847717285\n",
      "2.3335511684417725\n",
      "2.4735286235809326\n",
      "2.964740753173828\n",
      "2.4649603366851807\n",
      "2.8326473236083984\n",
      "2.2958178520202637\n",
      "2.7554268836975098\n",
      "2.580481767654419\n",
      "2.4553003311157227\n",
      "2.7328813076019287\n",
      "2.63668155670166\n",
      "2.645547389984131\n",
      "3.0148708820343018\n",
      "2.6599111557006836\n",
      "2.720322370529175\n",
      "3.106370449066162\n",
      "3.0284640789031982\n",
      "2.341942548751831\n",
      "2.4981918334960938\n",
      "2.3189752101898193\n",
      "2.684434652328491\n",
      "2.9001238346099854\n",
      "2.4928715229034424\n",
      "2.764953851699829\n",
      "2.485771417617798\n",
      "2.5632705688476562\n",
      "2.75826096534729\n",
      "2.3723347187042236\n",
      "2.55253005027771\n",
      "2.478595733642578\n",
      "3.003077745437622\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):   \n",
    "    # forward pass\n",
    "    #emb = C[X] # [32, 3, 2]\n",
    "    # this works but is slow. let's use random subsets of the data to train instead in order to have faster\n",
    "    # training cycles\n",
    "    \n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    emb = C[X[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e589f785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([113986,  97814, 119276, 220918,  70124, 170090,  30613,  71458, 204581,\n",
       "        210888, 123932, 125875, 187881, 130994, 103850, 184238,  68689,  52239,\n",
       "        191210,  92469,  50722,  13683,  99119,  56141, 119390, 146085,   6020,\n",
       "        106655,  93211,  55883, 184747, 155798])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example mini batch\n",
    "torch.randint(0, X.shape[0], (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ff20e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7032, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the loss on all the examples\n",
    "emb = C[X]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e134fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 5\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba1ebe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's learn how to LEARN a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9353f1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ad8973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5088c45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a03bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4cc4d807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # lre -- learning rate exponents\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f966d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.926361083984375\n",
      "17.96976089477539\n",
      "19.12790298461914\n",
      "18.475414276123047\n",
      "19.031558990478516\n",
      "17.445112228393555\n",
      "20.316877365112305\n",
      "16.234058380126953\n",
      "18.89381980895996\n",
      "19.689109802246094\n",
      "17.13533592224121\n",
      "19.461009979248047\n",
      "19.31509780883789\n",
      "18.497644424438477\n",
      "18.090028762817383\n",
      "20.408456802368164\n",
      "21.15532875061035\n",
      "20.94685935974121\n",
      "20.127437591552734\n",
      "19.460851669311523\n",
      "14.743107795715332\n",
      "21.47182273864746\n",
      "19.41636848449707\n",
      "20.41328239440918\n",
      "20.18423080444336\n",
      "17.83328628540039\n",
      "19.17543601989746\n",
      "21.293420791625977\n",
      "19.28510856628418\n",
      "17.594036102294922\n",
      "20.001293182373047\n",
      "18.634349822998047\n",
      "17.334148406982422\n",
      "21.92535400390625\n",
      "19.182601928710938\n",
      "17.861494064331055\n",
      "16.884449005126953\n",
      "19.382043838500977\n",
      "17.17072105407715\n",
      "18.223838806152344\n",
      "18.948537826538086\n",
      "18.68193244934082\n",
      "20.951894760131836\n",
      "19.237808227539062\n",
      "18.053560256958008\n",
      "15.30856990814209\n",
      "20.355192184448242\n",
      "18.263954162597656\n",
      "16.824811935424805\n",
      "18.457429885864258\n",
      "16.87924575805664\n",
      "19.143169403076172\n",
      "21.071929931640625\n",
      "17.65934944152832\n",
      "19.081153869628906\n",
      "16.046640396118164\n",
      "18.601686477661133\n",
      "21.465049743652344\n",
      "16.797094345092773\n",
      "16.714174270629883\n",
      "17.359731674194336\n",
      "17.352479934692383\n",
      "19.504375457763672\n",
      "17.266422271728516\n",
      "17.50028419494629\n",
      "17.128664016723633\n",
      "17.742956161499023\n",
      "18.197460174560547\n",
      "14.801097869873047\n",
      "14.659689903259277\n",
      "16.602176666259766\n",
      "18.520700454711914\n",
      "17.47651481628418\n",
      "18.878828048706055\n",
      "16.22191047668457\n",
      "17.1168212890625\n",
      "16.651058197021484\n",
      "18.329750061035156\n",
      "16.940893173217773\n",
      "17.02141761779785\n",
      "15.45364761352539\n",
      "16.395488739013672\n",
      "18.061248779296875\n",
      "15.812508583068848\n",
      "20.358966827392578\n",
      "16.190208435058594\n",
      "17.652246475219727\n",
      "17.86497688293457\n",
      "16.231510162353516\n",
      "19.318580627441406\n",
      "20.193187713623047\n",
      "18.807432174682617\n",
      "17.603450775146484\n",
      "18.04934310913086\n",
      "17.021869659423828\n",
      "14.858508110046387\n",
      "16.177295684814453\n",
      "15.072505950927734\n",
      "17.322864532470703\n",
      "15.386859893798828\n",
      "19.02376365661621\n",
      "15.824185371398926\n",
      "16.961740493774414\n",
      "18.412630081176758\n",
      "16.166763305664062\n",
      "17.568859100341797\n",
      "15.406793594360352\n",
      "16.639232635498047\n",
      "18.91107177734375\n",
      "17.96074676513672\n",
      "17.22933006286621\n",
      "13.63883113861084\n",
      "13.400774955749512\n",
      "16.48192596435547\n",
      "17.307573318481445\n",
      "16.839982986450195\n",
      "18.74069595336914\n",
      "15.368584632873535\n",
      "13.910571098327637\n",
      "17.52071762084961\n",
      "15.640637397766113\n",
      "14.442239761352539\n",
      "15.486769676208496\n",
      "15.983314514160156\n",
      "19.73188018798828\n",
      "15.657649040222168\n",
      "17.622779846191406\n",
      "17.099313735961914\n",
      "16.004972457885742\n",
      "13.801366806030273\n",
      "17.322893142700195\n",
      "17.207317352294922\n",
      "17.98723602294922\n",
      "18.104782104492188\n",
      "15.03431510925293\n",
      "14.125597953796387\n",
      "15.375842094421387\n",
      "15.255697250366211\n",
      "14.13585090637207\n",
      "15.428977966308594\n",
      "15.99940299987793\n",
      "16.34671974182129\n",
      "11.647624015808105\n",
      "13.469596862792969\n",
      "16.120868682861328\n",
      "13.370149612426758\n",
      "14.738761901855469\n",
      "15.532526969909668\n",
      "14.807799339294434\n",
      "15.663549423217773\n",
      "15.3944091796875\n",
      "13.895393371582031\n",
      "14.966485977172852\n",
      "14.100547790527344\n",
      "16.031248092651367\n",
      "13.572635650634766\n",
      "13.686445236206055\n",
      "14.564105987548828\n",
      "13.626189231872559\n",
      "14.84328842163086\n",
      "16.422101974487305\n",
      "13.546361923217773\n",
      "15.42561149597168\n",
      "15.081560134887695\n",
      "16.757183074951172\n",
      "13.430949211120605\n",
      "15.35684585571289\n",
      "15.930182456970215\n",
      "14.668488502502441\n",
      "12.719016075134277\n",
      "14.499262809753418\n",
      "13.483879089355469\n",
      "15.543058395385742\n",
      "15.35599422454834\n",
      "16.40696144104004\n",
      "14.429946899414062\n",
      "14.188450813293457\n",
      "13.18194580078125\n",
      "15.192365646362305\n",
      "16.977985382080078\n",
      "12.117926597595215\n",
      "17.865705490112305\n",
      "15.35578727722168\n",
      "13.763588905334473\n",
      "14.968819618225098\n",
      "15.563003540039062\n",
      "14.052284240722656\n",
      "14.525506019592285\n",
      "11.472674369812012\n",
      "11.094475746154785\n",
      "12.82127857208252\n",
      "12.337178230285645\n",
      "11.902908325195312\n",
      "16.00402069091797\n",
      "13.967050552368164\n",
      "12.777754783630371\n",
      "12.326041221618652\n",
      "10.954654693603516\n",
      "12.139875411987305\n",
      "12.555047988891602\n",
      "15.732953071594238\n",
      "13.389729499816895\n",
      "13.822489738464355\n",
      "15.554572105407715\n",
      "13.875167846679688\n",
      "11.036681175231934\n",
      "13.234625816345215\n",
      "14.09466552734375\n",
      "13.247174263000488\n",
      "11.902588844299316\n",
      "13.477506637573242\n",
      "14.538827896118164\n",
      "12.614715576171875\n",
      "14.53296184539795\n",
      "11.54257869720459\n",
      "11.846915245056152\n",
      "13.164773941040039\n",
      "14.631537437438965\n",
      "15.250198364257812\n",
      "11.67768383026123\n",
      "12.349523544311523\n",
      "14.973474502563477\n",
      "13.431755065917969\n",
      "15.160274505615234\n",
      "12.819969177246094\n",
      "13.001787185668945\n",
      "13.173798561096191\n",
      "13.290870666503906\n",
      "14.37843132019043\n",
      "11.833767890930176\n",
      "15.391905784606934\n",
      "12.592656135559082\n",
      "13.780632019042969\n",
      "12.955063819885254\n",
      "12.9783935546875\n",
      "10.751439094543457\n",
      "13.088494300842285\n",
      "15.116833686828613\n",
      "12.07552433013916\n",
      "10.4296293258667\n",
      "13.465112686157227\n",
      "11.189680099487305\n",
      "11.616793632507324\n",
      "13.322502136230469\n",
      "13.574426651000977\n",
      "13.600411415100098\n",
      "12.626605033874512\n",
      "12.07877254486084\n",
      "11.035968780517578\n",
      "15.33714771270752\n",
      "12.387476921081543\n",
      "11.267435073852539\n",
      "12.08510971069336\n",
      "13.013401985168457\n",
      "14.145139694213867\n",
      "14.960332870483398\n",
      "11.514937400817871\n",
      "12.575888633728027\n",
      "9.774063110351562\n",
      "10.607023239135742\n",
      "13.51283073425293\n",
      "8.975007057189941\n",
      "9.439043045043945\n",
      "12.931340217590332\n",
      "11.240830421447754\n",
      "11.46721363067627\n",
      "10.596610069274902\n",
      "11.340157508850098\n",
      "11.553116798400879\n",
      "10.262099266052246\n",
      "11.888223648071289\n",
      "11.782423973083496\n",
      "9.758356094360352\n",
      "10.586752891540527\n",
      "10.911273002624512\n",
      "11.558817863464355\n",
      "12.366316795349121\n",
      "12.626745223999023\n",
      "8.097862243652344\n",
      "11.861708641052246\n",
      "10.800864219665527\n",
      "10.522205352783203\n",
      "12.208049774169922\n",
      "9.596796989440918\n",
      "11.904821395874023\n",
      "10.927410125732422\n",
      "11.396961212158203\n",
      "10.722151756286621\n",
      "12.037431716918945\n",
      "9.197468757629395\n",
      "12.550925254821777\n",
      "9.878762245178223\n",
      "10.732566833496094\n",
      "11.439469337463379\n",
      "12.533563613891602\n",
      "9.98681926727295\n",
      "8.936797142028809\n",
      "9.915451049804688\n",
      "11.054606437683105\n",
      "11.339055061340332\n",
      "12.47586727142334\n",
      "10.442773818969727\n",
      "7.890836238861084\n",
      "10.251663208007812\n",
      "12.77644157409668\n",
      "9.86372184753418\n",
      "10.291169166564941\n",
      "11.00705337524414\n",
      "11.689188957214355\n",
      "12.488354682922363\n",
      "10.491710662841797\n",
      "12.234870910644531\n",
      "10.538125991821289\n",
      "11.708866119384766\n",
      "9.692270278930664\n",
      "10.861154556274414\n",
      "11.375718116760254\n",
      "9.548640251159668\n",
      "7.054093837738037\n",
      "10.41452407836914\n",
      "8.864091873168945\n",
      "12.909666061401367\n",
      "7.397414684295654\n",
      "10.101783752441406\n",
      "9.201750755310059\n",
      "8.59550952911377\n",
      "10.673409461975098\n",
      "7.6205363273620605\n",
      "11.877446174621582\n",
      "10.154582977294922\n",
      "9.706400871276855\n",
      "10.685835838317871\n",
      "10.34277057647705\n",
      "10.579635620117188\n",
      "9.534600257873535\n",
      "12.769232749938965\n",
      "8.594490051269531\n",
      "10.614278793334961\n",
      "9.179215431213379\n",
      "10.438211441040039\n",
      "9.851079940795898\n",
      "9.146479606628418\n",
      "8.930940628051758\n",
      "10.453550338745117\n",
      "8.000916481018066\n",
      "10.755420684814453\n",
      "12.366013526916504\n",
      "12.531702041625977\n",
      "10.869022369384766\n",
      "10.070561408996582\n",
      "10.41668701171875\n",
      "9.642420768737793\n",
      "10.069092750549316\n",
      "9.14160442352295\n",
      "7.1710991859436035\n",
      "8.222733497619629\n",
      "8.520965576171875\n",
      "9.80627155303955\n",
      "9.560528755187988\n",
      "9.80008316040039\n",
      "10.979628562927246\n",
      "8.41164779663086\n",
      "10.5346097946167\n",
      "10.358466148376465\n",
      "9.109672546386719\n",
      "8.686250686645508\n",
      "8.978307723999023\n",
      "8.580249786376953\n",
      "8.184359550476074\n",
      "9.092455863952637\n",
      "7.246810436248779\n",
      "8.786827087402344\n",
      "5.806435585021973\n",
      "6.59333610534668\n",
      "7.487067699432373\n",
      "8.53940200805664\n",
      "7.732248306274414\n",
      "9.466215133666992\n",
      "8.200461387634277\n",
      "6.825649261474609\n",
      "8.94509506225586\n",
      "8.37594985961914\n",
      "9.33330249786377\n",
      "7.825534343719482\n",
      "10.227387428283691\n",
      "7.868706703186035\n",
      "5.495728015899658\n",
      "7.4185614585876465\n",
      "7.5618109703063965\n",
      "8.421402931213379\n",
      "8.9841947555542\n",
      "7.927957057952881\n",
      "8.852789878845215\n",
      "9.759056091308594\n",
      "6.959822654724121\n",
      "7.840565204620361\n",
      "8.53597354888916\n",
      "10.429691314697266\n",
      "8.158646583557129\n",
      "9.479438781738281\n",
      "6.570241928100586\n",
      "8.257488250732422\n",
      "8.089462280273438\n",
      "8.983010292053223\n",
      "8.029279708862305\n",
      "7.951261043548584\n",
      "8.760985374450684\n",
      "8.268047332763672\n",
      "8.621634483337402\n",
      "7.220653533935547\n",
      "9.033879280090332\n",
      "9.383421897888184\n",
      "7.407112121582031\n",
      "8.155887603759766\n",
      "7.818645477294922\n",
      "7.027314186096191\n",
      "8.834261894226074\n",
      "7.607511520385742\n",
      "7.896197319030762\n",
      "6.271510601043701\n",
      "7.812317371368408\n",
      "5.528955459594727\n",
      "6.1544294357299805\n",
      "7.104102611541748\n",
      "7.081270217895508\n",
      "7.492180347442627\n",
      "6.320327281951904\n",
      "9.067916870117188\n",
      "7.607625961303711\n",
      "8.597249031066895\n",
      "8.907572746276855\n",
      "7.573896408081055\n",
      "6.072526931762695\n",
      "6.581428050994873\n",
      "5.933528900146484\n",
      "5.742088317871094\n",
      "7.386606693267822\n",
      "5.773727893829346\n",
      "6.673554420471191\n",
      "7.000082492828369\n",
      "7.18955659866333\n",
      "7.464727878570557\n",
      "8.583378791809082\n",
      "5.652982711791992\n",
      "5.429564952850342\n",
      "6.916292190551758\n",
      "7.355704307556152\n",
      "7.1195759773254395\n",
      "5.943604946136475\n",
      "6.220529556274414\n",
      "5.884341239929199\n",
      "6.89666223526001\n",
      "7.391059398651123\n",
      "7.000967502593994\n",
      "6.578342437744141\n",
      "7.669013023376465\n",
      "5.971911430358887\n",
      "6.638279914855957\n",
      "6.477176189422607\n",
      "6.481043815612793\n",
      "4.986568927764893\n",
      "6.100688457489014\n",
      "6.508950233459473\n",
      "6.09435510635376\n",
      "4.904580593109131\n",
      "6.9205145835876465\n",
      "5.009102821350098\n",
      "5.618606090545654\n",
      "5.871403694152832\n",
      "5.753418445587158\n",
      "6.959126949310303\n",
      "5.545250415802002\n",
      "8.2601957321167\n",
      "6.835438251495361\n",
      "6.514615058898926\n",
      "6.813348770141602\n",
      "5.184213161468506\n",
      "8.110002517700195\n",
      "6.129911422729492\n",
      "4.584641933441162\n",
      "5.7855305671691895\n",
      "7.672501564025879\n",
      "5.642524719238281\n",
      "4.804074764251709\n",
      "5.921530723571777\n",
      "4.864751815795898\n",
      "5.489877700805664\n",
      "5.636269569396973\n",
      "5.8634934425354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.264141082763672\n",
      "5.626675128936768\n",
      "5.200405120849609\n",
      "6.0723419189453125\n",
      "5.702256679534912\n",
      "5.325406074523926\n",
      "5.870796203613281\n",
      "6.00186824798584\n",
      "5.464966773986816\n",
      "5.4074788093566895\n",
      "5.357289791107178\n",
      "6.798704147338867\n",
      "5.453858852386475\n",
      "5.743854999542236\n",
      "4.784970760345459\n",
      "5.167202472686768\n",
      "3.8996200561523438\n",
      "4.709230899810791\n",
      "4.965834617614746\n",
      "4.752670764923096\n",
      "3.7197089195251465\n",
      "4.099640369415283\n",
      "4.848707675933838\n",
      "4.6380391120910645\n",
      "7.466917037963867\n",
      "5.1437764167785645\n",
      "5.392070770263672\n",
      "5.678283214569092\n",
      "5.1654157638549805\n",
      "5.28420877456665\n",
      "5.30387544631958\n",
      "4.689038276672363\n",
      "4.707107067108154\n",
      "4.621033191680908\n",
      "4.132054805755615\n",
      "5.447067737579346\n",
      "5.775485038757324\n",
      "5.248813629150391\n",
      "4.124370574951172\n",
      "5.844115734100342\n",
      "4.223869323730469\n",
      "6.630006790161133\n",
      "4.949919700622559\n",
      "4.750371932983398\n",
      "5.716800212860107\n",
      "3.4730911254882812\n",
      "5.96843147277832\n",
      "4.099976062774658\n",
      "4.83737325668335\n",
      "4.853448867797852\n",
      "4.096416473388672\n",
      "3.9120278358459473\n",
      "4.966629505157471\n",
      "4.8097004890441895\n",
      "5.002754211425781\n",
      "4.516347885131836\n",
      "4.1267170906066895\n",
      "5.295389652252197\n",
      "4.950339317321777\n",
      "4.346039772033691\n",
      "4.3640828132629395\n",
      "3.4247870445251465\n",
      "5.408266544342041\n",
      "3.8777425289154053\n",
      "3.895103931427002\n",
      "4.00885534286499\n",
      "4.013895034790039\n",
      "4.737373352050781\n",
      "4.8120245933532715\n",
      "3.1770482063293457\n",
      "5.155544757843018\n",
      "4.10327672958374\n",
      "4.392632961273193\n",
      "3.5194075107574463\n",
      "4.683304786682129\n",
      "5.1542510986328125\n",
      "4.491247653961182\n",
      "3.934752941131592\n",
      "3.7778658866882324\n",
      "3.383397340774536\n",
      "4.6836066246032715\n",
      "3.3938097953796387\n",
      "3.5937209129333496\n",
      "5.763393878936768\n",
      "5.87376594543457\n",
      "3.8988680839538574\n",
      "4.337911128997803\n",
      "5.143792152404785\n",
      "3.0949316024780273\n",
      "4.5221381187438965\n",
      "3.910104751586914\n",
      "3.410335063934326\n",
      "3.8630495071411133\n",
      "3.9231159687042236\n",
      "5.3632330894470215\n",
      "3.5880653858184814\n",
      "3.794816255569458\n",
      "4.477766513824463\n",
      "3.778083562850952\n",
      "3.5788238048553467\n",
      "2.719999313354492\n",
      "3.2813470363616943\n",
      "5.412910461425781\n",
      "3.7510297298431396\n",
      "3.322538375854492\n",
      "3.94604229927063\n",
      "4.044753074645996\n",
      "3.130018711090088\n",
      "3.232956647872925\n",
      "3.4457552433013916\n",
      "4.1846489906311035\n",
      "4.386946678161621\n",
      "4.023471355438232\n",
      "4.840534210205078\n",
      "3.7525367736816406\n",
      "3.477848529815674\n",
      "3.9842958450317383\n",
      "3.7247331142425537\n",
      "3.943707227706909\n",
      "3.928633689880371\n",
      "3.7235097885131836\n",
      "4.368484020233154\n",
      "3.252030372619629\n",
      "3.7558484077453613\n",
      "4.46180534362793\n",
      "3.639005661010742\n",
      "3.8470444679260254\n",
      "4.012204170227051\n",
      "3.900657892227173\n",
      "3.78033185005188\n",
      "3.7265326976776123\n",
      "3.4407737255096436\n",
      "3.8336563110351562\n",
      "3.12324595451355\n",
      "3.406247615814209\n",
      "3.4204976558685303\n",
      "3.1242425441741943\n",
      "3.3282089233398438\n",
      "4.495802879333496\n",
      "3.976029396057129\n",
      "3.06415057182312\n",
      "3.7336618900299072\n",
      "3.8100812435150146\n",
      "3.2434194087982178\n",
      "3.9509284496307373\n",
      "3.6477067470550537\n",
      "3.450943946838379\n",
      "3.5601329803466797\n",
      "3.110246419906616\n",
      "3.3209667205810547\n",
      "3.442300319671631\n",
      "3.219069004058838\n",
      "3.7761659622192383\n",
      "2.9119622707366943\n",
      "3.1229047775268555\n",
      "3.584484338760376\n",
      "3.77071475982666\n",
      "2.778918743133545\n",
      "3.4315025806427\n",
      "3.7482545375823975\n",
      "3.7633590698242188\n",
      "3.3773069381713867\n",
      "4.124083995819092\n",
      "3.3431003093719482\n",
      "3.486525058746338\n",
      "3.2992794513702393\n",
      "2.943375587463379\n",
      "3.144239902496338\n",
      "3.7044217586517334\n",
      "2.8721935749053955\n",
      "3.3043222427368164\n",
      "3.656404972076416\n",
      "3.0887460708618164\n",
      "3.4238383769989014\n",
      "2.8768835067749023\n",
      "3.215240240097046\n",
      "2.7196462154388428\n",
      "2.899216890335083\n",
      "3.6866719722747803\n",
      "2.953932046890259\n",
      "3.203361988067627\n",
      "3.127535343170166\n",
      "3.5876848697662354\n",
      "3.5455009937286377\n",
      "2.4105799198150635\n",
      "3.109029769897461\n",
      "3.3119707107543945\n",
      "3.492638111114502\n",
      "2.838369369506836\n",
      "3.0256993770599365\n",
      "3.3400230407714844\n",
      "2.8707919120788574\n",
      "2.964144229888916\n",
      "2.562589406967163\n",
      "3.3841967582702637\n",
      "2.671856164932251\n",
      "2.970951557159424\n",
      "2.714101791381836\n",
      "3.204625129699707\n",
      "3.674281358718872\n",
      "3.3829987049102783\n",
      "3.5153329372406006\n",
      "3.387308359146118\n",
      "3.121164083480835\n",
      "3.445460796356201\n",
      "2.7150192260742188\n",
      "4.023317337036133\n",
      "3.678389310836792\n",
      "2.5593931674957275\n",
      "3.5794389247894287\n",
      "3.031137228012085\n",
      "3.224978446960449\n",
      "2.9351794719696045\n",
      "2.7952091693878174\n",
      "3.444775104522705\n",
      "3.363884687423706\n",
      "2.5572171211242676\n",
      "3.2592668533325195\n",
      "3.027261257171631\n",
      "3.367281675338745\n",
      "3.471708297729492\n",
      "3.353985071182251\n",
      "3.851917266845703\n",
      "3.944762706756592\n",
      "2.8857927322387695\n",
      "3.7566635608673096\n",
      "2.9125969409942627\n",
      "3.9940130710601807\n",
      "2.921328544616699\n",
      "2.5066280364990234\n",
      "3.472644090652466\n",
      "3.0329627990722656\n",
      "3.2872872352600098\n",
      "3.0264298915863037\n",
      "3.147679567337036\n",
      "3.303527593612671\n",
      "3.4891796112060547\n",
      "2.88086199760437\n",
      "2.8569769859313965\n",
      "2.940674304962158\n",
      "2.5751113891601562\n",
      "3.1727240085601807\n",
      "2.6342668533325195\n",
      "3.1064963340759277\n",
      "3.228652238845825\n",
      "4.542004108428955\n",
      "2.9002058506011963\n",
      "3.924842357635498\n",
      "3.849241256713867\n",
      "3.3836395740509033\n",
      "3.2751450538635254\n",
      "3.748333692550659\n",
      "2.917393445968628\n",
      "3.249152660369873\n",
      "2.847029685974121\n",
      "4.213749885559082\n",
      "2.9991233348846436\n",
      "2.982717990875244\n",
      "3.2512545585632324\n",
      "3.0120058059692383\n",
      "3.2242820262908936\n",
      "3.1064774990081787\n",
      "2.847350597381592\n",
      "2.935645580291748\n",
      "3.0389606952667236\n",
      "3.1519153118133545\n",
      "3.2770538330078125\n",
      "3.1889307498931885\n",
      "3.1691460609436035\n",
      "3.3378539085388184\n",
      "3.04292893409729\n",
      "3.4485201835632324\n",
      "2.7263495922088623\n",
      "2.632737398147583\n",
      "3.4220635890960693\n",
      "3.683095932006836\n",
      "3.2020792961120605\n",
      "3.0982162952423096\n",
      "3.883312940597534\n",
      "3.423530101776123\n",
      "3.6122939586639404\n",
      "3.2821388244628906\n",
      "4.13594388961792\n",
      "3.7985644340515137\n",
      "3.1667587757110596\n",
      "3.8433265686035156\n",
      "2.8377063274383545\n",
      "3.4613327980041504\n",
      "3.696209192276001\n",
      "3.077850341796875\n",
      "3.877397298812866\n",
      "2.6982789039611816\n",
      "3.3001155853271484\n",
      "3.324985980987549\n",
      "3.081416130065918\n",
      "3.9385218620300293\n",
      "3.501004934310913\n",
      "3.427250862121582\n",
      "3.067173480987549\n",
      "3.406630277633667\n",
      "2.8736612796783447\n",
      "3.396757125854492\n",
      "3.4251561164855957\n",
      "2.9667577743530273\n",
      "3.9639265537261963\n",
      "4.925808429718018\n",
      "3.8272180557250977\n",
      "3.3447906970977783\n",
      "3.383619546890259\n",
      "4.081783294677734\n",
      "3.071591377258301\n",
      "3.4240972995758057\n",
      "3.543225049972534\n",
      "3.5511534214019775\n",
      "3.6370186805725098\n",
      "3.2286951541900635\n",
      "4.62801456451416\n",
      "3.1316680908203125\n",
      "3.963857889175415\n",
      "3.1339008808135986\n",
      "3.072822332382202\n",
      "2.0729892253875732\n",
      "3.502725601196289\n",
      "4.990385055541992\n",
      "4.5773024559021\n",
      "3.6160340309143066\n",
      "3.237133741378784\n",
      "3.328944444656372\n",
      "3.2276227474212646\n",
      "3.60927152633667\n",
      "3.460214138031006\n",
      "3.8820252418518066\n",
      "3.6816511154174805\n",
      "3.394412040710449\n",
      "3.181692600250244\n",
      "3.4456369876861572\n",
      "3.8835606575012207\n",
      "3.8247735500335693\n",
      "4.3077921867370605\n",
      "3.4527125358581543\n",
      "4.281980991363525\n",
      "3.2178077697753906\n",
      "3.554802179336548\n",
      "4.399176597595215\n",
      "4.127906799316406\n",
      "3.1255931854248047\n",
      "2.8779683113098145\n",
      "3.5144524574279785\n",
      "3.728975296020508\n",
      "3.571094274520874\n",
      "3.51946759223938\n",
      "4.075737476348877\n",
      "4.19013786315918\n",
      "4.351646423339844\n",
      "5.0110344886779785\n",
      "5.010444641113281\n",
      "4.86210823059082\n",
      "3.967768669128418\n",
      "3.8821170330047607\n",
      "3.634721517562866\n",
      "3.796236515045166\n",
      "4.875633239746094\n",
      "3.230767011642456\n",
      "4.862833023071289\n",
      "4.627094745635986\n",
      "5.438356876373291\n",
      "4.415954113006592\n",
      "3.530425548553467\n",
      "3.5204505920410156\n",
      "4.392280101776123\n",
      "4.678955554962158\n",
      "2.8825321197509766\n",
      "3.0906975269317627\n",
      "4.436741828918457\n",
      "3.0531811714172363\n",
      "3.368866443634033\n",
      "3.232314109802246\n",
      "3.003300666809082\n",
      "3.7766218185424805\n",
      "3.65830135345459\n",
      "4.304699897766113\n",
      "3.8436031341552734\n",
      "3.378875732421875\n",
      "5.310656547546387\n",
      "3.8818931579589844\n",
      "3.7997207641601562\n",
      "4.337902545928955\n",
      "5.42473840713501\n",
      "5.330197334289551\n",
      "5.849088668823242\n",
      "4.661998748779297\n",
      "4.2137837409973145\n",
      "3.6393654346466064\n",
      "3.4757609367370605\n",
      "4.911798477172852\n",
      "4.572011470794678\n",
      "5.4603166580200195\n",
      "5.092371463775635\n",
      "6.10405158996582\n",
      "4.963146209716797\n",
      "5.931220531463623\n",
      "4.605547904968262\n",
      "5.0090131759643555\n",
      "3.399937868118286\n",
      "4.357268333435059\n",
      "5.446742534637451\n",
      "4.988241672515869\n",
      "3.8457584381103516\n",
      "4.119117259979248\n",
      "3.4024088382720947\n",
      "4.0273823738098145\n",
      "3.6695597171783447\n",
      "3.5704092979431152\n",
      "4.187619209289551\n",
      "4.43163537979126\n",
      "6.362919330596924\n",
      "5.807700157165527\n",
      "7.144545555114746\n",
      "5.420128345489502\n",
      "4.433332920074463\n",
      "3.918102264404297\n",
      "5.835960388183594\n",
      "4.936388969421387\n",
      "4.178987503051758\n",
      "4.62785005569458\n",
      "4.225539207458496\n",
      "5.117194175720215\n",
      "4.505519390106201\n",
      "3.7869811058044434\n",
      "4.27872371673584\n",
      "5.2130446434021\n",
      "6.390346050262451\n",
      "5.519254207611084\n",
      "4.4255595207214355\n",
      "5.537789821624756\n",
      "6.077065467834473\n",
      "5.22313928604126\n",
      "4.545902252197266\n",
      "4.651834964752197\n",
      "4.326313495635986\n",
      "6.4509429931640625\n",
      "5.318501949310303\n",
      "5.131775856018066\n",
      "5.690068244934082\n",
      "5.315908908843994\n",
      "4.925691604614258\n",
      "6.911960124969482\n",
      "6.8709869384765625\n",
      "4.112520694732666\n",
      "5.4632415771484375\n",
      "7.526127815246582\n",
      "5.964852809906006\n",
      "6.157564163208008\n",
      "6.835712432861328\n",
      "6.028216361999512\n",
      "5.851258277893066\n",
      "6.051572322845459\n",
      "4.2302937507629395\n",
      "3.8657960891723633\n",
      "5.3714680671691895\n",
      "5.726950645446777\n",
      "5.3340959548950195\n",
      "6.540643215179443\n",
      "4.621257305145264\n",
      "5.704417705535889\n",
      "5.889547824859619\n",
      "5.077591896057129\n",
      "3.7066216468811035\n",
      "6.795790672302246\n",
      "3.7489373683929443\n",
      "5.528082847595215\n",
      "5.3235764503479\n",
      "5.612151145935059\n",
      "5.954873085021973\n",
      "8.571799278259277\n",
      "6.348613262176514\n",
      "7.607997417449951\n",
      "4.984717845916748\n",
      "5.826923370361328\n",
      "5.560624599456787\n",
      "7.7169904708862305\n",
      "7.246033191680908\n",
      "8.902892112731934\n",
      "6.652707099914551\n",
      "9.545317649841309\n",
      "7.26045560836792\n",
      "9.033371925354004\n",
      "7.2452545166015625\n",
      "10.968265533447266\n",
      "6.852506637573242\n",
      "7.309892654418945\n",
      "10.187299728393555\n",
      "8.4368314743042\n",
      "6.973562240600586\n",
      "4.610184669494629\n",
      "7.967813014984131\n",
      "7.129955768585205\n",
      "6.602243423461914\n",
      "7.4754319190979\n",
      "6.902971267700195\n",
      "5.921250343322754\n",
      "8.37711238861084\n",
      "7.738358974456787\n",
      "8.710630416870117\n",
      "7.704695701599121\n",
      "8.760478973388672\n",
      "9.420289993286133\n",
      "5.682145118713379\n",
      "8.060544967651367\n",
      "6.669436454772949\n",
      "7.0669355392456055\n"
     ]
    }
   ],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[X[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track learning rate stats\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d80d637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa1eb472b30>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91bfc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so 10 ** -1.0 or 0.1 is a pretty good learning rate\n",
    "# ... why exactly??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ef99f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8918, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the loss on all the examples\n",
    "emb = C[X]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8481c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 6\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f02abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c2fc8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.657775640487671\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[X[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "728eaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after doing a lot of iterations at a certain learning rate, \"people like to\" do\n",
    "# what's called learning rate decay, i.e. lowering learning rate e.g. by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d2f6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6114, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the loss on all the examples\n",
    "emb = C[X]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ef05b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks good, because we got a much lower loss (2.28 compared to 2.45 (!))\n",
    "# however when we use increasingly complex models there becomes a greater danger of overfitting\n",
    "# therefore we should split the data into multiple batches:\n",
    "# 1. training split      80% -- to optimize parameters of the model\n",
    "# 2. dev/validate split  10% -- to optimize hyperparameters (e.g. size of hidden layer, size of embedding, etc.)\n",
    "# 3. test split          10% -- to evaluate performance at the very end (to avoid overfitting)\n",
    "# (roughly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9bb0781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 6 -- TRAIN/TEST SPLIT (OH YEAH, TITLES ARE BACK!)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75f46ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8f2fdb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.int64, torch.Size([182625]), torch.int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Xtr.dtype, Ytr.shape, Ytr.dtype   # new dataset attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d6f08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "70b0895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.595453977584839\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5aafc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5638, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29c2fce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5687, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be7d3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we see the dev and training loss are both roughly equal.\n",
    "# we're not overfitting.\n",
    "# according to Andrej, we're actually underfitting\n",
    "# so let's BEEF up those parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "af378008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3851, -0.3709],\n",
       "        [ 0.1804, -0.3348],\n",
       "        [-0.0119, -0.3095],\n",
       "        [-0.0728, -0.4066],\n",
       "        [-0.1313, -0.3339],\n",
       "        [ 0.0987, -0.2879],\n",
       "        [-0.1223, -0.3759],\n",
       "        [ 0.5488,  2.3140],\n",
       "        [-0.1801, -0.2523],\n",
       "        [-0.0312, -0.2908],\n",
       "        [-0.2623, -0.3394],\n",
       "        [-0.1888, -0.3638],\n",
       "        [ 0.4153,  2.1372],\n",
       "        [-0.1805, -0.3696],\n",
       "        [-0.1599, -0.1802],\n",
       "        [ 0.1281, -0.3109],\n",
       "        [ 0.3540,  1.9610],\n",
       "        [-0.3655, -0.5487],\n",
       "        [-0.1915, -0.3829],\n",
       "        [-0.0066, -0.4195],\n",
       "        [-0.1072, -0.4421],\n",
       "        [ 0.5456,  2.1743],\n",
       "        [-0.2539, -0.4180],\n",
       "        [-0.1869, -0.4036],\n",
       "        [-0.1847, -0.2332],\n",
       "        [-0.0951, -0.2321],\n",
       "        [-0.1869, -0.3577]], requires_grad=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "97b9fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 6 -- MORE PARAMETERS\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75fd0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 300), generator=g) # weights\n",
    "b1 = torch.randn(300, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((300, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# easiest way to increase the size of the nerual net is to increase the size of the hidden layer (27,100) -> (27,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7c974aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10281"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "815a78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lri = []\n",
    "#lossi = []\n",
    "#stepi = []\n",
    "\n",
    "for i in range(10000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15a845d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5312, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f089aa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5389, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "27de6dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39f90786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 7 -- MORE (MORE) PARAMETERS\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a929eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we're going to increase the size of the embeddings...\n",
    "# (and slightly decrease the size of the hidden layer)\n",
    "\n",
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((30, 200), generator=g) # weights\n",
    "b1 = torch.randn(200, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "946784ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "58c45496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lri = []\n",
    "#lossi = []\n",
    "#stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7fd4aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f41deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8df2b413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2730, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3c926ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2576, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]    \n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "00327e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING FROM THE MODEL !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
