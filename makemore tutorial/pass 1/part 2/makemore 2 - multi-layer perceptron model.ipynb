{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552aa5a8-1c05-4ab5-9653-656cb4c89428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name generation model\n",
    "# based on this paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f694b08d-9534-4b04-ab57-5ad37b89c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e918d40a-6dd5-4d61-a6f5-ea7d05a999df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read  in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe49c448-a633-4076-9353-5d244ea8a0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b384bf-c57e-4dce-ae24-6c9f51f7b0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5004cf44-226d-4c23-a423-280db68ee8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf87248-14c8-4fc2-ac4e-d46d3c345409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c3003e-1b6c-40db-a50c-438b6bfd9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9904fee-f22b-4395-8502-074bb204d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed input in lower dimensional space\n",
    "# original paper embeds 17,000 words in 30 dimensional space\n",
    "# we have 27 possible input characters. let's try a 2 dimensional space.\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab79473-5ba9-49f4-885d-dce3acfae1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2794,  0.9109],\n",
       "        [ 0.1409,  0.4732],\n",
       "        [ 0.6917, -0.0952]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7])] # using list or tensor as index instead of number gives us a tensor of the respective values in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a294cf2-6958-42ed-b7ec-98fb78f11ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 1.2794,  0.9109]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 1.2794,  0.9109],\n",
       "         [-1.3216, -0.7413]],\n",
       "\n",
       "        [[ 1.2794,  0.9109],\n",
       "         [-1.3216, -0.7413],\n",
       "         [-1.3216, -0.7413]],\n",
       "\n",
       "        [[-1.3216, -0.7413],\n",
       "         [-1.3216, -0.7413],\n",
       "         [-0.6083,  1.1529]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.4639,  0.7713]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.4639,  0.7713],\n",
       "         [ 0.5806,  0.4032]],\n",
       "\n",
       "        [[ 0.4639,  0.7713],\n",
       "         [ 0.5806,  0.4032],\n",
       "         [ 0.4205, -0.9643]],\n",
       "\n",
       "        [[ 0.5806,  0.4032],\n",
       "         [ 0.4205, -0.9643],\n",
       "         [-0.0470,  0.4804]],\n",
       "\n",
       "        [[ 0.4205, -0.9643],\n",
       "         [-0.0470,  0.4804],\n",
       "         [ 0.4205, -0.9643]],\n",
       "\n",
       "        [[-0.0470,  0.4804],\n",
       "         [ 0.4205, -0.9643],\n",
       "         [-0.6083,  1.1529]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [-0.6083,  1.1529]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [-0.6083,  1.1529],\n",
       "         [-0.0470,  0.4804]],\n",
       "\n",
       "        [[-0.6083,  1.1529],\n",
       "         [-0.0470,  0.4804],\n",
       "         [-0.6083,  1.1529]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.4205, -0.9643]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.4205, -0.9643],\n",
       "         [ 0.9335, -0.2228]],\n",
       "\n",
       "        [[ 0.4205, -0.9643],\n",
       "         [ 0.9335, -0.2228],\n",
       "         [-0.6083,  1.1529]],\n",
       "\n",
       "        [[ 0.9335, -0.2228],\n",
       "         [-0.6083,  1.1529],\n",
       "         [-2.1662, -1.3280]],\n",
       "\n",
       "        [[-0.6083,  1.1529],\n",
       "         [-2.1662, -1.3280],\n",
       "         [ 1.2794,  0.9109]],\n",
       "\n",
       "        [[-2.1662, -1.3280],\n",
       "         [ 1.2794,  0.9109],\n",
       "         [ 0.5806,  0.4032]],\n",
       "\n",
       "        [[ 1.2794,  0.9109],\n",
       "         [ 0.5806,  0.4032],\n",
       "         [ 0.5806,  0.4032]],\n",
       "\n",
       "        [[ 0.5806,  0.4032],\n",
       "         [ 0.5806,  0.4032],\n",
       "         [-0.6083,  1.1529]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.0538, -0.7437],\n",
       "         [ 0.9335, -0.2228]],\n",
       "\n",
       "        [[ 0.0538, -0.7437],\n",
       "         [ 0.9335, -0.2228],\n",
       "         [ 0.4639,  0.7713]],\n",
       "\n",
       "        [[ 0.9335, -0.2228],\n",
       "         [ 0.4639,  0.7713],\n",
       "         [-0.3905, -0.6170]],\n",
       "\n",
       "        [[ 0.4639,  0.7713],\n",
       "         [-0.3905, -0.6170],\n",
       "         [ 0.2407,  1.0063]],\n",
       "\n",
       "        [[-0.3905, -0.6170],\n",
       "         [ 0.2407,  1.0063],\n",
       "         [ 0.4205, -0.9643]],\n",
       "\n",
       "        [[ 0.2407,  1.0063],\n",
       "         [ 0.4205, -0.9643],\n",
       "         [-0.6083,  1.1529]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # we can also index with multidimensional integers\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fbd16b4-0eca-492f-844c-1184a67e2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76af0273-b13a-4991-ac7f-4f86f62fa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "W1 = torch.randn((6, 100)) # weights\n",
    "# 6 x 100 because:\n",
    "# (number of inputs == 6 == embedding dimensions (2) x n-embeddings per input (3))\n",
    "#   x \n",
    "# (number of neurons in this layer == some arbitrary amount of neurons (100))\n",
    "b1 = torch.randn(100) # biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad715e39-e0bb-4802-8e11-e8bd74fcea65",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m# what we want to do\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1 + b1 # what we want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf27005-cc6d-4d0b-8341-cbb6800957a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but it doesn't work because each element in emb has size (3x2) instead of size 6.\n",
    "# we need to smush these together somehow.\n",
    "# there are multiple ways to acheive this depending on exact requirements\n",
    "# we'll use torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4244c02-0362-4a0b-87a2-fe6741ede611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  1.2794,  0.9109],\n",
       "        [ 0.0538, -0.7437,  1.2794,  0.9109, -1.3216, -0.7413],\n",
       "        [ 1.2794,  0.9109, -1.3216, -0.7413, -1.3216, -0.7413],\n",
       "        [-1.3216, -0.7413, -1.3216, -0.7413, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.4639,  0.7713],\n",
       "        [ 0.0538, -0.7437,  0.4639,  0.7713,  0.5806,  0.4032],\n",
       "        [ 0.4639,  0.7713,  0.5806,  0.4032,  0.4205, -0.9643],\n",
       "        [ 0.5806,  0.4032,  0.4205, -0.9643, -0.0470,  0.4804],\n",
       "        [ 0.4205, -0.9643, -0.0470,  0.4804,  0.4205, -0.9643],\n",
       "        [-0.0470,  0.4804,  0.4205, -0.9643, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437, -0.6083,  1.1529, -0.0470,  0.4804],\n",
       "        [-0.6083,  1.1529, -0.0470,  0.4804, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.4205, -0.9643],\n",
       "        [ 0.0538, -0.7437,  0.4205, -0.9643,  0.9335, -0.2228],\n",
       "        [ 0.4205, -0.9643,  0.9335, -0.2228, -0.6083,  1.1529],\n",
       "        [ 0.9335, -0.2228, -0.6083,  1.1529, -2.1662, -1.3280],\n",
       "        [-0.6083,  1.1529, -2.1662, -1.3280,  1.2794,  0.9109],\n",
       "        [-2.1662, -1.3280,  1.2794,  0.9109,  0.5806,  0.4032],\n",
       "        [ 1.2794,  0.9109,  0.5806,  0.4032,  0.5806,  0.4032],\n",
       "        [ 0.5806,  0.4032,  0.5806,  0.4032, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.9335, -0.2228],\n",
       "        [ 0.0538, -0.7437,  0.9335, -0.2228,  0.4639,  0.7713],\n",
       "        [ 0.9335, -0.2228,  0.4639,  0.7713, -0.3905, -0.6170],\n",
       "        [ 0.4639,  0.7713, -0.3905, -0.6170,  0.2407,  1.0063],\n",
       "        [-0.3905, -0.6170,  0.2407,  1.0063,  0.4205, -0.9643],\n",
       "        [ 0.2407,  1.0063,  0.4205, -0.9643, -0.6083,  1.1529]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
    "# this will do it, but it's ugly because it's hard-coded.\n",
    "# what if we wanted to change the dimensions of emb from e.g. (_, 3, _) to (_, M, _) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8297fa8-0e83-46db-b304-72030fe44e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  1.2794,  0.9109],\n",
       "        [ 0.0538, -0.7437,  1.2794,  0.9109, -1.3216, -0.7413],\n",
       "        [ 1.2794,  0.9109, -1.3216, -0.7413, -1.3216, -0.7413],\n",
       "        [-1.3216, -0.7413, -1.3216, -0.7413, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.4639,  0.7713],\n",
       "        [ 0.0538, -0.7437,  0.4639,  0.7713,  0.5806,  0.4032],\n",
       "        [ 0.4639,  0.7713,  0.5806,  0.4032,  0.4205, -0.9643],\n",
       "        [ 0.5806,  0.4032,  0.4205, -0.9643, -0.0470,  0.4804],\n",
       "        [ 0.4205, -0.9643, -0.0470,  0.4804,  0.4205, -0.9643],\n",
       "        [-0.0470,  0.4804,  0.4205, -0.9643, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437, -0.6083,  1.1529, -0.0470,  0.4804],\n",
       "        [-0.6083,  1.1529, -0.0470,  0.4804, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.4205, -0.9643],\n",
       "        [ 0.0538, -0.7437,  0.4205, -0.9643,  0.9335, -0.2228],\n",
       "        [ 0.4205, -0.9643,  0.9335, -0.2228, -0.6083,  1.1529],\n",
       "        [ 0.9335, -0.2228, -0.6083,  1.1529, -2.1662, -1.3280],\n",
       "        [-0.6083,  1.1529, -2.1662, -1.3280,  1.2794,  0.9109],\n",
       "        [-2.1662, -1.3280,  1.2794,  0.9109,  0.5806,  0.4032],\n",
       "        [ 1.2794,  0.9109,  0.5806,  0.4032,  0.5806,  0.4032],\n",
       "        [ 0.5806,  0.4032,  0.5806,  0.4032, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.9335, -0.2228],\n",
       "        [ 0.0538, -0.7437,  0.9335, -0.2228,  0.4639,  0.7713],\n",
       "        [ 0.9335, -0.2228,  0.4639,  0.7713, -0.3905, -0.6170],\n",
       "        [ 0.4639,  0.7713, -0.3905, -0.6170,  0.2407,  1.0063],\n",
       "        [-0.3905, -0.6170,  0.2407,  1.0063,  0.4205, -0.9643],\n",
       "        [ 0.2407,  1.0063,  0.4205, -0.9643, -0.6083,  1.1529]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ans: use torch.unbind\n",
    "#torch.unbind(emb, 1) # == [emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]]\n",
    "torch.cat(torch.unbind(emb, 1), 1)\n",
    "# unfortunately, this is SUPA INEFFICIENT, because torch.cat will copy everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "774e0659-255f-4c07-a471-0c1166417e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  1.2794,  0.9109],\n",
       "        [ 0.0538, -0.7437,  1.2794,  0.9109, -1.3216, -0.7413],\n",
       "        [ 1.2794,  0.9109, -1.3216, -0.7413, -1.3216, -0.7413],\n",
       "        [-1.3216, -0.7413, -1.3216, -0.7413, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.4639,  0.7713],\n",
       "        [ 0.0538, -0.7437,  0.4639,  0.7713,  0.5806,  0.4032],\n",
       "        [ 0.4639,  0.7713,  0.5806,  0.4032,  0.4205, -0.9643],\n",
       "        [ 0.5806,  0.4032,  0.4205, -0.9643, -0.0470,  0.4804],\n",
       "        [ 0.4205, -0.9643, -0.0470,  0.4804,  0.4205, -0.9643],\n",
       "        [-0.0470,  0.4804,  0.4205, -0.9643, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437, -0.6083,  1.1529, -0.0470,  0.4804],\n",
       "        [-0.6083,  1.1529, -0.0470,  0.4804, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.4205, -0.9643],\n",
       "        [ 0.0538, -0.7437,  0.4205, -0.9643,  0.9335, -0.2228],\n",
       "        [ 0.4205, -0.9643,  0.9335, -0.2228, -0.6083,  1.1529],\n",
       "        [ 0.9335, -0.2228, -0.6083,  1.1529, -2.1662, -1.3280],\n",
       "        [-0.6083,  1.1529, -2.1662, -1.3280,  1.2794,  0.9109],\n",
       "        [-2.1662, -1.3280,  1.2794,  0.9109,  0.5806,  0.4032],\n",
       "        [ 1.2794,  0.9109,  0.5806,  0.4032,  0.5806,  0.4032],\n",
       "        [ 0.5806,  0.4032,  0.5806,  0.4032, -0.6083,  1.1529],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.0538, -0.7437],\n",
       "        [ 0.0538, -0.7437,  0.0538, -0.7437,  0.9335, -0.2228],\n",
       "        [ 0.0538, -0.7437,  0.9335, -0.2228,  0.4639,  0.7713],\n",
       "        [ 0.9335, -0.2228,  0.4639,  0.7713, -0.3905, -0.6170],\n",
       "        [ 0.4639,  0.7713, -0.3905, -0.6170,  0.2407,  1.0063],\n",
       "        [-0.3905, -0.6170,  0.2407,  1.0063,  0.4205, -0.9643],\n",
       "        [ 0.2407,  1.0063,  0.4205, -0.9643, -0.6083,  1.1529]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# even BETTER + EFFICIENT ans: use tensor.view!\n",
    "# changes how tensor is INDEXED instead of STORED. efficient!\n",
    "# changes storage offset, strides, and shapes\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e162c283-a869-4ea7-9f80-c76bae5755c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1)\n",
    "# activations of inputs emb (h for \"hidden states\")\n",
    "# could do emb.view(-1, 6) and pytorch would figure out what the first dimension needs to be in order to make the size work\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddb4a76e-31d6-4bda-baed-82542573ba08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5682,  0.0052,  0.3113,  ...,  0.5839, -0.1124,  0.9348],\n",
       "        [ 0.8612,  0.7693, -0.9890,  ...,  0.9578,  0.4915, -0.9999],\n",
       "        [-0.9992, -0.8587, -0.0996,  ...,  0.9258,  0.2002,  0.9999],\n",
       "        ...,\n",
       "        [ 0.9914, -0.9194, -0.7099,  ...,  0.5958, -0.8240, -0.9951],\n",
       "        [-0.0539, -0.1342, -0.9891,  ...,  0.9787, -0.3020,  0.9816],\n",
       "        [ 0.6416, -0.8814,  0.9068,  ..., -0.0152, -0.4138, -0.9641]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe14e3-9712-433c-990c-44dbc901d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make sure the expression \"emb.view(emb.shape[0], 6) @ W1 + b1\"\n",
    "# is broadcasting correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c134812-2567-4e1d-a324-ac789d7b2ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(emb.shape[0], 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d2e6050-2795-418d-a55c-5fdd19e33c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76ff1e7b-ca4e-428d-b876-559dc8e240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align on right, make missing dimensions size 1, then copy all dimensions of size 1\n",
    "# 32 100    32 100    32 100\n",
    "#    100 ->  1 100 -> 32 100\n",
    "# so it's correct (note: in case this looks wrong because we know \n",
    "#                        matrix multiplication needs (N, M) x (M, O) dimensions,\n",
    "#                        remember we're not doing a multiplication on these\n",
    "#                        two values, we're doing an addition, so we actually\n",
    "#                        do want identical sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8512fe1c-42bb-4274-8962-83ca09ec6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer\n",
    "W2 = torch.randn((100, 27)) # input size -> 27 characters\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5428ae0-f5ed-48c5-8b2a-cd5d1805ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd1f5f4a-3f12-4fa0-9b08-c6f3c0b298e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a158efa-6a0f-4ba6-a4be-5ffee43f5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c61aff17-6da2-4c86-8e05-c6129bb05068",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d79288c9-6da3-4e77-ad83-f60f716986e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70f85842-098b-47a0-8fb5-144857217217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65748a42-446c-4dec-af52-5fab6b9469eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5315e-13, 4.0494e-08, 5.5371e-07, 3.1887e-03, 1.2082e-09, 9.9565e-01,\n",
       "         2.7799e-09, 6.4958e-11, 9.3622e-09, 4.1729e-14, 3.5972e-11, 1.1511e-04,\n",
       "         2.2061e-04, 2.4117e-08, 1.5218e-09, 8.9600e-05, 7.2825e-11, 1.4529e-04,\n",
       "         2.9543e-04, 8.4674e-16, 7.2751e-07, 1.6533e-04, 5.0219e-10, 3.8599e-06,\n",
       "         1.2581e-04, 8.1741e-12, 4.2301e-09],\n",
       "        [6.0061e-16, 7.4413e-06, 5.7400e-09, 6.1091e-06, 3.9415e-06, 9.9492e-01,\n",
       "         1.7604e-08, 2.0985e-07, 1.2121e-07, 9.6651e-13, 6.6366e-12, 5.1197e-05,\n",
       "         6.9465e-07, 1.5825e-09, 1.9254e-03, 5.8585e-07, 3.6856e-12, 1.0943e-10,\n",
       "         2.2530e-10, 9.5484e-15, 2.0856e-05, 1.9202e-06, 1.2106e-09, 5.7655e-09,\n",
       "         3.0658e-03, 6.5779e-15, 3.9983e-08],\n",
       "        [2.0248e-07, 1.7439e-12, 1.8397e-12, 3.0563e-10, 1.6212e-13, 3.6910e-08,\n",
       "         8.6244e-09, 4.0961e-13, 9.9812e-11, 4.8850e-10, 1.4072e-09, 2.3538e-06,\n",
       "         1.7619e-09, 9.1004e-08, 1.3357e-13, 2.2425e-09, 4.2242e-09, 5.8343e-09,\n",
       "         1.0000e+00, 3.7715e-10, 6.1454e-11, 1.4766e-07, 1.9182e-12, 2.5797e-09,\n",
       "         8.3769e-09, 2.0850e-06, 2.2581e-11],\n",
       "        [8.5686e-05, 3.0565e-09, 8.4684e-01, 1.2706e-01, 1.0174e-04, 3.6290e-08,\n",
       "         8.2889e-09, 9.7339e-06, 1.1731e-08, 1.5615e-10, 1.1741e-11, 8.6193e-05,\n",
       "         1.5533e-04, 2.2903e-03, 2.6503e-07, 5.2878e-04, 2.1775e-02, 2.3826e-11,\n",
       "         2.3067e-07, 7.0288e-05, 5.6375e-08, 1.0015e-03, 5.0374e-11, 2.8985e-07,\n",
       "         3.5584e-08, 1.3584e-12, 9.6333e-08],\n",
       "        [1.1007e-08, 2.1844e-06, 5.8419e-08, 9.5187e-01, 3.1540e-07, 2.3613e-03,\n",
       "         1.1213e-12, 2.4223e-08, 1.0816e-03, 3.6973e-10, 1.3063e-09, 2.1778e-08,\n",
       "         1.3364e-10, 1.3868e-13, 6.3150e-04, 2.0729e-05, 9.3994e-17, 9.9862e-06,\n",
       "         3.1648e-07, 8.1657e-11, 8.6836e-11, 3.7563e-06, 2.2788e-04, 7.9061e-04,\n",
       "         4.2996e-02, 2.2452e-10, 6.1145e-13],\n",
       "        [5.5315e-13, 4.0494e-08, 5.5371e-07, 3.1887e-03, 1.2082e-09, 9.9565e-01,\n",
       "         2.7799e-09, 6.4958e-11, 9.3622e-09, 4.1729e-14, 3.5972e-11, 1.1511e-04,\n",
       "         2.2061e-04, 2.4117e-08, 1.5218e-09, 8.9600e-05, 7.2825e-11, 1.4529e-04,\n",
       "         2.9543e-04, 8.4674e-16, 7.2751e-07, 1.6533e-04, 5.0219e-10, 3.8599e-06,\n",
       "         1.2581e-04, 8.1741e-12, 4.2301e-09],\n",
       "        [1.7578e-16, 1.8456e-06, 1.2301e-08, 4.3762e-06, 1.4040e-07, 9.9971e-01,\n",
       "         3.0442e-10, 5.8842e-09, 9.3395e-07, 4.7709e-14, 1.1315e-10, 2.1334e-05,\n",
       "         1.6730e-06, 5.4856e-11, 7.8599e-06, 3.2836e-06, 6.3127e-12, 8.8232e-08,\n",
       "         1.6407e-08, 9.5093e-14, 9.3906e-09, 4.7188e-05, 7.7981e-09, 7.6180e-08,\n",
       "         2.0209e-04, 3.7400e-15, 2.4771e-08],\n",
       "        [6.7878e-10, 1.2561e-09, 8.0993e-10, 7.7418e-11, 3.3318e-15, 6.0327e-01,\n",
       "         7.5229e-10, 2.9140e-09, 1.0535e-08, 6.3718e-10, 9.1359e-08, 5.8720e-04,\n",
       "         3.3747e-07, 2.0653e-08, 5.2247e-06, 2.4799e-07, 4.4082e-09, 9.9400e-09,\n",
       "         6.8785e-04, 2.6961e-13, 3.9542e-01, 2.2059e-05, 4.0257e-14, 1.3483e-09,\n",
       "         1.2257e-09, 1.7210e-08, 5.0950e-09],\n",
       "        [1.2895e-07, 5.1882e-05, 1.1392e-08, 1.9150e-05, 1.2327e-05, 3.2596e-05,\n",
       "         1.8799e-05, 1.0940e-08, 6.0551e-14, 8.8433e-08, 1.8102e-12, 6.4419e-03,\n",
       "         1.3432e-04, 5.0890e-05, 7.1586e-07, 8.5777e-01, 8.1044e-02, 1.6838e-08,\n",
       "         1.2432e-07, 6.9212e-08, 1.9934e-03, 5.2433e-02, 2.4990e-13, 6.3777e-12,\n",
       "         2.1590e-07, 1.2380e-07, 2.0400e-07],\n",
       "        [9.1230e-16, 4.3931e-06, 4.0784e-11, 3.2858e-03, 2.8526e-02, 8.2574e-03,\n",
       "         2.0329e-07, 7.3710e-08, 1.9778e-07, 6.5318e-10, 4.3048e-10, 7.5893e-07,\n",
       "         1.1003e-04, 2.3089e-09, 3.1170e-04, 5.8001e-03, 9.7559e-06, 5.1064e-08,\n",
       "         1.1137e-09, 2.8865e-09, 6.7849e-11, 9.1881e-01, 2.0636e-09, 4.5087e-08,\n",
       "         3.4882e-02, 6.3467e-10, 4.3184e-08],\n",
       "        [4.1914e-11, 1.7440e-08, 1.1959e-05, 2.3962e-08, 1.3113e-13, 6.6345e-03,\n",
       "         1.4310e-06, 1.1411e-10, 2.9535e-08, 4.1651e-13, 6.8758e-10, 1.6556e-03,\n",
       "         1.7226e-05, 1.1822e-08, 6.9718e-10, 6.3168e-07, 6.3963e-06, 2.2075e-08,\n",
       "         7.1315e-04, 1.0081e-11, 9.9094e-01, 1.9816e-07, 6.6111e-14, 1.4570e-05,\n",
       "         2.2306e-11, 7.5776e-14, 1.4437e-07],\n",
       "        [6.1507e-14, 7.5279e-09, 7.1557e-11, 1.1906e-04, 4.2221e-04, 1.5925e-06,\n",
       "         4.0842e-10, 9.5914e-09, 5.9574e-09, 4.4418e-11, 1.1339e-11, 6.8759e-06,\n",
       "         3.1487e-07, 8.7135e-10, 1.4442e-07, 4.9259e-04, 4.1424e-12, 5.8229e-09,\n",
       "         4.8478e-09, 1.3232e-08, 1.9238e-14, 2.5255e-05, 1.2468e-09, 1.2016e-07,\n",
       "         9.9893e-01, 3.9140e-07, 3.5126e-10],\n",
       "        [5.5315e-13, 4.0494e-08, 5.5371e-07, 3.1887e-03, 1.2082e-09, 9.9565e-01,\n",
       "         2.7799e-09, 6.4958e-11, 9.3622e-09, 4.1729e-14, 3.5972e-11, 1.1511e-04,\n",
       "         2.2061e-04, 2.4117e-08, 1.5218e-09, 8.9600e-05, 7.2825e-11, 1.4529e-04,\n",
       "         2.9543e-04, 8.4674e-16, 7.2751e-07, 1.6533e-04, 5.0219e-10, 3.8599e-06,\n",
       "         1.2581e-04, 8.1741e-12, 4.2301e-09],\n",
       "        [8.6327e-13, 5.0347e-07, 1.9062e-05, 1.7319e-04, 2.3461e-07, 9.2255e-01,\n",
       "         5.9481e-09, 5.2644e-08, 4.6075e-02, 2.0633e-10, 1.4633e-06, 9.9703e-05,\n",
       "         6.8081e-03, 3.1362e-07, 5.4887e-03, 1.6986e-04, 4.7828e-09, 1.0085e-05,\n",
       "         8.7018e-03, 5.1334e-07, 2.1489e-14, 2.1240e-03, 8.3169e-06, 4.3779e-04,\n",
       "         7.3296e-03, 7.1202e-09, 1.1196e-08],\n",
       "        [4.6337e-07, 2.8573e-10, 1.1555e-01, 2.5819e-08, 1.1194e-13, 4.5574e-03,\n",
       "         2.8726e-07, 1.6339e-06, 7.9836e-06, 9.9988e-10, 7.3346e-07, 6.9361e-01,\n",
       "         2.0307e-08, 4.6715e-06, 5.4690e-05, 8.5749e-08, 2.5585e-08, 1.6928e-07,\n",
       "         4.7864e-02, 1.9424e-09, 1.3828e-01, 6.7792e-05, 4.3341e-11, 8.1469e-07,\n",
       "         2.6792e-11, 2.8489e-08, 2.7812e-07],\n",
       "        [6.8234e-03, 3.0335e-06, 7.8110e-07, 1.5509e-03, 1.7550e-03, 1.6653e-05,\n",
       "         4.9063e-07, 1.1257e-05, 4.2678e-06, 2.8968e-05, 6.4456e-10, 9.0497e-01,\n",
       "         2.7940e-06, 1.0515e-06, 7.3039e-05, 2.2535e-03, 3.8159e-07, 9.7192e-09,\n",
       "         3.2372e-04, 1.1514e-02, 4.9066e-07, 8.1165e-05, 1.4560e-07, 3.5953e-04,\n",
       "         5.2811e-02, 1.7417e-02, 3.7261e-09],\n",
       "        [5.5315e-13, 4.0494e-08, 5.5371e-07, 3.1887e-03, 1.2082e-09, 9.9565e-01,\n",
       "         2.7799e-09, 6.4958e-11, 9.3622e-09, 4.1729e-14, 3.5972e-11, 1.1511e-04,\n",
       "         2.2061e-04, 2.4117e-08, 1.5218e-09, 8.9600e-05, 7.2825e-11, 1.4529e-04,\n",
       "         2.9543e-04, 8.4674e-16, 7.2751e-07, 1.6533e-04, 5.0219e-10, 3.8599e-06,\n",
       "         1.2581e-04, 8.1741e-12, 4.2301e-09],\n",
       "        [4.5359e-14, 9.6824e-09, 7.8188e-08, 1.4093e-03, 2.4221e-09, 9.9850e-01,\n",
       "         2.9309e-09, 1.0790e-11, 7.5258e-11, 8.4495e-15, 5.0346e-12, 1.7341e-05,\n",
       "         1.1167e-05, 7.6820e-09, 4.5719e-11, 5.6360e-06, 3.3510e-12, 2.3850e-06,\n",
       "         6.5706e-07, 4.1202e-18, 2.7436e-07, 4.2145e-05, 1.3328e-10, 4.3305e-08,\n",
       "         9.4221e-06, 3.0214e-12, 4.4190e-09],\n",
       "        [1.4483e-18, 5.2989e-08, 5.5662e-10, 1.2428e-05, 5.3717e-09, 9.9971e-01,\n",
       "         7.4547e-11, 1.6470e-10, 1.0997e-12, 3.1286e-15, 1.7267e-12, 5.1803e-06,\n",
       "         1.4382e-08, 1.5936e-10, 1.0290e-10, 2.5291e-07, 8.2397e-15, 2.6612e-07,\n",
       "         3.1191e-10, 9.1843e-20, 1.4917e-08, 7.0242e-06, 8.8341e-10, 1.5400e-10,\n",
       "         2.6070e-04, 1.4191e-14, 9.7395e-07],\n",
       "        [1.6247e-12, 6.3718e-10, 1.7135e-10, 9.2086e-10, 3.6018e-11, 1.4925e-01,\n",
       "         1.6699e-09, 2.3817e-10, 5.7120e-06, 1.3297e-10, 1.1961e-05, 2.3017e-04,\n",
       "         1.4162e-03, 2.5592e-05, 5.2045e-07, 6.3879e-05, 7.2689e-07, 1.8678e-07,\n",
       "         8.4411e-01, 6.0121e-07, 7.6007e-14, 4.8771e-03, 3.3819e-09, 1.4489e-06,\n",
       "         3.6653e-07, 3.4636e-08, 5.9406e-10],\n",
       "        [1.0733e-04, 6.4649e-07, 4.5741e-02, 1.9197e-06, 3.4135e-10, 3.0272e-05,\n",
       "         2.2417e-04, 1.2701e-05, 1.7245e-04, 3.8502e-07, 4.6030e-07, 1.8749e-01,\n",
       "         1.4612e-04, 5.9029e-07, 2.7232e-09, 3.6811e-04, 7.0778e-05, 1.1374e-08,\n",
       "         7.1001e-01, 2.0421e-04, 1.1193e-03, 3.6908e-04, 3.8062e-11, 5.3930e-02,\n",
       "         3.3991e-09, 6.4595e-10, 1.1846e-06],\n",
       "        [1.0636e-12, 3.0358e-04, 2.1965e-05, 1.1170e-01, 1.2341e-04, 2.7971e-05,\n",
       "         2.4854e-12, 9.5446e-08, 2.5557e-08, 1.2843e-11, 2.0401e-14, 9.0577e-09,\n",
       "         3.8408e-08, 4.4355e-14, 2.4847e-03, 5.3192e-05, 7.9637e-10, 2.5204e-11,\n",
       "         2.6072e-14, 2.7688e-12, 9.4706e-08, 2.5280e-09, 4.7242e-09, 6.9646e-08,\n",
       "         8.8529e-01, 6.2120e-16, 3.5529e-07],\n",
       "        [1.1403e-07, 3.5473e-09, 7.3550e-12, 1.7522e-11, 2.1369e-14, 8.5981e-07,\n",
       "         2.3954e-13, 4.3343e-12, 1.7922e-09, 3.6417e-05, 6.1541e-10, 8.5972e-04,\n",
       "         7.4208e-09, 2.5706e-08, 6.8601e-07, 6.1029e-08, 9.3231e-15, 1.3696e-04,\n",
       "         9.0380e-02, 1.1822e-16, 4.0435e-03, 7.8731e-11, 1.4589e-09, 4.6098e-07,\n",
       "         9.8758e-05, 9.0444e-01, 2.9298e-10],\n",
       "        [1.6428e-12, 3.0639e-08, 6.9567e-09, 1.5767e-08, 2.6723e-07, 4.2257e-04,\n",
       "         5.8414e-05, 4.8814e-10, 7.2854e-13, 2.0528e-10, 3.0596e-10, 6.8622e-06,\n",
       "         8.9456e-07, 6.8573e-11, 7.5337e-05, 1.1614e-05, 9.9939e-01, 9.3469e-16,\n",
       "         1.8889e-12, 7.3121e-08, 1.3138e-07, 3.6250e-05, 5.5781e-17, 1.7966e-13,\n",
       "         6.0413e-11, 1.7677e-14, 4.8049e-07],\n",
       "        [7.2071e-07, 2.1139e-08, 1.1880e-07, 2.4446e-06, 2.0230e-07, 6.6281e-02,\n",
       "         6.1081e-04, 2.8094e-05, 5.8961e-04, 7.4478e-07, 2.0699e-05, 1.2688e-01,\n",
       "         1.9818e-04, 2.8501e-06, 3.7312e-04, 1.5351e-02, 3.1588e-03, 1.5418e-09,\n",
       "         3.5909e-01, 4.2179e-01, 3.2570e-07, 5.3085e-03, 3.4100e-11, 1.0183e-04,\n",
       "         6.5585e-08, 2.0304e-04, 1.2279e-07],\n",
       "        [5.5315e-13, 4.0494e-08, 5.5371e-07, 3.1887e-03, 1.2082e-09, 9.9565e-01,\n",
       "         2.7799e-09, 6.4958e-11, 9.3622e-09, 4.1729e-14, 3.5972e-11, 1.1511e-04,\n",
       "         2.2061e-04, 2.4117e-08, 1.5218e-09, 8.9600e-05, 7.2825e-11, 1.4529e-04,\n",
       "         2.9543e-04, 8.4674e-16, 7.2751e-07, 1.6533e-04, 5.0219e-10, 3.8599e-06,\n",
       "         1.2581e-04, 8.1741e-12, 4.2301e-09],\n",
       "        [6.3939e-18, 3.1980e-07, 3.8715e-09, 2.2901e-06, 2.1598e-10, 9.9999e-01,\n",
       "         1.1223e-12, 5.0151e-12, 4.5782e-13, 4.9942e-16, 2.0675e-13, 4.1193e-06,\n",
       "         6.6525e-10, 5.1654e-12, 2.5933e-11, 4.9502e-09, 9.6318e-15, 7.2502e-09,\n",
       "         3.1049e-11, 1.3902e-20, 2.1403e-09, 2.6102e-07, 4.3573e-11, 8.0066e-11,\n",
       "         2.4004e-06, 8.4711e-17, 6.3092e-09],\n",
       "        [5.7582e-16, 1.2453e-07, 1.1777e-14, 1.0707e-09, 7.3779e-12, 9.9986e-01,\n",
       "         4.9457e-11, 2.8888e-11, 1.0954e-10, 3.7567e-12, 2.2066e-10, 2.5951e-07,\n",
       "         4.0332e-07, 1.4539e-09, 2.0277e-07, 2.0179e-05, 1.0467e-12, 7.7678e-09,\n",
       "         4.0364e-06, 5.1677e-13, 2.2755e-07, 1.0930e-04, 4.2942e-11, 1.2435e-09,\n",
       "         7.6271e-06, 3.6019e-11, 1.2870e-10],\n",
       "        [3.5985e-08, 1.4458e-07, 4.0787e-05, 4.5306e-06, 8.6600e-12, 1.0339e-04,\n",
       "         8.9048e-05, 2.8461e-08, 2.3548e-08, 6.5305e-09, 7.1717e-07, 5.5063e-01,\n",
       "         5.5775e-04, 5.7191e-07, 3.7340e-08, 6.3030e-04, 1.6672e-03, 1.3702e-08,\n",
       "         4.4483e-01, 2.3849e-04, 2.9352e-04, 8.1105e-04, 1.3804e-12, 9.5681e-05,\n",
       "         2.4776e-10, 2.6264e-08, 3.4387e-06],\n",
       "        [6.9434e-13, 9.2534e-08, 3.9529e-07, 4.9085e-04, 1.1224e-01, 1.0537e-02,\n",
       "         5.5737e-11, 7.3635e-07, 1.6423e-07, 1.0559e-10, 2.5560e-12, 4.9790e-07,\n",
       "         7.5062e-08, 2.0312e-11, 8.5434e-01, 1.1769e-06, 4.8700e-07, 2.1852e-12,\n",
       "         6.5035e-13, 1.5710e-09, 7.0277e-10, 1.8414e-02, 8.5549e-11, 4.9999e-09,\n",
       "         3.9759e-03, 3.8068e-12, 1.0843e-09],\n",
       "        [1.8973e-07, 4.7070e-08, 3.6504e-09, 1.1986e-09, 1.6604e-14, 1.1696e-04,\n",
       "         1.0111e-07, 1.7481e-09, 9.5235e-11, 3.9853e-08, 2.2332e-11, 7.0119e-05,\n",
       "         4.3383e-08, 6.0798e-08, 9.4780e-10, 2.9962e-07, 2.2086e-08, 2.7597e-04,\n",
       "         1.9334e-03, 7.9851e-13, 9.9760e-01, 1.6976e-09, 8.4660e-13, 5.0366e-06,\n",
       "         7.0962e-09, 1.5858e-06, 5.3749e-07],\n",
       "        [9.7461e-13, 3.3886e-08, 1.2614e-09, 7.4746e-04, 1.0145e-01, 7.5578e-06,\n",
       "         5.8309e-08, 7.8214e-08, 3.0430e-08, 1.2377e-09, 2.1732e-11, 3.2292e-06,\n",
       "         2.5036e-08, 9.8727e-09, 2.5010e-07, 8.5155e-04, 3.5747e-07, 5.0105e-10,\n",
       "         4.1976e-10, 3.6984e-07, 6.3642e-15, 1.1472e-03, 2.1532e-10, 5.5185e-08,\n",
       "         8.9579e-01, 2.1974e-07, 2.0011e-09]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d75948ba-eba6-4ad8-bf6f-4a06dd8ecb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9565e-01, 1.5825e-09, 9.1004e-08, 3.0565e-09, 1.1007e-08, 8.9600e-05,\n",
       "        1.6730e-06, 6.3718e-10, 2.4990e-13, 6.5318e-10, 1.7440e-08, 6.1507e-14,\n",
       "        4.0494e-08, 8.3169e-06, 2.8573e-10, 6.8234e-03, 4.1729e-14, 4.1202e-18,\n",
       "        5.2989e-08, 1.7135e-10, 3.0272e-05, 3.8408e-08, 7.4208e-09, 3.0639e-08,\n",
       "        7.2071e-07, 8.4674e-16, 4.9502e-09, 1.0467e-12, 2.3548e-08, 1.0559e-10,\n",
       "        4.7070e-08, 9.7461e-13])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at how likely the neural net thought the actual outputs were\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88929f87-05db-4e2d-b244-07bdc9f5b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's pretty bad, but it's ok because we haven't trained it at all yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4385fb57-2617-4559-b50b-8e528fdeafa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3de0adc1-d02f-40aa-a312-d46df4338fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22637fa3-d652-4305-924b-f0f4450a9a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.5794)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b4b09-f921-46af-b80d-71b1c1b36f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# TIDIED UP! (more respectable)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3103df59-90de-4c88-b710-8965e57948a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "990babc3-6a5b-4d0f-9f08-ed6e55999a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca7feef2-2a80-48d7-bbaf-06466a058cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32930ab5-b564-4554-81a5-7ebbbb8185bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # [32, 3, 2]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "299a18f9-5db5-45d9-a684-4ba02af2246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# TIDIED UP, 2! (even MORE respectable)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3ae2ff8-bb6a-4c55-ab20-09ea56320db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d12bf65-f375-4f22-b939-b86765d43488",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a22a244-1810-413d-a29b-a4a357b38141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "159eb64a-8aab-4dbd-b6a9-848de81fb0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # [32, 3, 2]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss\n",
    "# andrej explains a bunch of reasons to use F.cross_entropy instead of making your\n",
    "# own loss function\n",
    "# 1. efficiency -- pytorch can skip creating intermediate tensors which waste memory\n",
    "#    and can group operations together or something for more computational\n",
    "#    efficiency ??\n",
    "# 2. backward pass is more efficient because F.cross_entropy knows how to do\n",
    "#    backpropogation better or something ??\n",
    "# 3. better numerical behaviour -- you skip the bug where calling logits.exp() ends\n",
    "#    up giving you floating point infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "798aec57-8125-47bd-92dd-8c6c25c6ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# ITERATION 3, MAKE IT ITERATE! (now we're LEARNING with MACHINES)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f6c396c-61fc-43c9-a470-d8a93cd9499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2941b891-68f7-4e24-a4ed-79110a4d181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b48f3f67-0b2e-4d31-8160-7033e4b513ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "343ebbfb-369c-47ea-9f1e-ebcccf91c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8dbb9b13-391f-4389-bc02-5dd9507b527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.323263019323349\n",
      "0.3221277594566345\n",
      "0.32101118564605713\n",
      "0.319913774728775\n",
      "0.31883668899536133\n",
      "0.31778037548065186\n",
      "0.3167456090450287\n",
      "0.31573277711868286\n",
      "0.3147422671318054\n",
      "0.31377437710762024\n",
      "0.3128291666507721\n",
      "0.3119066655635834\n",
      "0.3110068142414093\n",
      "0.31012940406799316\n",
      "0.30927425622940063\n",
      "0.3084408938884735\n",
      "0.3076290190219879\n",
      "0.3068382740020752\n",
      "0.3060680031776428\n",
      "0.30531778931617737\n",
      "0.30458715558052063\n",
      "0.30387547612190247\n",
      "0.30318212509155273\n",
      "0.30250686407089233\n",
      "0.3018488585948944\n",
      "0.30120766162872314\n",
      "0.300582617521286\n",
      "0.2999732494354248\n",
      "0.2993791997432709\n",
      "0.29879969358444214\n",
      "0.29823437333106995\n",
      "0.2976827323436737\n",
      "0.2971442639827728\n",
      "0.29661864042282104\n",
      "0.2961055040359497\n",
      "0.2956041097640991\n",
      "0.29511430859565735\n",
      "0.2946355640888214\n",
      "0.29416772723197937\n",
      "0.29371026158332825\n",
      "0.29326289892196655\n",
      "0.2928251624107361\n",
      "0.2923969030380249\n",
      "0.29197773337364197\n",
      "0.29156750440597534\n",
      "0.29116570949554443\n",
      "0.2907721996307373\n",
      "0.290386825799942\n",
      "0.29000917077064514\n",
      "0.2896389365196228\n",
      "0.2892761826515198\n",
      "0.288920521736145\n",
      "0.28857165575027466\n",
      "0.28822946548461914\n",
      "0.28789374232292175\n",
      "0.2875644266605377\n",
      "0.2872411608695984\n",
      "0.2869238555431366\n",
      "0.286612331867218\n",
      "0.28630635142326355\n",
      "0.2860058546066284\n",
      "0.28571072220802307\n",
      "0.2854207456111908\n",
      "0.28513574600219727\n",
      "0.2848556935787201\n",
      "0.28458040952682495\n",
      "0.2843098044395447\n",
      "0.28404366970062256\n",
      "0.2837819755077362\n",
      "0.2835245132446289\n",
      "0.28327125310897827\n",
      "0.28302210569381714\n",
      "0.2827768921852112\n",
      "0.2825356125831604\n",
      "0.28229814767837524\n",
      "0.28206437826156616\n",
      "0.2818341553211212\n",
      "0.28160756826400757\n",
      "0.28138434886932373\n",
      "0.2811644673347473\n",
      "0.2809479236602783\n",
      "0.28073450922966003\n",
      "0.2805243730545044\n",
      "0.2803172469139099\n",
      "0.2801130712032318\n",
      "0.2799118459224701\n",
      "0.27971351146698\n",
      "0.2795180380344391\n",
      "0.2793251872062683\n",
      "0.2791351079940796\n",
      "0.278947651386261\n",
      "0.27876269817352295\n",
      "0.27858033776283264\n",
      "0.27840039134025574\n",
      "0.2782229483127594\n",
      "0.2780477702617645\n",
      "0.27787503600120544\n",
      "0.2777044177055359\n",
      "0.2775360941886902\n",
      "0.27736997604370117\n",
      "0.2772059440612793\n",
      "0.2770440876483917\n",
      "0.2768842577934265\n",
      "0.27672645449638367\n",
      "0.2765706181526184\n",
      "0.27641668915748596\n",
      "0.27626463770866394\n",
      "0.2761145234107971\n",
      "0.27596622705459595\n",
      "0.27581971883773804\n",
      "0.275674968957901\n",
      "0.27553194761276245\n",
      "0.275390625\n",
      "0.27525094151496887\n",
      "0.27511292695999146\n",
      "0.27497655153274536\n",
      "0.2748417556285858\n",
      "0.27470850944519043\n",
      "0.2745767831802368\n",
      "0.2744464874267578\n",
      "0.27431780099868774\n",
      "0.2741905152797699\n",
      "0.2740646004676819\n",
      "0.2739401161670685\n",
      "0.2738170027732849\n",
      "0.2736952304840088\n",
      "0.2735747992992401\n",
      "0.2734557092189789\n",
      "0.27333787083625793\n",
      "0.27322131395339966\n",
      "0.27310600876808167\n",
      "0.27299192547798157\n",
      "0.272879034280777\n",
      "0.2727673053741455\n",
      "0.27265676856040955\n",
      "0.2725474238395691\n",
      "0.272439181804657\n",
      "0.27233198285102844\n",
      "0.2722259759902954\n",
      "0.27212101221084595\n",
      "0.27201709151268005\n",
      "0.2719142735004425\n",
      "0.27181243896484375\n",
      "0.27171164751052856\n",
      "0.2716118097305298\n",
      "0.271512895822525\n",
      "0.2714150846004486\n",
      "0.27131813764572144\n",
      "0.27122220396995544\n",
      "0.2711271643638611\n",
      "0.27103301882743835\n",
      "0.2709397077560425\n",
      "0.2708474397659302\n",
      "0.27075597643852234\n",
      "0.27066534757614136\n",
      "0.270575612783432\n",
      "0.2704866826534271\n",
      "0.2703986167907715\n",
      "0.27031129598617554\n",
      "0.27022480964660645\n",
      "0.2701391279697418\n",
      "0.27005425095558167\n",
      "0.2699700593948364\n",
      "0.26988667249679565\n",
      "0.26980409026145935\n",
      "0.26972222328186035\n",
      "0.26964104175567627\n",
      "0.2695606052875519\n",
      "0.2694808542728424\n",
      "0.26940181851387024\n",
      "0.269323468208313\n",
      "0.26924580335617065\n",
      "0.26916879415512085\n",
      "0.26909250020980835\n",
      "0.2690168023109436\n",
      "0.2689417898654938\n",
      "0.2688674032688141\n",
      "0.26879364252090454\n",
      "0.2687205374240875\n",
      "0.2686479985713959\n",
      "0.26857608556747437\n",
      "0.2685047686100006\n",
      "0.2684340476989746\n",
      "0.26836392283439636\n",
      "0.2682943344116211\n",
      "0.26822537183761597\n",
      "0.2681569457054138\n",
      "0.26808905601501465\n",
      "0.26802173256874084\n",
      "0.26795494556427\n",
      "0.2678886950016022\n",
      "0.2678230106830597\n",
      "0.267757773399353\n",
      "0.26769310235977173\n",
      "0.26762890815734863\n",
      "0.2675652503967285\n",
      "0.2675020694732666\n",
      "0.2674393951892853\n",
      "0.26737719774246216\n",
      "0.26731550693511963\n",
      "0.2672542631626129\n",
      "0.2671935260295868\n",
      "0.2671332359313965\n",
      "0.2670733630657196\n",
      "0.2670139968395233\n",
      "0.26695504784584045\n",
      "0.2668965458869934\n",
      "0.2668384611606598\n",
      "0.26678088307380676\n",
      "0.2667236626148224\n",
      "0.2666669189929962\n",
      "0.2666105329990387\n",
      "0.2665546238422394\n",
      "0.2664991319179535\n",
      "0.26644396781921387\n",
      "0.26638928055763245\n",
      "0.26633498072624207\n",
      "0.26628100872039795\n",
      "0.26622745394706726\n",
      "0.26617431640625\n",
      "0.266121506690979\n",
      "0.2660691440105438\n",
      "0.2660171091556549\n",
      "0.26596543192863464\n",
      "0.2659141421318054\n",
      "0.26586323976516724\n",
      "0.26581260561943054\n",
      "0.2657623887062073\n",
      "0.2657124996185303\n",
      "0.26566293835639954\n",
      "0.26561373472213745\n",
      "0.265564888715744\n",
      "0.26551637053489685\n",
      "0.26546818017959595\n",
      "0.2654202878475189\n",
      "0.26537278294563293\n",
      "0.26532551646232605\n",
      "0.2652786076068878\n",
      "0.26523205637931824\n",
      "0.26518577337265015\n",
      "0.2651398181915283\n",
      "0.265094131231308\n",
      "0.2650487720966339\n",
      "0.2650037407875061\n",
      "0.2649589478969574\n",
      "0.26491448283195496\n",
      "0.2648702561855316\n",
      "0.2648264169692993\n",
      "0.26478278636932373\n",
      "0.2647394835948944\n",
      "0.2646963894367218\n",
      "0.26465362310409546\n",
      "0.2646111249923706\n",
      "0.26456886529922485\n",
      "0.26452696323394775\n",
      "0.2644851803779602\n",
      "0.26444384455680847\n",
      "0.2644026577472687\n",
      "0.26436173915863037\n",
      "0.26432105898857117\n",
      "0.2642807066440582\n",
      "0.264240562915802\n",
      "0.2642006278038025\n",
      "0.26416099071502686\n",
      "0.26412156224250793\n",
      "0.2640824019908905\n",
      "0.26404350996017456\n",
      "0.26400479674339294\n",
      "0.2639663517475128\n",
      "0.2639281153678894\n",
      "0.2638901472091675\n",
      "0.26385241746902466\n",
      "0.26381486654281616\n",
      "0.26377755403518677\n",
      "0.2637404501438141\n",
      "0.2637035846710205\n",
      "0.2636670172214508\n",
      "0.26363053917884827\n",
      "0.26359429955482483\n",
      "0.2635583281517029\n",
      "0.26352253556251526\n",
      "0.26348698139190674\n",
      "0.26345157623291016\n",
      "0.26341643929481506\n",
      "0.2633815109729767\n",
      "0.263346791267395\n",
      "0.2633122205734253\n",
      "0.26327788829803467\n",
      "0.263243705034256\n",
      "0.2632097899913788\n",
      "0.2631760239601135\n",
      "0.26314249634742737\n",
      "0.26310911774635315\n",
      "0.26307591795921326\n",
      "0.26304301619529724\n",
      "0.2630102038383484\n",
      "0.262977659702301\n",
      "0.26294517517089844\n",
      "0.2629129886627197\n",
      "0.26288095116615295\n",
      "0.2628491520881653\n",
      "0.26281753182411194\n",
      "0.2627860903739929\n",
      "0.2627548575401306\n",
      "0.26272380352020264\n",
      "0.26269298791885376\n",
      "0.2626623213291168\n",
      "0.26263192296028137\n",
      "0.26260170340538025\n",
      "0.2625717520713806\n",
      "0.2625420093536377\n",
      "0.26251253485679626\n",
      "0.26248329877853394\n",
      "0.2624543309211731\n",
      "0.2624257206916809\n",
      "0.2623973488807678\n",
      "0.2623693645000458\n",
      "0.26234179735183716\n",
      "0.2623146176338196\n",
      "0.2622879147529602\n",
      "0.2622618079185486\n",
      "0.26223617792129517\n",
      "0.2622114419937134\n",
      "0.26218727231025696\n",
      "0.26216426491737366\n",
      "0.26214200258255005\n",
      "0.2621212899684906\n",
      "0.26210159063339233\n",
      "0.26208409667015076\n",
      "0.2620677649974823\n",
      "0.2620546817779541\n",
      "0.26204293966293335\n",
      "0.26203596591949463\n",
      "0.26203039288520813\n",
      "0.2620319724082947\n",
      "0.26203465461730957\n",
      "0.2620478868484497\n",
      "0.26206162571907043\n",
      "0.2620909512042999\n",
      "0.26211875677108765\n",
      "0.26216965913772583\n",
      "0.2622150182723999\n",
      "0.26229435205459595\n",
      "0.26236018538475037\n",
      "0.2624754309654236\n",
      "0.2625628411769867\n",
      "0.26272088289260864\n",
      "0.2628272771835327\n",
      "0.26303258538246155\n",
      "0.2631493806838989\n",
      "0.2634008526802063\n",
      "0.26351186633110046\n",
      "0.26380082964897156\n",
      "0.2638838589191437\n",
      "0.2641945481300354\n",
      "0.26422667503356934\n",
      "0.264539897441864\n",
      "0.2645048201084137\n",
      "0.2648034691810608\n",
      "0.26469656825065613\n",
      "0.2649700343608856\n",
      "0.26479899883270264\n",
      "0.26504459977149963\n",
      "0.26482442021369934\n",
      "0.2650444805622101\n",
      "0.2647915184497833\n",
      "0.26499074697494507\n",
      "0.2647190988063812\n",
      "0.26490193605422974\n",
      "0.26462188363075256\n",
      "0.26479196548461914\n",
      "0.26451051235198975\n",
      "0.26467034220695496\n",
      "0.2643914818763733\n",
      "0.26454296708106995\n",
      "0.2642691731452942\n",
      "0.26441341638565063\n",
      "0.2641461491584778\n",
      "0.2642839848995209\n",
      "0.264024019241333\n",
      "0.26415619254112244\n",
      "0.26390379667282104\n",
      "0.26403066515922546\n",
      "0.26378580927848816\n",
      "0.2639077305793762\n",
      "0.2636702358722687\n",
      "0.2637876272201538\n",
      "0.26355740427970886\n",
      "0.263670414686203\n",
      "0.26344728469848633\n",
      "0.2635561525821686\n",
      "0.2633398771286011\n",
      "0.2634448707103729\n",
      "0.26323509216308594\n",
      "0.2633364796638489\n",
      "0.2631329894065857\n",
      "0.2632308900356293\n",
      "0.2630334198474884\n",
      "0.26312804222106934\n",
      "0.26293641328811646\n",
      "0.26302793622016907\n",
      "0.2628418207168579\n",
      "0.2629302740097046\n",
      "0.26274940371513367\n",
      "0.2628350853919983\n",
      "0.2626592814922333\n",
      "0.2627422511577606\n",
      "0.2625713050365448\n",
      "0.2626517117023468\n",
      "0.2624855041503906\n",
      "0.26256340742111206\n",
      "0.26240161061286926\n",
      "0.26247724890708923\n",
      "0.2623197138309479\n",
      "0.2623929977416992\n",
      "0.26223960518836975\n",
      "0.26231080293655396\n",
      "0.2621613144874573\n",
      "0.26223039627075195\n",
      "0.2620847225189209\n",
      "0.26215195655822754\n",
      "0.262009859085083\n",
      "0.2620752155780792\n",
      "0.26193666458129883\n",
      "0.2620002031326294\n",
      "0.26186493039131165\n",
      "0.2619268000125885\n",
      "0.2617947459220886\n",
      "0.26185500621795654\n",
      "0.2617260217666626\n",
      "0.2617846727371216\n",
      "0.2616586983203888\n",
      "0.26171591877937317\n",
      "0.26159271597862244\n",
      "0.2616485357284546\n",
      "0.2615280747413635\n",
      "0.26158252358436584\n",
      "0.2614647448062897\n",
      "0.26151779294013977\n",
      "0.26140257716178894\n",
      "0.2614544630050659\n",
      "0.2613416314125061\n",
      "0.26139235496520996\n",
      "0.26128190755844116\n",
      "0.2613314688205719\n",
      "0.261223167181015\n",
      "0.26127156615257263\n",
      "0.26116544008255005\n",
      "0.2612127661705017\n",
      "0.2611088156700134\n",
      "0.26115521788597107\n",
      "0.26105332374572754\n",
      "0.2610986828804016\n",
      "0.2609989047050476\n",
      "0.2610434293746948\n",
      "0.26094532012939453\n",
      "0.26098886132240295\n",
      "0.2608926296234131\n",
      "0.26093536615371704\n",
      "0.2608409523963928\n",
      "0.2608828842639923\n",
      "0.2607901096343994\n",
      "0.2608312666416168\n",
      "0.2607400715351105\n",
      "0.2607804536819458\n",
      "0.260690838098526\n",
      "0.26073047518730164\n",
      "0.2606423795223236\n",
      "0.2606813609600067\n",
      "0.2605947256088257\n",
      "0.2606329321861267\n",
      "0.26054778695106506\n",
      "0.26058533787727356\n",
      "0.26050159335136414\n",
      "0.26053857803344727\n",
      "0.26045602560043335\n",
      "0.2604924440383911\n",
      "0.26041126251220703\n",
      "0.26044708490371704\n",
      "0.26036712527275085\n",
      "0.2604024410247803\n",
      "0.260323703289032\n",
      "0.26035842299461365\n",
      "0.2602807879447937\n",
      "0.2603149712085724\n",
      "0.2602385878562927\n",
      "0.26027223467826843\n",
      "0.26019686460494995\n",
      "0.2602299749851227\n",
      "0.260155588388443\n",
      "0.26018834114074707\n",
      "0.2601151168346405\n",
      "0.26014745235443115\n",
      "0.260075181722641\n",
      "0.26010704040527344\n",
      "0.2600356936454773\n",
      "0.2600671648979187\n",
      "0.25999677181243896\n",
      "0.26002779603004456\n",
      "0.25995832681655884\n",
      "0.259988933801651\n",
      "0.2599203884601593\n",
      "0.2599506080150604\n",
      "0.2598828971385956\n",
      "0.2599128186702728\n",
      "0.25984591245651245\n",
      "0.25987544655799866\n",
      "0.25980934500694275\n",
      "0.2598385214805603\n",
      "0.25977325439453125\n",
      "0.2598021328449249\n",
      "0.25973767042160034\n",
      "0.25976622104644775\n",
      "0.2597023546695709\n",
      "0.2597305476665497\n",
      "0.25966742634773254\n",
      "0.2596953809261322\n",
      "0.25963303446769714\n",
      "0.2596607208251953\n",
      "0.2595990300178528\n",
      "0.2596263587474823\n",
      "0.2595652937889099\n",
      "0.2595923840999603\n",
      "0.25953209400177\n",
      "0.25955891609191895\n",
      "0.25949913263320923\n",
      "0.25952571630477905\n",
      "0.25946661829948425\n",
      "0.2594929337501526\n",
      "0.25943437218666077\n",
      "0.2594604790210724\n",
      "0.2594025135040283\n",
      "0.25942838191986084\n",
      "0.25937098264694214\n",
      "0.25939661264419556\n",
      "0.2593397796154022\n",
      "0.2593652009963989\n",
      "0.2593088746070862\n",
      "0.2593340277671814\n",
      "0.2592783272266388\n",
      "0.2593032121658325\n",
      "0.2592480182647705\n",
      "0.2592727541923523\n",
      "0.2592179775238037\n",
      "0.2592424750328064\n",
      "0.2591882646083832\n",
      "0.25921255350112915\n",
      "0.25915881991386414\n",
      "0.2591829299926758\n",
      "0.25912970304489136\n",
      "0.25915366411209106\n",
      "0.25910085439682007\n",
      "0.25912460684776306\n",
      "0.2590722143650055\n",
      "0.25909578800201416\n",
      "0.25904381275177\n",
      "0.2590671479701996\n",
      "0.2590157091617584\n",
      "0.2590389549732208\n",
      "0.2589878737926483\n",
      "0.259010910987854\n",
      "0.2589602470397949\n",
      "0.2589830756187439\n",
      "0.258932888507843\n",
      "0.25895553827285767\n",
      "0.25890570878982544\n",
      "0.25892817974090576\n",
      "0.25887879729270935\n",
      "0.25890111923217773\n",
      "0.25885215401649475\n",
      "0.2588743567466736\n",
      "0.2588256895542145\n",
      "0.258847713470459\n",
      "0.2587994337081909\n",
      "0.2588213086128235\n",
      "0.2587732970714569\n",
      "0.25879502296447754\n",
      "0.2587474286556244\n",
      "0.2587689459323883\n",
      "0.2587217390537262\n",
      "0.25874316692352295\n",
      "0.2586963474750519\n",
      "0.2587175965309143\n",
      "0.25867098569869995\n",
      "0.2586921453475952\n",
      "0.2586459517478943\n",
      "0.2586669325828552\n",
      "0.25862109661102295\n",
      "0.25864195823669434\n",
      "0.2585964798927307\n",
      "0.25861719250679016\n",
      "0.25857195258140564\n",
      "0.2585925757884979\n",
      "0.2585476338863373\n",
      "0.25856807827949524\n",
      "0.25852349400520325\n",
      "0.25854384899139404\n",
      "0.2584995627403259\n",
      "0.2585197985172272\n",
      "0.25847581028938293\n",
      "0.25849589705467224\n",
      "0.2584521770477295\n",
      "0.25847214460372925\n",
      "0.25842875242233276\n",
      "0.2584485709667206\n",
      "0.2584054470062256\n",
      "0.2584250867366791\n",
      "0.25838226079940796\n",
      "0.2584018111228943\n",
      "0.25835931301116943\n",
      "0.258378803730011\n",
      "0.25833648443222046\n",
      "0.25835585594177246\n",
      "0.2583138942718506\n",
      "0.2583330571651459\n",
      "0.25829142332077026\n",
      "0.258310467004776\n",
      "0.2582690715789795\n",
      "0.25828802585601807\n",
      "0.25824686884880066\n",
      "0.2582656741142273\n",
      "0.2582247853279114\n",
      "0.25824350118637085\n",
      "0.25820282101631165\n",
      "0.25822144746780396\n",
      "0.258181095123291\n",
      "0.25819963216781616\n",
      "0.2581595182418823\n",
      "0.2581779360771179\n",
      "0.2581380009651184\n",
      "0.2581562399864197\n",
      "0.25811660289764404\n",
      "0.2581346333026886\n",
      "0.25809529423713684\n",
      "0.2581133544445038\n",
      "0.25807419419288635\n",
      "0.25809213519096375\n",
      "0.2580532133579254\n",
      "0.25807103514671326\n",
      "0.25803232192993164\n",
      "0.2580500543117523\n",
      "0.2580115795135498\n",
      "0.25802916288375854\n",
      "0.25799092650413513\n",
      "0.2580083906650543\n",
      "0.2579704225063324\n",
      "0.25798776745796204\n",
      "0.2579500079154968\n",
      "0.2579672932624817\n",
      "0.25792980194091797\n",
      "0.2579469680786133\n",
      "0.2579095959663391\n",
      "0.25792667269706726\n",
      "0.25788962841033936\n",
      "0.2579065263271332\n",
      "0.2578697204589844\n",
      "0.25788646936416626\n",
      "0.2578498125076294\n",
      "0.2578665614128113\n",
      "0.2578301429748535\n",
      "0.25784680247306824\n",
      "0.25781071186065674\n",
      "0.2578272223472595\n",
      "0.25779128074645996\n",
      "0.2578076422214508\n",
      "0.25777187943458557\n",
      "0.25778815150260925\n",
      "0.2577526569366455\n",
      "0.2577688694000244\n",
      "0.257733553647995\n",
      "0.25774961709976196\n",
      "0.25771456956863403\n",
      "0.2577305734157562\n",
      "0.25769567489624023\n",
      "0.25771158933639526\n",
      "0.257676899433136\n",
      "0.25769272446632385\n",
      "0.25765830278396606\n",
      "0.2576740086078644\n",
      "0.25763973593711853\n",
      "0.2576552927494049\n",
      "0.25762122869491577\n",
      "0.2576367259025574\n",
      "0.25760290026664734\n",
      "0.257618248462677\n",
      "0.25758451223373413\n",
      "0.257599800825119\n",
      "0.2575663626194\n",
      "0.25758159160614014\n",
      "0.25754833221435547\n",
      "0.25756341218948364\n",
      "0.2575303316116333\n",
      "0.2575453221797943\n",
      "0.2575124204158783\n",
      "0.25752732157707214\n",
      "0.25749465823173523\n",
      "0.25750941038131714\n",
      "0.25747692584991455\n",
      "0.25749170780181885\n",
      "0.2574594020843506\n",
      "0.25747397541999817\n",
      "0.2574418783187866\n",
      "0.2574564218521118\n",
      "0.2574245035648346\n",
      "0.25743892788887024\n",
      "0.25740718841552734\n",
      "0.2574214041233063\n",
      "0.25738993287086487\n",
      "0.2574041783809662\n",
      "0.25737279653549194\n",
      "0.2573869526386261\n",
      "0.2573557198047638\n",
      "0.2573697865009308\n",
      "0.2573387920856476\n",
      "0.2573527693748474\n",
      "0.2573219835758209\n",
      "0.2573358416557312\n",
      "0.2573052942752838\n",
      "0.25731900334358215\n",
      "0.25728854537010193\n",
      "0.2573022246360779\n",
      "0.25727200508117676\n",
      "0.25728553533554077\n",
      "0.257255494594574\n",
      "0.2572689652442932\n",
      "0.2572390139102936\n",
      "0.25725239515304565\n",
      "0.2572227418422699\n",
      "0.2572360038757324\n",
      "0.25720643997192383\n",
      "0.25721970200538635\n",
      "0.2571903467178345\n",
      "0.25720348954200745\n",
      "0.2571742832660675\n",
      "0.2571873366832733\n",
      "0.2571583390235901\n",
      "0.25717127323150635\n",
      "0.25714248418807983\n",
      "0.25715529918670654\n",
      "0.2571265697479248\n",
      "0.2571393549442291\n",
      "0.2571108639240265\n",
      "0.25712352991104126\n",
      "0.2570951581001282\n",
      "0.25710779428482056\n",
      "0.2570795714855194\n",
      "0.2570921778678894\n",
      "0.2570641338825226\n",
      "0.2570765018463135\n",
      "0.257048636674881\n",
      "0.2570609748363495\n",
      "0.25703325867652893\n",
      "0.25704559683799744\n",
      "0.2570180296897888\n",
      "0.25703027844429016\n",
      "0.25700294971466064\n",
      "0.25701504945755005\n",
      "0.2569878399372101\n",
      "0.2569999396800995\n",
      "0.2569728493690491\n",
      "0.25698480010032654\n",
      "0.25695791840553284\n",
      "0.2569698095321655\n",
      "0.256943017244339\n",
      "0.2569548189640045\n",
      "0.2569282352924347\n",
      "0.25693991780281067\n",
      "0.2569134831428528\n",
      "0.256925106048584\n",
      "0.25689876079559326\n",
      "0.2569103240966797\n",
      "0.25688421726226807\n",
      "0.25689569115638733\n",
      "0.25686970353126526\n",
      "0.25688111782073975\n",
      "0.2568552792072296\n",
      "0.25686657428741455\n",
      "0.25684085488319397\n",
      "0.2568521201610565\n",
      "0.25682660937309265\n",
      "0.25683775544166565\n",
      "0.25681236386299133\n",
      "0.25682348012924194\n",
      "0.2567982077598572\n",
      "0.2568092346191406\n",
      "0.2567841112613678\n",
      "0.25679507851600647\n",
      "0.2567701041698456\n",
      "0.25678104162216187\n",
      "0.2567562460899353\n",
      "0.25676703453063965\n",
      "0.25674232840538025\n",
      "0.2567530572414398\n",
      "0.25672850012779236\n",
      "0.25673916935920715\n",
      "0.256714791059494\n",
      "0.2567253112792969\n",
      "0.2567010521888733\n",
      "0.25671160221099854\n",
      "0.2566875219345093\n",
      "0.2566978931427002\n",
      "0.2566739022731781\n",
      "0.25668421387672424\n",
      "0.25666043162345886\n",
      "0.2566707134246826\n",
      "0.256646990776062\n",
      "0.256657212972641\n",
      "0.2566336393356323\n",
      "0.25664380192756653\n",
      "0.2566203773021698\n",
      "0.25663045048713684\n",
      "0.2566071152687073\n",
      "0.25661712884902954\n",
      "0.2565939724445343\n",
      "0.2566038966178894\n",
      "0.25658082962036133\n",
      "0.25659075379371643\n",
      "0.25656792521476746\n",
      "0.25657767057418823\n",
      "0.2565549314022064\n",
      "0.2565646767616272\n",
      "0.25654205679893494\n",
      "0.25655171275138855\n",
      "0.25652921199798584\n",
      "0.2565388083457947\n",
      "0.2565165162086487\n",
      "0.25652602314949036\n",
      "0.25650376081466675\n",
      "0.25651317834854126\n",
      "0.2564910352230072\n",
      "0.2565004229545593\n",
      "0.2564784586429596\n",
      "0.25648781657218933\n",
      "0.2564659118652344\n",
      "0.25647521018981934\n",
      "0.25645342469215393\n",
      "0.25646263360977173\n",
      "0.25644105672836304\n",
      "0.25645020604133606\n",
      "0.25642871856689453\n",
      "0.25643783807754517\n",
      "0.2564164400100708\n",
      "0.2564254701137543\n",
      "0.25640419125556946\n",
      "0.25641316175460815\n",
      "0.2563920319080353\n",
      "0.2564009726047516\n",
      "0.2563799023628235\n",
      "0.256388783454895\n",
      "0.25636792182922363\n",
      "0.2563766539096832\n",
      "0.256355881690979\n",
      "0.2563645839691162\n",
      "0.25634390115737915\n",
      "0.25635257363319397\n",
      "0.2563319802284241\n",
      "0.2563405930995941\n",
      "0.25632017850875854\n",
      "0.2563287019729614\n",
      "0.256308376789093\n",
      "0.2563168704509735\n",
      "0.2562966048717499\n",
      "0.256305068731308\n",
      "0.2562849521636963\n",
      "0.2562933564186096\n",
      "0.25627338886260986\n",
      "0.25628170371055603\n",
      "0.2562618553638458\n",
      "0.25627005100250244\n",
      "0.2562503218650818\n",
      "0.256258487701416\n",
      "0.25623881816864014\n",
      "0.25624704360961914\n",
      "0.2562274932861328\n",
      "0.25623559951782227\n",
      "0.2562161982059479\n",
      "0.25622427463531494\n",
      "0.25620487332344055\n",
      "0.25621283054351807\n",
      "0.256193608045578\n",
      "0.2562015652656555\n",
      "0.2561824321746826\n",
      "0.25619035959243774\n",
      "0.25617140531539917\n",
      "0.2561792731285095\n",
      "0.25616031885147095\n",
      "0.25616809725761414\n",
      "0.2561492621898651\n",
      "0.2561570107936859\n",
      "0.25613829493522644\n",
      "0.25614601373672485\n",
      "0.25612735748291016\n",
      "0.2561349868774414\n",
      "0.25611644983291626\n",
      "0.25612401962280273\n",
      "0.2561056315898895\n",
      "0.2561131417751312\n",
      "0.2560948133468628\n",
      "0.2561022937297821\n",
      "0.2560840845108032\n",
      "0.256091445684433\n",
      "0.25607335567474365\n",
      "0.2560807466506958\n",
      "0.25606274604797363\n",
      "0.2560700476169586\n",
      "0.256052166223526\n",
      "0.2560594081878662\n",
      "0.25604158639907837\n",
      "0.2560487985610962\n",
      "0.25603100657463074\n",
      "0.25603821873664856\n",
      "0.25602054595947266\n",
      "0.2560276985168457\n",
      "0.25601014494895935\n",
      "0.25601726770401\n",
      "0.2559998333454132\n",
      "0.2560068964958191\n",
      "0.25598955154418945\n",
      "0.25599658489227295\n",
      "0.25597935914993286\n",
      "0.2559862732887268\n",
      "0.2559691369533539\n",
      "0.2559760808944702\n",
      "0.25595900416374207\n",
      "0.25596582889556885\n",
      "0.25594887137413025\n",
      "0.25595569610595703\n",
      "0.2559387683868408\n",
      "0.2559455931186676\n",
      "0.25592875480651855\n",
      "0.25593551993370056\n",
      "0.2559187412261963\n",
      "0.2559254467487335\n",
      "0.2559088170528412\n",
      "0.25591546297073364\n",
      "0.2558988928794861\n",
      "0.25590553879737854\n",
      "0.25588905811309814\n",
      "0.25589561462402344\n",
      "0.2558792531490326\n",
      "0.2558857798576355\n",
      "0.2558695077896118\n",
      "0.25587594509124756\n",
      "0.25585976243019104\n",
      "0.2558661997318268\n",
      "0.25585007667541504\n",
      "0.255856454372406\n",
      "0.2558404505252838\n",
      "0.25584685802459717\n",
      "0.25583088397979736\n",
      "0.2558372914791107\n",
      "0.2558213770389557\n",
      "0.2558276951313019\n",
      "0.2558118999004364\n",
      "0.2558181583881378\n",
      "0.2558024227619171\n",
      "0.2558087110519409\n",
      "0.2557930648326874\n",
      "0.25579920411109924\n",
      "0.25578370690345764\n",
      "0.2557898461818695\n",
      "0.2557743191719055\n",
      "0.25578048825263977\n",
      "0.25576508045196533\n",
      "0.2557711601257324\n",
      "0.255755752325058\n",
      "0.2557618021965027\n",
      "0.2557465136051178\n",
      "0.2557525336742401\n",
      "0.2557373642921448\n",
      "0.2557433545589447\n",
      "0.25572824478149414\n",
      "0.2557341754436493\n",
      "0.2557191848754883\n",
      "0.25572511553764343\n",
      "0.2557101845741272\n",
      "0.2557160258293152\n",
      "0.25570112466812134\n",
      "0.2557069957256317\n",
      "0.25569215416908264\n",
      "0.25569796562194824\n",
      "0.2556832730770111\n",
      "0.25568902492523193\n",
      "0.2556743919849396\n",
      "0.2556800842285156\n",
      "0.25566548109054565\n",
      "0.2556711435317993\n",
      "0.2556566298007965\n",
      "0.2556622624397278\n",
      "0.25564780831336975\n",
      "0.255653440952301\n",
      "0.2556390166282654\n",
      "0.25564464926719666\n",
      "0.2556302845478058\n",
      "0.2556358575820923\n",
      "0.25562161207199097\n",
      "0.2556270956993103\n",
      "0.25561296939849854\n",
      "0.2556183636188507\n",
      "0.2556042969226837\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    # forward pass\n",
    "    emb = C[X] # [32, 3, 2]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bd446-8dc6-4696-ab55-7cb9ffa70223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
