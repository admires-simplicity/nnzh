{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552aa5a8-1c05-4ab5-9653-656cb4c89428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name generation model\n",
    "# based on this paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f694b08d-9534-4b04-ab57-5ad37b89c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e918d40a-6dd5-4d61-a6f5-ea7d05a999df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read  in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe49c448-a633-4076-9353-5d244ea8a0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b384bf-c57e-4dce-ae24-6c9f51f7b0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5004cf44-226d-4c23-a423-280db68ee8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf87248-14c8-4fc2-ac4e-d46d3c345409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c3003e-1b6c-40db-a50c-438b6bfd9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9904fee-f22b-4395-8502-074bb204d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed input in lower dimensional space\n",
    "# original paper embeds 17,000 words in 30 dimensional space\n",
    "# we have 27 possible input characters. let's try a 2 dimensional space.\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab79473-5ba9-49f4-885d-dce3acfae1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6529, -0.5013],\n",
       "        [ 0.7260, -1.0445],\n",
       "        [ 1.2410,  0.3941]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7])] # using list or tensor as index instead of number gives us a tensor of the respective values in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a294cf2-6958-42ed-b7ec-98fb78f11ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 0.6529, -0.5013]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 0.6529, -0.5013],\n",
       "         [-1.1747, -0.1716]],\n",
       "\n",
       "        [[ 0.6529, -0.5013],\n",
       "         [-1.1747, -0.1716],\n",
       "         [-1.1747, -0.1716]],\n",
       "\n",
       "        [[-1.1747, -0.1716],\n",
       "         [-1.1747, -0.1716],\n",
       "         [ 0.3782,  0.7977]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 0.2579,  2.1787]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 0.2579,  2.1787],\n",
       "         [ 2.6669,  0.9808]],\n",
       "\n",
       "        [[ 0.2579,  2.1787],\n",
       "         [ 2.6669,  0.9808],\n",
       "         [ 0.9003, -0.4425]],\n",
       "\n",
       "        [[ 2.6669,  0.9808],\n",
       "         [ 0.9003, -0.4425],\n",
       "         [ 0.8257,  0.4894]],\n",
       "\n",
       "        [[ 0.9003, -0.4425],\n",
       "         [ 0.8257,  0.4894],\n",
       "         [ 0.9003, -0.4425]],\n",
       "\n",
       "        [[ 0.8257,  0.4894],\n",
       "         [ 0.9003, -0.4425],\n",
       "         [ 0.3782,  0.7977]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 0.3782,  0.7977]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 0.3782,  0.7977],\n",
       "         [ 0.8257,  0.4894]],\n",
       "\n",
       "        [[ 0.3782,  0.7977],\n",
       "         [ 0.8257,  0.4894],\n",
       "         [ 0.3782,  0.7977]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 0.9003, -0.4425]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 0.9003, -0.4425],\n",
       "         [-0.3051,  0.4799]],\n",
       "\n",
       "        [[ 0.9003, -0.4425],\n",
       "         [-0.3051,  0.4799],\n",
       "         [ 0.3782,  0.7977]],\n",
       "\n",
       "        [[-0.3051,  0.4799],\n",
       "         [ 0.3782,  0.7977],\n",
       "         [ 0.6223, -1.8771]],\n",
       "\n",
       "        [[ 0.3782,  0.7977],\n",
       "         [ 0.6223, -1.8771],\n",
       "         [ 0.6529, -0.5013]],\n",
       "\n",
       "        [[ 0.6223, -1.8771],\n",
       "         [ 0.6529, -0.5013],\n",
       "         [ 2.6669,  0.9808]],\n",
       "\n",
       "        [[ 0.6529, -0.5013],\n",
       "         [ 2.6669,  0.9808],\n",
       "         [ 2.6669,  0.9808]],\n",
       "\n",
       "        [[ 2.6669,  0.9808],\n",
       "         [ 2.6669,  0.9808],\n",
       "         [ 0.3782,  0.7977]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [ 1.0715,  0.6954],\n",
       "         [-0.3051,  0.4799]],\n",
       "\n",
       "        [[ 1.0715,  0.6954],\n",
       "         [-0.3051,  0.4799],\n",
       "         [ 0.2579,  2.1787]],\n",
       "\n",
       "        [[-0.3051,  0.4799],\n",
       "         [ 0.2579,  2.1787],\n",
       "         [ 0.2501, -1.0460]],\n",
       "\n",
       "        [[ 0.2579,  2.1787],\n",
       "         [ 0.2501, -1.0460],\n",
       "         [ 1.8043, -0.8059]],\n",
       "\n",
       "        [[ 0.2501, -1.0460],\n",
       "         [ 1.8043, -0.8059],\n",
       "         [ 0.9003, -0.4425]],\n",
       "\n",
       "        [[ 1.8043, -0.8059],\n",
       "         [ 0.9003, -0.4425],\n",
       "         [ 0.3782,  0.7977]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # we can also index with multidimensional integers\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fbd16b4-0eca-492f-844c-1184a67e2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76af0273-b13a-4991-ac7f-4f86f62fa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "W1 = torch.randn((6, 100)) # weights\n",
    "# 6 x 100 because:\n",
    "# (number of inputs == 6 == embedding dimensions (2) x n-embeddings per input (3))\n",
    "#   x \n",
    "# (number of neurons in this layer == some arbitrary amount of neurons (100))\n",
    "b1 = torch.randn(100) # biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad715e39-e0bb-4802-8e11-e8bd74fcea65",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m# what we want to do\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1 + b1 # what we want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf27005-cc6d-4d0b-8341-cbb6800957a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but it doesn't work because each element in emb has size (3x2) instead of size 6.\n",
    "# we need to smush these together somehow.\n",
    "# there are multiple ways to acheive this depending on exact requirements\n",
    "# we'll use torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4244c02-0362-4a0b-87a2-fe6741ede611",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
    "# this will do it, but it's ugly because it's hard-coded.\n",
    "# what if we wanted to change the dimensions of emb from e.g. (_, 3, _) to (_, M, _) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8297fa8-0e83-46db-b304-72030fe44e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans: use torch.unbind\n",
    "#torch.unbind(emb, 1) # == [emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]]\n",
    "torch.cat(torch.unbind(emb, 1), 1)\n",
    "# unfortunately, this is SUPA INEFFICIENT, because torch.cat will copy everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e0659-255f-4c07-a471-0c1166417e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even BETTER + EFFICIENT ans: use tensor.view!\n",
    "# changes how tensor is INDEXED instead of STORED. efficient!\n",
    "# changes storage offset, strides, and shapes\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162c283-a869-4ea7-9f80-c76bae5755c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1)\n",
    "# activations of inputs emb (h for \"hidden states\")\n",
    "# could do emb.view(-1, 6) and pytorch would figure out what the first dimension needs to be in order to make the size work\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4a76e-31d6-4bda-baed-82542573ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe14e3-9712-433c-990c-44dbc901d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make sure the expression \"emb.view(emb.shape[0], 6) @ W1 + b1\"\n",
    "# is broadcasting correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c134812-2567-4e1d-a324-ac789d7b2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(emb.view(emb.shape[0], 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e6050-2795-418d-a55c-5fdd19e33c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff1e7b-ca4e-428d-b876-559dc8e240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align on right, make missing dimensions size 1, then copy all dimensions of size 1\n",
    "# 32 100    32 100    32 100\n",
    "#    100 ->  1 100 -> 32 100\n",
    "# so it's correct (note: in case this looks wrong because we know \n",
    "#                        matrix multiplication needs (N, M) x (M, O) dimensions,\n",
    "#                        remember we're not doing a multiplication on these\n",
    "#                        two values, we're doing an addition, so we actually\n",
    "#                        do want identical sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512fe1c-42bb-4274-8962-83ca09ec6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer\n",
    "W2 = torch.randn((100, 27)) # input size -> 27 characters\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5428ae0-f5ed-48c5-8b2a-cd5d1805ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f5f4a-3f12-4fa0-9b08-c6f3c0b298e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a158efa-6a0f-4ba6-a4be-5ffee43f5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61aff17-6da2-4c86-8e05-c6129bb05068",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79288c9-6da3-4e77-ad83-f60f716986e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f85842-098b-47a0-8fb5-144857217217",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65748a42-446c-4dec-af52-5fab6b9469eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75948ba-eba6-4ad8-bf6f-4a06dd8ecb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at how likely the neural net thought the actual outputs were\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88929f87-05db-4e2d-b244-07bdc9f5b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's pretty bad, but it's ok because we haven't trained it at all yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385fb57-2617-4559-b50b-8e528fdeafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0adc1-d02f-40aa-a312-d46df4338fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22637fa3-d652-4305-924b-f0f4450a9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b4b09-f921-46af-b80d-71b1c1b36f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# TIDIED UP! (more respectable)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103df59-90de-4c88-b710-8965e57948a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990babc3-6a5b-4d0f-9f08-ed6e55999a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7feef2-2a80-48d7-bbaf-06466a058cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32930ab5-b564-4554-81a5-7ebbbb8185bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # [32, 3, 2]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a18f9-5db5-45d9-a684-4ba02af2246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# TIDIED UP, 2! (even MORE respectable)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2ff8-bb6a-4c55-ab20-09ea56320db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12bf65-f375-4f22-b939-b86765d43488",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22a244-1810-413d-a29b-a4a357b38141",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eb64a-8aab-4dbd-b6a9-848de81fb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # [32, 3, 2]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss\n",
    "# andrej explains a bunch of reasons to use F.cross_entropy instead of making your\n",
    "# own loss function\n",
    "# 1. efficiency -- pytorch can skip creating intermediate tensors which waste memory\n",
    "#    and can group operations together or something for more computational\n",
    "#    efficiency ??\n",
    "# 2. backward pass is more efficient because F.cross_entropy knows how to do\n",
    "#    backpropogation better or something ??\n",
    "# 3. better numerical behaviour -- you skip the bug where calling logits.exp() ends\n",
    "#    up giving you floating point infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798aec57-8125-47bd-92dd-8c6c25c6ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# ITERATION 3, MAKE IT ITERATE! (now we're LEARNING with MACHINES)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c396c-61fc-43c9-a470-d8a93cd9499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941b891-68f7-4e24-a4ed-79110a4d181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f3f67-0b2e-4d31-8160-7033e4b513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ebbfb-369c-47ea-9f1e-ebcccf91c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb9b13-391f-4389-bc02-5dd9507b527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    emb = C[X] # [32, 3, 2]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bd446-8dc6-4696-ab55-7cb9ffa70223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the actual highest likelihood predictions\n",
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 4 -- FULL DATASET TIME!\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed724eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851abb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e038938",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b845f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):   \n",
    "    # forward pass\n",
    "    #emb = C[X] # [32, 3, 2]\n",
    "    # this works but is slow. let's use random subsets of the data to train instead in order to have faster\n",
    "    # training cycles\n",
    "    \n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    emb = C[X[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example mini batch\n",
    "torch.randint(0, X.shape[0], (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff20e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the loss on all the examples\n",
    "emb = C[X]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 5\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ebe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's learn how to LEARN a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # lre -- learning rate exponents\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[X[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track learning rate stats\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so 10 ** -1.0 or 0.1 is a pretty good learning rate\n",
    "# ... why exactly??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef99f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the loss on all the examples\n",
    "emb = C[X]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 6\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f02abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2fc8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24192525446414948\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[X[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "728eaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after doing a lot of iterations at a certain learning rate, \"people like to\" do\n",
    "# what's called learning rate decay, i.e. lowering learning rate e.g. by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d2f6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3218, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the loss on all the examples\n",
    "emb = C[X]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef05b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks good, because we got a much lower loss (2.28 compared to 2.45 (!))\n",
    "# however when we use increasingly complex models there becomes a greater danger of overfitting\n",
    "# therefore we should split the data into multiple batches:\n",
    "# 1. training split      80% -- to optimize parameters of the model\n",
    "# 2. dev/validate split  10% -- to optimize hyperparameters (e.g. size of hidden layer, size of embedding, etc.)\n",
    "# 3. test split          10% -- to evaluate performance at the very end (to avoid overfitting)\n",
    "# (roughly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bb0781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 6 -- TRAIN/TEST SPLIT (OH YEAH, TITLES ARE BACK!)\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75f46ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2fdb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.int64, torch.Size([182625]), torch.int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Xtr.dtype, Ytr.shape, Ytr.dtype   # new dataset attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d6f08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b0895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.031980514526367\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5aafc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5639, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29c2fce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5693, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be7d3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we see the dev and training loss are both roughly equal.\n",
    "# we're not overfitting.\n",
    "# according to Andrej, we're actually underfitting\n",
    "# so let's BEEF up those parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af378008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3855, -0.3094],\n",
       "        [ 0.1915, -0.3524],\n",
       "        [ 0.0054, -0.2830],\n",
       "        [-0.0558, -0.3448],\n",
       "        [-0.1479, -0.3297],\n",
       "        [ 0.1020, -0.3132],\n",
       "        [-0.1356, -0.3468],\n",
       "        [ 0.4918,  2.3182],\n",
       "        [-0.1914, -0.2304],\n",
       "        [-0.0098, -0.3318],\n",
       "        [-0.2634, -0.3320],\n",
       "        [-0.1862, -0.3411],\n",
       "        [ 0.3739,  2.1840],\n",
       "        [-0.2378, -0.2958],\n",
       "        [-0.1341, -0.1909],\n",
       "        [ 0.1308, -0.3527],\n",
       "        [ 0.3356,  2.0098],\n",
       "        [-0.3106, -0.5611],\n",
       "        [-0.2049, -0.3302],\n",
       "        [-0.0267, -0.4137],\n",
       "        [-0.1287, -0.4183],\n",
       "        [ 0.5165,  2.2625],\n",
       "        [-0.2649, -0.3766],\n",
       "        [-0.2109, -0.3993],\n",
       "        [-0.1846, -0.2327],\n",
       "        [-0.0642, -0.2249],\n",
       "        [-0.2041, -0.3225]], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97b9fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 6 -- MORE PARAMETERS\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75fd0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((6, 300), generator=g) # weights\n",
    "b1 = torch.randn(300, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((300, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# easiest way to increase the size of the nerual net is to increase the size of the hidden layer (27,100) -> (27,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c974aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10281"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "815a78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lri = []\n",
    "#lossi = []\n",
    "#stepi = []\n",
    "\n",
    "for i in range(10000):   \n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15a845d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5198, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f089aa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5293, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]    \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27de6dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39f90786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iteration 7 -- MORE (MORE) PARAMETERS\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a929eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we're going to increase the size of the embeddings...\n",
    "# (and slightly decrease the size of the hidden layer)\n",
    "\n",
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((30, 200), generator=g) # weights\n",
    "b1 = torch.randn(200, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "946784ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58c45496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lri = []\n",
    "#lossi = []\n",
    "#stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7fd4aa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):   \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (256,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7f41deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8df2b413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1912, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3c926ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1667, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]    \n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00327e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING FROM THE MODEL !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3f61ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "mayanniee.\n",
      "mad.\n",
      "rylle.\n",
      "emmancendraek.\n",
      "adelynnelin.\n",
      "shy.\n",
      "jen.\n",
      "edennestanaraelynn.\n",
      "houra.\n",
      "noshibergihimiest.\n",
      "jair.\n",
      "jenslen.\n",
      "pulfzun.\n",
      "macdariyah.\n",
      "faeha.\n",
      "kaysh.\n",
      "samyah.\n",
      "hal.\n",
      "salynn.\n",
      "juluan.\n",
      "leouren.\n",
      "cre.\n",
      "kaveaisten.\n",
      "adi.\n",
      "fen.\n",
      "oewen.\n",
      "zorie.\n",
      "samuey.\n",
      "con.\n",
      "reon.\n",
      "isa.\n",
      "iri.\n",
      "evon.\n",
      "walla.\n",
      "ortaraszin.\n",
      "desist.\n",
      "alingt.\n",
      "dabilin.\n",
      "aimellakeyanni.\n",
      "sxavin.\n",
      "damariennccayne.\n",
      "aud.\n",
      "aiwe.\n",
      "dah.\n",
      "virley.\n",
      "jayerancoo.\n",
      "grellya.\n",
      "iimarilon.\n",
      "ellah.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba07e7-3a3d-4b9e-a01e-c04e56a2beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best test loss from the video is 2.1701\n",
    "# exercise to reader is get a better loss using this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d34461-64d0-45a4-aef2-214151e08c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 8\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8d776920-278f-4fe0-846d-cee3f3e77c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we're going to increase the size of the embeddings...\n",
    "# (and slightly decrease the size of the hidden layer)\n",
    "\n",
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 4), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((12, 300), generator=g) # weights\n",
    "b1 = torch.randn(300, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((300, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3182de6a-f26a-495a-aada-f68be1d69f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12135"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "55146fa2-e378-40d5-b77a-da1b8b1ecd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.115422010421753\n",
      "1000: 2.2850892543792725\n",
      "2000: 2.448554515838623\n",
      "3000: 2.2070627212524414\n",
      "4000: 2.0405824184417725\n",
      "5000: 2.063225507736206\n",
      "6000: 2.068718910217285\n",
      "7000: 2.4837722778320312\n",
      "8000: 2.2944014072418213\n",
      "9000: 2.245450973510742\n",
      "10000: 2.367769718170166\n",
      "11000: 2.3223841190338135\n",
      "12000: 2.012387275695801\n",
      "13000: 1.904250144958496\n",
      "14000: 2.2287001609802246\n",
      "15000: 2.007561206817627\n",
      "16000: 2.0154168605804443\n",
      "17000: 2.323148012161255\n",
      "18000: 2.0671329498291016\n",
      "19000: 2.1220552921295166\n",
      "20000: 2.242379665374756\n",
      "21000: 2.3672664165496826\n",
      "22000: 2.0212714672088623\n",
      "23000: 2.203503370285034\n",
      "24000: 2.4236955642700195\n",
      "25000: 1.9961293935775757\n",
      "26000: 2.050748586654663\n",
      "27000: 2.0382094383239746\n",
      "28000: 2.001460313796997\n",
      "29000: 2.073377847671509\n",
      "30000: 2.3961193561553955\n",
      "31000: 2.2380309104919434\n",
      "32000: 2.327364683151245\n",
      "33000: 2.3748645782470703\n",
      "34000: 2.3117423057556152\n",
      "35000: 2.3180580139160156\n",
      "36000: 2.0150034427642822\n",
      "37000: 2.0387368202209473\n",
      "38000: 2.348905086517334\n",
      "39000: 2.130556583404541\n",
      "40000: 2.049065113067627\n",
      "41000: 2.2776763439178467\n",
      "42000: 2.187685012817383\n",
      "43000: 2.3770017623901367\n",
      "44000: 2.0712361335754395\n",
      "45000: 2.2530932426452637\n",
      "46000: 1.825695276260376\n",
      "47000: 2.255354404449463\n",
      "48000: 1.9639002084732056\n",
      "49000: 2.289019823074341\n"
     ]
    }
   ],
   "source": [
    "for i in range(50000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (64,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 12) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.001\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "41818cff-f2f9-4029-ae4f-f38f80b3800f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2189, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 12) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258b3e6-69a5-4909-b710-e47ca513cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best loss I seem to be able to get is 2.2189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9f47cf42-d5f6-45fb-b25a-0bd6db2de67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "mayanniee.\n",
      "med.\n",
      "ryla.\n",
      "rethrelengrlee.\n",
      "aderydaelin.\n",
      "shyone.\n",
      "leigh.\n",
      "estanar.\n",
      "kayzioh.\n",
      "kamin.\n",
      "shubrrgyni.\n",
      "jest.\n",
      "jair.\n",
      "jelionn.\n",
      "paitoun.\n",
      "mace.\n",
      "ryyah.\n",
      "faeh.\n",
      "yuve.\n",
      "myonnyamihalina.\n",
      "yansun.\n",
      "zakelveuren.\n",
      "cre.\n",
      "kiveaon.\n",
      "marid.\n",
      "jahnise.\n",
      "ban.\n",
      "prick.\n",
      "amuez.\n",
      "con.\n",
      "reon.\n",
      "isa.\n",
      "iri.\n",
      "evondwhlan.\n",
      "ortaraszin.\n",
      "desiah.\n",
      "alingtelvissivia.\n",
      "meliaketarriy.\n",
      "xavin.\n",
      "damaitenickarionaud.\n",
      "aive.\n",
      "dih.\n",
      "virle.\n",
      "ajalena.\n",
      "moiah.\n",
      "rictaviiah.\n",
      "glon.\n",
      "ethayderlon.\n",
      "jairy.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad71513-729f-4733-a207-2684ba165214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 8\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "98d723ba-c962-4244-9096-74efedf46492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we're going to increase the size of the embeddings...\n",
    "# (and slightly decrease the size of the hidden layer)\n",
    "\n",
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((30, 150), generator=g) # weights\n",
    "b1 = torch.randn(150, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((150, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f24c03ab-106c-42f4-a695-48533603a2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8997"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "089b69ed-f27a-4f80-9d32-3a65de1dfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "197061c6-998a-4d18-9710-19acd2685247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.1193010807037354\n",
      "100: 2.1520023345947266\n",
      "200: 2.204747200012207\n",
      "300: 2.1593122482299805\n",
      "400: 2.093663215637207\n",
      "500: 2.0906312465667725\n",
      "600: 2.178438663482666\n",
      "700: 2.159015417098999\n",
      "800: 2.092186689376831\n",
      "900: 2.1526906490325928\n",
      "1000: 2.0854556560516357\n",
      "1100: 2.232177257537842\n",
      "1200: 2.12353515625\n",
      "1300: 2.157930612564087\n",
      "1400: 2.150251626968384\n",
      "1500: 2.0945663452148438\n",
      "1600: 2.084001064300537\n",
      "1700: 2.0780463218688965\n",
      "1800: 2.2673988342285156\n",
      "1900: 2.1284596920013428\n",
      "2000: 2.131715774536133\n",
      "2100: 2.130709409713745\n",
      "2200: 2.135843276977539\n",
      "2300: 2.095097541809082\n",
      "2400: 2.136087656021118\n",
      "2500: 2.1322505474090576\n",
      "2600: 2.1583902835845947\n",
      "2700: 2.136890172958374\n",
      "2800: 2.1116011142730713\n",
      "2900: 2.1519131660461426\n",
      "3000: 2.2225863933563232\n",
      "3100: 2.1293132305145264\n",
      "3200: 2.215528964996338\n",
      "3300: 2.1937143802642822\n",
      "3400: 2.15860652923584\n",
      "3500: 2.140834331512451\n",
      "3600: 2.1777970790863037\n",
      "3700: 2.1546807289123535\n",
      "3800: 2.159527063369751\n",
      "3900: 2.184126138687134\n",
      "4000: 2.0617079734802246\n",
      "4100: 2.1829538345336914\n",
      "4200: 2.1271140575408936\n",
      "4300: 2.1318204402923584\n",
      "4400: 2.1953141689300537\n",
      "4500: 2.1351218223571777\n",
      "4600: 2.1559019088745117\n",
      "4700: 2.14149808883667\n",
      "4800: 2.2029597759246826\n",
      "4900: 2.0992257595062256\n",
      "5000: 2.1721043586730957\n",
      "5100: 2.1891252994537354\n",
      "5200: 2.136935234069824\n",
      "5300: 2.1436471939086914\n",
      "5400: 2.1006855964660645\n",
      "5500: 2.17746639251709\n",
      "5600: 2.104562282562256\n",
      "5700: 2.215820074081421\n",
      "5800: 2.130066156387329\n",
      "5900: 2.110252618789673\n",
      "6000: 2.1559195518493652\n",
      "6100: 2.142388105392456\n",
      "6200: 2.1800405979156494\n",
      "6300: 2.1207077503204346\n",
      "6400: 2.1771371364593506\n",
      "6500: 2.140667676925659\n",
      "6600: 2.155961036682129\n",
      "6700: 2.2187485694885254\n",
      "6800: 2.1507999897003174\n",
      "6900: 2.20849609375\n",
      "7000: 2.1494908332824707\n",
      "7100: 2.1546823978424072\n",
      "7200: 2.1843268871307373\n",
      "7300: 2.239825487136841\n",
      "7400: 2.065793991088867\n",
      "7500: 2.214393377304077\n",
      "7600: 2.134194850921631\n",
      "7700: 2.1960859298706055\n",
      "7800: 2.136823892593384\n",
      "7900: 2.184112310409546\n",
      "8000: 2.1257753372192383\n",
      "8100: 2.112886667251587\n",
      "8200: 2.1048357486724854\n",
      "8300: 2.157949924468994\n",
      "8400: 2.1280386447906494\n",
      "8500: 2.1093289852142334\n",
      "8600: 2.1142799854278564\n",
      "8700: 2.1735830307006836\n",
      "8800: 2.1424362659454346\n",
      "8900: 2.122025728225708\n",
      "9000: 2.1247918605804443\n",
      "9100: 2.0890908241271973\n",
      "9200: 2.1483700275421143\n",
      "9300: 2.143639087677002\n",
      "9400: 2.1565144062042236\n",
      "9500: 2.1753668785095215\n",
      "9600: 2.156737804412842\n",
      "9700: 2.17470121383667\n",
      "9800: 2.1732282638549805\n",
      "9900: 2.1290717124938965\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (1000,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "37d1c988-6779-4e43-ab01-3a891ce19c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe78ce2dd90>]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "af14b538-c312-4583-b079-58f880247ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1748, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "227f7d66-988f-4eb8-ab9b-601a282587d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1448, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]    \n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fff47-13d9-482f-b9fc-94e7ade805ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best loss is 2.14 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ff936904-b73f-4d2d-b4c7-ac1ab314772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "kayah.\n",
      "see.\n",
      "med.\n",
      "rylla.\n",
      "emmruttadrlee.\n",
      "adelynneliah.\n",
      "milopi.\n",
      "eden.\n",
      "estanara.\n",
      "myki.\n",
      "hotalin.\n",
      "sher.\n",
      "roshiriel.\n",
      "kinto.\n",
      "jenslen.\n",
      "parrius.\n",
      "kynder.\n",
      "yadleyel.\n",
      "yume.\n",
      "myskeyla.\n",
      "hal.\n",
      "salyansuf.\n",
      "zakhloe.\n",
      "ren.\n",
      "crevis.\n",
      "jaiel.\n",
      "pordin.\n",
      "kyloe.\n",
      "bayziei.\n",
      "jorettyn.\n",
      "kharlo.\n",
      "jian.\n",
      "iri.\n",
      "evon.\n",
      "walla.\n",
      "ortarashten.\n",
      "goela.\n",
      "alitan.\n",
      "debil.\n",
      "vid.\n",
      "mellanetarriy.\n",
      "xavi.\n",
      "jakelizenickayionald.\n",
      "aive.\n",
      "dihevirley.\n",
      "jayena.\n",
      "moialorictamiia.\n",
      "jolon.\n",
      "ethayderlina.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22426312-4077-400f-8573-89975cd4d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 9 -- let's try more context!!\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "dc4fda39-744d-4d26-b000-f1249d8e3fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182456, 8]) torch.Size([182456])\n",
      "torch.Size([22752, 8]) torch.Size([22752])\n",
      "torch.Size([22938, 8]) torch.Size([22938])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "9b91765e-1880-4ea8-955d-59b605bfe0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 5), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((40, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "2050efe3-a412-42b7-bfd1-28998f903cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6962"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "041193ea-28df-477e-805d-14eb6b43e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.3255388736724854\n",
      "100: 2.3185596466064453\n",
      "200: 2.266429901123047\n",
      "300: 2.3607137203216553\n",
      "400: 2.2683298587799072\n",
      "500: 2.322235584259033\n",
      "600: 2.295442819595337\n",
      "700: 2.3104755878448486\n",
      "800: 2.282677173614502\n",
      "900: 2.270017385482788\n",
      "1000: 2.239888906478882\n",
      "1100: 2.3697705268859863\n",
      "1200: 2.2212793827056885\n",
      "1300: 2.32245135307312\n",
      "1400: 2.3299078941345215\n",
      "1500: 2.3041253089904785\n",
      "1600: 2.238553047180176\n",
      "1700: 2.2814693450927734\n",
      "1800: 2.338242530822754\n",
      "1900: 2.2348952293395996\n",
      "2000: 2.286409854888916\n",
      "2100: 2.29797101020813\n",
      "2200: 2.2634520530700684\n",
      "2300: 2.269585371017456\n",
      "2400: 2.31355357170105\n",
      "2500: 2.357513904571533\n",
      "2600: 2.1928603649139404\n",
      "2700: 2.2710089683532715\n",
      "2800: 2.323629140853882\n",
      "2900: 2.2475507259368896\n",
      "3000: 2.2773640155792236\n",
      "3100: 2.2593796253204346\n",
      "3200: 2.3024513721466064\n",
      "3300: 2.258265733718872\n",
      "3400: 2.303119659423828\n",
      "3500: 2.281431198120117\n",
      "3600: 2.326287031173706\n",
      "3700: 2.2870404720306396\n",
      "3800: 2.338315963745117\n",
      "3900: 2.33196759223938\n",
      "4000: 2.252747058868408\n",
      "4100: 2.2524728775024414\n",
      "4200: 2.318753957748413\n",
      "4300: 2.301090955734253\n",
      "4400: 2.307453155517578\n",
      "4500: 2.3356194496154785\n",
      "4600: 2.243673801422119\n",
      "4700: 2.2802722454071045\n",
      "4800: 2.3160204887390137\n",
      "4900: 2.3085203170776367\n",
      "5000: 2.333019733428955\n",
      "5100: 2.3041763305664062\n",
      "5200: 2.308865785598755\n",
      "5300: 2.358410596847534\n",
      "5400: 2.3137757778167725\n",
      "5500: 2.363126277923584\n",
      "5600: 2.2281510829925537\n",
      "5700: 2.3128750324249268\n",
      "5800: 2.264896869659424\n",
      "5900: 2.249565839767456\n",
      "6000: 2.28576922416687\n",
      "6100: 2.2507708072662354\n",
      "6200: 2.3319509029388428\n",
      "6300: 2.176737070083618\n",
      "6400: 2.3218748569488525\n",
      "6500: 2.3374814987182617\n",
      "6600: 2.35123872756958\n",
      "6700: 2.266542434692383\n",
      "6800: 2.3007304668426514\n",
      "6900: 2.3587722778320312\n",
      "7000: 2.3447017669677734\n",
      "7100: 2.316348075866699\n",
      "7200: 2.3259706497192383\n",
      "7300: 2.353896379470825\n",
      "7400: 2.309939384460449\n",
      "7500: 2.2910358905792236\n",
      "7600: 2.2267301082611084\n",
      "7700: 2.2565014362335205\n",
      "7800: 2.2769298553466797\n",
      "7900: 2.3391497135162354\n",
      "8000: 2.3281173706054688\n",
      "8100: 2.3113811016082764\n",
      "8200: 2.3781301975250244\n",
      "8300: 2.3573622703552246\n",
      "8400: 2.2678041458129883\n",
      "8500: 2.277805805206299\n",
      "8600: 2.238140821456909\n",
      "8700: 2.278149366378784\n",
      "8800: 2.2655038833618164\n",
      "8900: 2.306985855102539\n",
      "9000: 2.281338930130005\n",
      "9100: 2.3158414363861084\n",
      "9200: 2.2928080558776855\n",
      "9300: 2.2748398780822754\n",
      "9400: 2.2761101722717285\n",
      "9500: 2.2532095909118652\n",
      "9600: 2.2551681995391846\n",
      "9700: 2.247154951095581\n",
      "9800: 2.2843735218048096\n",
      "9900: 2.2859182357788086\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (1000,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 40) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "33744457-c0f3-4ea3-978b-5c747014ea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3066, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 40) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "89e576f6-6195-4aac-a258-f889f441f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mriaalmya.\n",
      "zieel.\n",
      "ndhayah.\n",
      "rethan.\n",
      "ejdrar.\n",
      "cadered.\n",
      "eliileli.\n",
      "jellene.\n",
      "sosoraa.\n",
      "selten.\n",
      "hokea.\n",
      "noshdber.\n",
      "shimies.\n",
      "kinir.\n",
      "jelilan.\n",
      "pucriu.\n",
      "zeyvde.\n",
      "kyleli.\n",
      "ehsylae.\n",
      "myskeyd.\n",
      "ahilina.\n",
      "yansuf.\n",
      "zalel.\n",
      "juren.\n",
      "cutris.\n",
      "jaoen.\n",
      "pordin.\n",
      "kykoe.\n",
      "bhigpein.\n",
      "samuey.\n",
      "chhmreo.\n",
      "miilani.\n",
      "jevondwal.\n",
      "ayleta.\n",
      "shithne.\n",
      "sila.\n",
      "alityn.\n",
      "dabisin.\n",
      "dameel.\n",
      "ketanna.\n",
      "saavin.\n",
      "damacienna.\n",
      "alin.\n",
      "audtii.\n",
      "bryah.\n",
      "vorle.\n",
      "ajalera.\n",
      "moialo.\n",
      "litamii.\n",
      "karlon.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90d6e7-5505-43d2-a1cf-c719478f2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 10 -- fat hidden layer\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "b150c1a4-1057-44d3-8490-cd5c7bc9e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182544, 3]) torch.Size([182544])\n",
      "torch.Size([22740, 3]) torch.Size([22740])\n",
      "torch.Size([22862, 3]) torch.Size([22862])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "fde11ea5-3f03-4038-a028-f73d2e9508af",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 4), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((12, 500), generator=g) # weights\n",
    "b1 = torch.randn(500, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((500, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "d52b84cd-c87d-4561-8cad-8d9c715dc399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20135"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "0bd01e45-4323-4fbd-a9dc-6f7a23db645f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.400026798248291\n",
      "1000: 2.516403913497925\n",
      "2000: 2.4230902194976807\n",
      "3000: 2.5435056686401367\n",
      "4000: 2.444976329803467\n",
      "5000: 2.3983139991760254\n",
      "6000: 2.4675819873809814\n",
      "7000: 2.6710329055786133\n",
      "8000: 2.5114474296569824\n",
      "9000: 2.3859541416168213\n",
      "10000: 2.4838268756866455\n",
      "11000: 2.405278205871582\n",
      "12000: 2.4701006412506104\n",
      "13000: 2.357053518295288\n",
      "14000: 2.462113857269287\n",
      "15000: 2.3935656547546387\n",
      "16000: 2.3927245140075684\n",
      "17000: 2.0862677097320557\n",
      "18000: 2.405197858810425\n",
      "19000: 2.271212100982666\n",
      "20000: 2.304227113723755\n",
      "21000: 2.3869893550872803\n",
      "22000: 2.2916066646575928\n",
      "23000: 2.3300557136535645\n",
      "24000: 2.214477777481079\n",
      "25000: 2.469085216522217\n",
      "26000: 2.3047218322753906\n",
      "27000: 2.4863810539245605\n",
      "28000: 2.2919373512268066\n",
      "29000: 2.328839063644409\n",
      "30000: 2.399650812149048\n",
      "31000: 2.2756433486938477\n",
      "32000: 2.3534815311431885\n",
      "33000: 2.1997742652893066\n",
      "34000: 2.411691188812256\n",
      "35000: 2.489468812942505\n",
      "36000: 2.2106142044067383\n",
      "37000: 2.440631151199341\n",
      "38000: 2.2793445587158203\n",
      "39000: 2.172184467315674\n",
      "40000: 2.1857521533966064\n",
      "41000: 2.5087921619415283\n",
      "42000: 2.430079460144043\n",
      "43000: 2.3045313358306885\n",
      "44000: 2.517206907272339\n",
      "45000: 2.6367852687835693\n",
      "46000: 2.3732800483703613\n",
      "47000: 2.329237461090088\n",
      "48000: 2.3509464263916016\n",
      "49000: 2.2136335372924805\n",
      "50000: 2.4581542015075684\n",
      "51000: 2.2345075607299805\n",
      "52000: 2.2509114742279053\n",
      "53000: 2.356142520904541\n",
      "54000: 2.1875011920928955\n",
      "55000: 2.415543556213379\n",
      "56000: 2.24505352973938\n",
      "57000: 2.43502140045166\n",
      "58000: 2.233445644378662\n",
      "59000: 2.386185646057129\n",
      "60000: 2.380619764328003\n",
      "61000: 2.4585676193237305\n",
      "62000: 2.341308116912842\n",
      "63000: 2.227530002593994\n",
      "64000: 2.164717197418213\n",
      "65000: 2.3308956623077393\n",
      "66000: 2.5311439037323\n",
      "67000: 2.44974422454834\n",
      "68000: 2.7079954147338867\n",
      "69000: 2.4204201698303223\n",
      "70000: 2.1718928813934326\n",
      "71000: 2.328188896179199\n",
      "72000: 2.365379810333252\n",
      "73000: 2.454496383666992\n",
      "74000: 2.3991129398345947\n",
      "75000: 2.294600009918213\n",
      "76000: 2.517493486404419\n",
      "77000: 2.494988441467285\n",
      "78000: 2.3155622482299805\n",
      "79000: 2.6632611751556396\n",
      "80000: 2.5548300743103027\n",
      "81000: 2.5144643783569336\n",
      "82000: 2.1719439029693604\n",
      "83000: 2.4006128311157227\n",
      "84000: 2.426901340484619\n",
      "85000: 2.605677604675293\n",
      "86000: 2.5068132877349854\n",
      "87000: 2.440315008163452\n",
      "88000: 2.332888126373291\n",
      "89000: 2.3727304935455322\n",
      "90000: 2.3582112789154053\n",
      "91000: 2.4664151668548584\n",
      "92000: 2.4533746242523193\n",
      "93000: 2.438542366027832\n",
      "94000: 2.2885544300079346\n",
      "95000: 2.464240074157715\n",
      "96000: 2.4121129512786865\n",
      "97000: 2.423931837081909\n",
      "98000: 2.1830663681030273\n",
      "99000: 2.47784686088562\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (100,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 12) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "fb551bfd-090f-4a5e-a12f-3cb5df95deb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3451, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 12) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "b11a14ea-2919-41d3-913b-7b12db913db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eria.\n",
      "kayah.\n",
      "keel.\n",
      "ndhryah.\n",
      "rethrelend.\n",
      "len.\n",
      "adered.\n",
      "elie.\n",
      "emi.\n",
      "jen.\n",
      "edeliean.\n",
      "aar.\n",
      "kayzioh.\n",
      "kalin.\n",
      "shub.\n",
      "rishirie.\n",
      "trin.\n",
      "renlyn.\n",
      "jethucan.\n",
      "kamyce.\n",
      "ryy.\n",
      "jul.\n",
      "ehs.\n",
      "kay.\n",
      "mystoyah.\n",
      "halina.\n",
      "yahsun.\n",
      "zakel.\n",
      "juren.\n",
      "cre.\n",
      "kaveaosten.\n",
      "adi.\n",
      "fenlo.\n",
      "obh.\n",
      "zpeila.\n",
      "keuthwe.\n",
      "hurer.\n",
      "min.\n",
      "kiri.\n",
      "evondwella.\n",
      "odtarastt.\n",
      "keysela.\n",
      "alin.\n",
      "fad.\n",
      "bra.\n",
      "vin.\n",
      "mellaketan.\n",
      "kylxan.\n",
      "avh.\n",
      "jacienn.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "8d007cd5-9ad9-4169-96af-a07626e71c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary:\n",
    "# i have no clue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da472264-05af-43ad-8b36-c40fe9666bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 11 -- maximize everything\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bae145-db29-400f-955c-dd0a32c055e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 8]) torch.Size([182580])\n",
      "torch.Size([22767, 8]) torch.Size([22767])\n",
      "torch.Size([22799, 8]) torch.Size([22799])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "               # original paper uses a cotext of 3 words\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd078ae8-5db0-4afd-9a7d-7251f3be406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 8), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((64, 500), generator=g) # weights\n",
    "b1 = torch.randn(500, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((500, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa00baf1-351a-4975-9ecd-22272bcd25d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e5664-728a-4c54-a8b3-e37546f8e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e6ccff-2aba-4d44-92ee-6ab5595d6abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 56.372703552246094\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (2,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.0001\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6968d1-b12b-4a43-b161-5a87c9d2b562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(44.0630, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21312f2-c3e7-4111-b1e1-dc295c7d94c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzuykvqk.\n",
      "yyzuukezyeecykawikvfy.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzuyvvqk.\n",
      "yyzuukezyeecykawikkfyrpgq.\n",
      "yqyvvyyyeckkyqyhkkeyhgobyruzvrduydcyyrmkkqgkzffrmfyqdsyymgkktgkkmsykkkykkk.\n",
      "yqyvvyyyeckkyqyhkkeyhgobyruzvrdxzyhkklm.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzurkeqwejqwrbsktfs.\n",
      "yyzuykgu.\n",
      "yyzurvkfyhsydyyymjkvkkyqkkkftkkutxkjtwkjtwrkawt.\n",
      "yyzuykgikepkeyqaxbtdfoxckyrz.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzuukeqwhkjyhkoiytkz.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yqyvvyyyeckkyqyhkkeyhgobykuzvrkf.\n",
      "yyzuykgikepkeyqatcfq.\n",
      "yyzuykguykkfqrkgq.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yqyvvyvyectkkqwxkf.\n",
      "yyzuukezyeecykawikvfy.\n",
      "yqyvvyyyeckkyqyhkkeyhgobyruzvrdxzyhkklq.\n",
      "yyzuyvvqk.\n",
      "yyzuukezyeecykawikqjyropyrikvfqrkxf.\n",
      "yqyvvyyyeckkyqk.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzuykgikepkeyqatcfqoowykzeinqoylmvzepkfiqdsjyhxtgq.\n",
      "yqyvvyvyec.\n",
      "yqyvvyyyeckkyqykju.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzuukeqwhkjyhzoiyzxy.\n",
      "yqyvvyyyeckkyqykjyfkvsykxi.\n",
      "yyzuukezyeokutqkkzq.\n",
      "yqyvvyyyeckkyqykjzfkjuqkojw.\n",
      "yyzurvkfyhsydyyymkkkkekokykyrakkkq.\n",
      "yyzuukezyeecykawikvfy.\n",
      "yyzuykgikepkeyqqkbyf.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzuukezyeecykawikvfqkkfq.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yyzurvkfyhsydyyymjkkkku.\n",
      "yqyvvyyyeckkyqykju.\n",
      "yqyvvyyyeckkyqyhkkly.\n",
      "yqyvvyvyectkkqwxkf.\n",
      "yyzurvkfyhsydyyymjkvkkyqkkkftdkotykakkrqkopf.\n",
      "yyzuukeqwhkjyhzoi.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198bfc14-f0fc-45a4-a783-a633b1e0c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best is about 2.5. can't figure out why I'm having trouble training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d092ae9-3553-4131-b13a-6ee3e5bba89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 12 -- \"finding a good initial learning rate\" revisited\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41a1c938-37a6-4b27-909d-2c274926ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182424, 8]) torch.Size([182424])\n",
      "torch.Size([22836, 8]) torch.Size([22836])\n",
      "torch.Size([22886, 8]) torch.Size([22886])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "               # original paper uses a cotext of 3 words\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "873ea0d3-ca82-4bdb-8613-e76e7648db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 8), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((64, 500), generator=g) # weights\n",
    "b1 = torch.randn(500, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((500, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4a30a69-c79d-45d9-b574-114399a8ba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46243"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14951cf8-7eea-408d-a18a-42e7f1a455bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 1.0023e-10, 1.0046e-10,  ..., 9.9540e-01, 9.9770e-01,\n",
       "        1.0000e+00])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-10, 0, 10000)\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "719d08fd-9c87-4d48-b4a8-9c095b391cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "#stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8c4a218-103c-4e21-a475-3f31519f9443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 47.0065803527832\n",
      "100: 47.5849494934082\n",
      "200: 44.084564208984375\n",
      "300: 49.797325134277344\n",
      "400: 55.064449310302734\n",
      "500: 48.966217041015625\n",
      "600: 41.51974868774414\n",
      "700: 46.09003448486328\n",
      "800: 48.06547927856445\n",
      "900: 44.45661544799805\n",
      "1000: 51.98072052001953\n",
      "1100: 41.90925598144531\n",
      "1200: 47.194252014160156\n",
      "1300: 40.077980041503906\n",
      "1400: 46.491275787353516\n",
      "1500: 45.51119613647461\n",
      "1600: 47.20030212402344\n",
      "1700: 47.48374557495117\n",
      "1800: 49.50501251220703\n",
      "1900: 47.608028411865234\n",
      "2000: 46.269615173339844\n",
      "2100: 50.73929977416992\n",
      "2200: 42.62544631958008\n",
      "2300: 44.26121520996094\n",
      "2400: 47.859867095947266\n",
      "2500: 43.727970123291016\n",
      "2600: 47.203636169433594\n",
      "2700: 47.85645294189453\n",
      "2800: 47.377986907958984\n",
      "2900: 45.77703857421875\n",
      "3000: 48.082157135009766\n",
      "3100: 41.89570617675781\n",
      "3200: 48.56153869628906\n",
      "3300: 43.193302154541016\n",
      "3400: 43.291378021240234\n",
      "3500: 48.145904541015625\n",
      "3600: 43.17989730834961\n",
      "3700: 46.13536071777344\n",
      "3800: 48.81465530395508\n",
      "3900: 45.28771209716797\n",
      "4000: 48.92469024658203\n",
      "4100: 49.38556671142578\n",
      "4200: 43.137969970703125\n",
      "4300: 41.66359329223633\n",
      "4400: 43.02587890625\n",
      "4500: 43.62525939941406\n",
      "4600: 48.57430648803711\n",
      "4700: 46.73372268676758\n",
      "4800: 47.011085510253906\n",
      "4900: 42.178401947021484\n",
      "5000: 41.987579345703125\n",
      "5100: 43.3134765625\n",
      "5200: 41.42856979370117\n",
      "5300: 46.94503402709961\n",
      "5400: 39.78673553466797\n",
      "5500: 44.29631042480469\n",
      "5600: 38.257694244384766\n",
      "5700: 40.448543548583984\n",
      "5800: 39.482818603515625\n",
      "5900: 39.90806198120117\n",
      "6000: 41.640037536621094\n",
      "6100: 38.707672119140625\n",
      "6200: 38.39336395263672\n",
      "6300: 40.09824752807617\n",
      "6400: 34.517730712890625\n",
      "6500: 38.02267074584961\n",
      "6600: 28.916332244873047\n",
      "6700: 30.807432174682617\n",
      "6800: 28.676597595214844\n",
      "6900: 28.187414169311523\n",
      "7000: 20.5025577545166\n",
      "7100: 26.748645782470703\n",
      "7200: 24.29469108581543\n",
      "7300: 22.61318588256836\n",
      "7400: 19.07689094543457\n",
      "7500: 20.541000366210938\n",
      "7600: 14.401211738586426\n",
      "7700: 16.805723190307617\n",
      "7800: 15.49291706085205\n",
      "7900: 11.151191711425781\n",
      "8000: 10.282615661621094\n",
      "8100: 11.610238075256348\n",
      "8200: 8.080693244934082\n",
      "8300: 9.652375221252441\n",
      "8400: 7.605132579803467\n",
      "8500: 9.66702651977539\n",
      "8600: 7.613177299499512\n",
      "8700: 6.896689414978027\n",
      "8800: 5.087751865386963\n",
      "8900: 3.7990870475769043\n",
      "9000: 5.059338569641113\n",
      "9100: 8.90091323852539\n",
      "9200: 7.847158908843994\n",
      "9300: 10.879505157470703\n",
      "9400: 13.400529861450195\n",
      "9500: 27.150386810302734\n",
      "9600: 42.63288116455078\n",
      "9700: 46.180686950683594\n",
      "9800: 57.286537170410156\n",
      "9900: 60.744598388671875\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (64,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "779f48e7-da04-4b33-a18d-f566aee57b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe407e0e510>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215a009-7f0f-4aba-bd5a-391f1094d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think andrej is trying to say that this graph should tell me a good learning\n",
    "# rate somehow, but I don't really get it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "632c401b-650e-42e3-a972-09a28b795da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(39.7612, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c19d82f-f6ca-4493-b207-bf9cd0b28652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uuumuu.\n",
      "gummmm.\n",
      "uumdtm.\n",
      "uuumuuu.\n",
      "uuumsm.\n",
      "uuumuu.\n",
      "uummmm.\n",
      "uummmm.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uersu.\n",
      "uumtto.\n",
      "uuumuu.\n",
      "uummmm.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuud.\n",
      "uumdtu.\n",
      "uuumuu.\n",
      "uuumsm.\n",
      "uuumsm.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uumdtm.\n",
      "uuumuu.\n",
      "uuumuud.\n",
      "uuumuu.\n",
      "uuumsm.\n",
      "uuudtm.\n",
      "uuudmm.\n",
      "gomttn.\n",
      "uuumuud.\n",
      "uuudmm.\n",
      "uuumuu.\n",
      "uuummm.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuumuu.\n",
      "uuudmm.\n",
      "uumdtm.\n",
      "uersuu.\n",
      "uevsn.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e9ede-bf2d-4dc4-9ee0-974cdcb3871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 12 -- \"finding a good initial learning rate\" revisited\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad79448d-521d-4a9e-a6de-91eeb07a88c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182512, 8]) torch.Size([182512])\n",
      "torch.Size([22860, 8]) torch.Size([22860])\n",
      "torch.Size([22774, 8]) torch.Size([22774])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "               # original paper uses a cotext of 3 words\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d12f2739-308c-4240-aafd-7f67aeba5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 8), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((64, 500), generator=g) # weights\n",
    "b1 = torch.randn(500, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((500, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d544363-4e2d-498e-bfa8-e2d6fc5d0c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46243"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "26a5acee-1eb1-4318-a147-357ad2015bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000e-10, 1.0018e-10, 1.0037e-10,  ..., 9.9632e-03, 9.9816e-03,\n",
       "         1.0000e-02]),\n",
       " 10000)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-10, -2, 10000)\n",
    "lrs = 10**lre\n",
    "lrs, len(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23c00622-242f-405d-abc5-fa651816c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "#stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "48df2b93-e522-4986-8878-fb488be8e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.3391175270080566\n",
      "100: 2.8070147037506104\n",
      "200: 2.3797178268432617\n",
      "300: 2.496537208557129\n",
      "400: 2.5398006439208984\n",
      "500: 2.571284055709839\n",
      "600: 2.6811602115631104\n",
      "700: 2.420034170150757\n",
      "800: 2.78875732421875\n",
      "900: 2.682877540588379\n",
      "1000: 2.6852965354919434\n",
      "1100: 2.4199185371398926\n",
      "1200: 2.69589900970459\n",
      "1300: 2.555596113204956\n",
      "1400: 2.3087005615234375\n",
      "1500: 2.7516753673553467\n",
      "1600: 2.4580025672912598\n",
      "1700: 2.4446187019348145\n",
      "1800: 2.353151321411133\n",
      "1900: 2.4845473766326904\n",
      "2000: 2.546217441558838\n",
      "2100: 2.3270339965820312\n",
      "2200: 2.411775588989258\n",
      "2300: 2.6635496616363525\n",
      "2400: 2.3707528114318848\n",
      "2500: 2.371311664581299\n",
      "2600: 2.5364863872528076\n",
      "2700: 2.6976816654205322\n",
      "2800: 2.4208197593688965\n",
      "2900: 2.572969675064087\n",
      "3000: 2.4632010459899902\n",
      "3100: 2.425497531890869\n",
      "3200: 2.5610456466674805\n",
      "3300: 2.608712911605835\n",
      "3400: 2.4549741744995117\n",
      "3500: 2.6657309532165527\n",
      "3600: 2.6430580615997314\n",
      "3700: 2.224351167678833\n",
      "3800: 2.436720848083496\n",
      "3900: 2.4235293865203857\n",
      "4000: 2.33117413520813\n",
      "4100: 2.558607339859009\n",
      "4200: 2.4565558433532715\n",
      "4300: 2.4024784564971924\n",
      "4400: 2.4825682640075684\n",
      "4500: 2.6437184810638428\n",
      "4600: 2.301595687866211\n",
      "4700: 2.791567325592041\n",
      "4800: 2.5210018157958984\n",
      "4900: 2.3851542472839355\n",
      "5000: 2.4393868446350098\n",
      "5100: 2.5247740745544434\n",
      "5200: 2.8002889156341553\n",
      "5300: 2.546170711517334\n",
      "5400: 2.403970718383789\n",
      "5500: 2.6641976833343506\n",
      "5600: 2.448209285736084\n",
      "5700: 2.5465128421783447\n",
      "5800: 2.9335734844207764\n",
      "5900: 2.563124179840088\n",
      "6000: 2.583346366882324\n",
      "6100: 2.4774794578552246\n",
      "6200: 2.581372022628784\n",
      "6300: 2.492827892303467\n",
      "6400: 2.601659059524536\n",
      "6500: 2.3917016983032227\n",
      "6600: 2.5362136363983154\n",
      "6700: 2.4125869274139404\n",
      "6800: 2.7074697017669678\n",
      "6900: 2.2857766151428223\n",
      "7000: 2.429546356201172\n",
      "7100: 2.600959062576294\n",
      "7200: 2.5794761180877686\n",
      "7300: 2.533953905105591\n",
      "7400: 2.627257823944092\n",
      "7500: 2.7685656547546387\n",
      "7600: 2.523308753967285\n",
      "7700: 2.345698118209839\n",
      "7800: 2.336005210876465\n",
      "7900: 2.615978240966797\n",
      "8000: 2.5424633026123047\n",
      "8100: 2.5987627506256104\n",
      "8200: 2.3558871746063232\n",
      "8300: 2.5744636058807373\n",
      "8400: 2.513536214828491\n",
      "8500: 2.4462521076202393\n",
      "8600: 2.402336359024048\n",
      "8700: 2.7166175842285156\n",
      "8800: 2.4632344245910645\n",
      "8900: 2.6225810050964355\n",
      "9000: 2.616231918334961\n",
      "9100: 2.4142799377441406\n",
      "9200: 2.2666127681732178\n",
      "9300: 2.3914036750793457\n",
      "9400: 2.4347078800201416\n",
      "9500: 2.36857271194458\n",
      "9600: 2.5946245193481445\n",
      "9700: 2.207164764404297\n",
      "9800: 2.4028258323669434\n",
      "9900: 2.6173408031463623\n",
      "10000: 2.5617616176605225\n",
      "10100: 2.66866135597229\n",
      "10200: 2.44581937789917\n",
      "10300: 2.4933695793151855\n",
      "10400: 2.8114101886749268\n",
      "10500: 2.226447820663452\n",
      "10600: 2.449093818664551\n",
      "10700: 2.5603384971618652\n",
      "10800: 2.293391704559326\n",
      "10900: 2.39357590675354\n",
      "11000: 2.4436304569244385\n",
      "11100: 2.773312568664551\n",
      "11200: 2.692073106765747\n",
      "11300: 2.1934263706207275\n",
      "11400: 2.5787813663482666\n",
      "11500: 2.3623268604278564\n",
      "11600: 2.5596108436584473\n",
      "11700: 2.3241324424743652\n",
      "11800: 2.193878412246704\n",
      "11900: 2.471951723098755\n",
      "12000: 2.2701244354248047\n",
      "12100: 2.409510850906372\n",
      "12200: 2.298243284225464\n",
      "12300: 2.6230828762054443\n",
      "12400: 2.4487106800079346\n",
      "12500: 2.6942389011383057\n",
      "12600: 2.830489158630371\n",
      "12700: 2.57891583442688\n",
      "12800: 2.4247353076934814\n",
      "12900: 2.7097301483154297\n",
      "13000: 2.3113057613372803\n",
      "13100: 2.262371063232422\n",
      "13200: 2.6938118934631348\n",
      "13300: 2.285367250442505\n",
      "13400: 2.3960137367248535\n",
      "13500: 2.655120849609375\n",
      "13600: 2.2564451694488525\n",
      "13700: 2.5192630290985107\n",
      "13800: 2.5988569259643555\n",
      "13900: 2.5840365886688232\n",
      "14000: 2.880061626434326\n",
      "14100: 2.401923894882202\n",
      "14200: 2.789057970046997\n",
      "14300: 2.5483391284942627\n",
      "14400: 2.6555826663970947\n",
      "14500: 2.4857020378112793\n",
      "14600: 2.475806713104248\n",
      "14700: 2.3687446117401123\n",
      "14800: 2.5224592685699463\n",
      "14900: 2.2922403812408447\n",
      "15000: 2.7026991844177246\n",
      "15100: 2.5819551944732666\n",
      "15200: 2.526132583618164\n",
      "15300: 2.457484245300293\n",
      "15400: 2.260392665863037\n",
      "15500: 2.340602159500122\n",
      "15600: 2.611119270324707\n",
      "15700: 2.316929817199707\n",
      "15800: 2.492033004760742\n",
      "15900: 2.5240519046783447\n",
      "16000: 2.5111138820648193\n",
      "16100: 2.647517204284668\n",
      "16200: 2.359008550643921\n",
      "16300: 2.282466411590576\n",
      "16400: 2.483525037765503\n",
      "16500: 2.422981023788452\n",
      "16600: 2.132638692855835\n",
      "16700: 2.457064390182495\n",
      "16800: 2.587158203125\n",
      "16900: 2.2842204570770264\n",
      "17000: 2.2663676738739014\n",
      "17100: 2.788158655166626\n",
      "17200: 2.5638952255249023\n",
      "17300: 2.472416877746582\n",
      "17400: 2.326711654663086\n",
      "17500: 2.672680139541626\n",
      "17600: 2.2605578899383545\n",
      "17700: 2.550471544265747\n",
      "17800: 1.9901390075683594\n",
      "17900: 2.5804407596588135\n",
      "18000: 2.509828567504883\n",
      "18100: 2.2560348510742188\n",
      "18200: 2.358813762664795\n",
      "18300: 2.555382251739502\n",
      "18400: 2.438420057296753\n",
      "18500: 2.6396381855010986\n",
      "18600: 2.484421730041504\n",
      "18700: 2.765939235687256\n",
      "18800: 2.73470401763916\n",
      "18900: 2.5864288806915283\n",
      "19000: 2.373772621154785\n",
      "19100: 2.566885232925415\n",
      "19200: 2.5376996994018555\n",
      "19300: 2.3290634155273438\n",
      "19400: 2.327744483947754\n",
      "19500: 2.616025447845459\n",
      "19600: 2.463911533355713\n",
      "19700: 2.4051425457000732\n",
      "19800: 2.527843475341797\n",
      "19900: 2.6097865104675293\n",
      "20000: 2.5984535217285156\n",
      "20100: 2.810181140899658\n",
      "20200: 2.528501033782959\n",
      "20300: 2.2564845085144043\n",
      "20400: 2.540982246398926\n",
      "20500: 2.457820415496826\n",
      "20600: 2.6673424243927\n",
      "20700: 2.3623204231262207\n",
      "20800: 2.4137167930603027\n",
      "20900: 2.2938265800476074\n",
      "21000: 2.613537311553955\n",
      "21100: 2.470320701599121\n",
      "21200: 2.410661458969116\n",
      "21300: 2.522533416748047\n",
      "21400: 2.686704397201538\n",
      "21500: 2.1341254711151123\n",
      "21600: 2.3800735473632812\n",
      "21700: 2.5647530555725098\n",
      "21800: 2.4899425506591797\n",
      "21900: 2.339712619781494\n",
      "22000: 2.385971784591675\n",
      "22100: 2.4598844051361084\n",
      "22200: 2.4919674396514893\n",
      "22300: 2.6208908557891846\n",
      "22400: 2.564727783203125\n",
      "22500: 2.277831792831421\n",
      "22600: 2.767357110977173\n",
      "22700: 2.5456674098968506\n",
      "22800: 2.567174196243286\n",
      "22900: 2.5533313751220703\n",
      "23000: 2.306363105773926\n",
      "23100: 2.269061803817749\n",
      "23200: 2.2909374237060547\n",
      "23300: 2.266359806060791\n",
      "23400: 2.5823776721954346\n",
      "23500: 2.3151986598968506\n",
      "23600: 2.389902353286743\n",
      "23700: 2.366894245147705\n",
      "23800: 2.5783884525299072\n",
      "23900: 2.5391125679016113\n",
      "24000: 2.4754159450531006\n",
      "24100: 2.3510451316833496\n",
      "24200: 2.246853828430176\n",
      "24300: 2.627267837524414\n",
      "24400: 2.5118157863616943\n",
      "24500: 2.6327450275421143\n",
      "24600: 2.300058126449585\n",
      "24700: 2.475707530975342\n",
      "24800: 2.581425666809082\n",
      "24900: 2.4426400661468506\n",
      "25000: 2.501340627670288\n",
      "25100: 2.828819990158081\n",
      "25200: 2.282832384109497\n",
      "25300: 2.772193670272827\n",
      "25400: 2.5438730716705322\n",
      "25500: 2.569399356842041\n",
      "25600: 2.681626558303833\n",
      "25700: 2.462172031402588\n",
      "25800: 2.398806095123291\n",
      "25900: 2.5277140140533447\n",
      "26000: 2.2417430877685547\n",
      "26100: 2.4570868015289307\n",
      "26200: 2.7265706062316895\n",
      "26300: 2.3935463428497314\n",
      "26400: 2.4566938877105713\n",
      "26500: 2.5292751789093018\n",
      "26600: 2.6484556198120117\n",
      "26700: 2.4403750896453857\n",
      "26800: 2.533907890319824\n",
      "26900: 2.288294553756714\n",
      "27000: 2.2811572551727295\n",
      "27100: 2.3212928771972656\n",
      "27200: 2.280121326446533\n",
      "27300: 2.459059953689575\n",
      "27400: 2.4051156044006348\n",
      "27500: 2.5059962272644043\n",
      "27600: 2.517850875854492\n",
      "27700: 2.306530714035034\n",
      "27800: 2.370776653289795\n",
      "27900: 2.4809634685516357\n",
      "28000: 2.481804132461548\n",
      "28100: 2.37971830368042\n",
      "28200: 2.6750118732452393\n",
      "28300: 2.3166935443878174\n",
      "28400: 2.65557861328125\n",
      "28500: 2.5978639125823975\n",
      "28600: 2.4192421436309814\n",
      "28700: 2.3209915161132812\n",
      "28800: 2.4600236415863037\n",
      "28900: 2.1672377586364746\n",
      "29000: 2.3788187503814697\n",
      "29100: 2.569866895675659\n",
      "29200: 2.118281602859497\n",
      "29300: 2.353762149810791\n",
      "29400: 2.6985180377960205\n",
      "29500: 2.4187541007995605\n",
      "29600: 2.5113906860351562\n",
      "29700: 2.3290164470672607\n",
      "29800: 2.5169999599456787\n",
      "29900: 2.384889841079712\n",
      "30000: 2.303508996963501\n",
      "30100: 2.359635591506958\n",
      "30200: 2.5809571743011475\n",
      "30300: 2.3953652381896973\n",
      "30400: 2.184915781021118\n",
      "30500: 2.406297206878662\n",
      "30600: 2.586909055709839\n",
      "30700: 2.327970266342163\n",
      "30800: 2.471191644668579\n",
      "30900: 2.3858683109283447\n",
      "31000: 2.276715040206909\n",
      "31100: 2.3401987552642822\n",
      "31200: 2.3955721855163574\n",
      "31300: 2.5371742248535156\n",
      "31400: 2.3609209060668945\n",
      "31500: 2.5616776943206787\n",
      "31600: 2.344242572784424\n",
      "31700: 2.3113796710968018\n",
      "31800: 2.5661377906799316\n",
      "31900: 2.4941155910491943\n",
      "32000: 2.5658154487609863\n",
      "32100: 2.6045706272125244\n",
      "32200: 2.328636884689331\n",
      "32300: 2.5149457454681396\n",
      "32400: 2.292797327041626\n",
      "32500: 2.7532780170440674\n",
      "32600: 2.335179328918457\n",
      "32700: 2.280555248260498\n",
      "32800: 2.6366305351257324\n",
      "32900: 2.4167046546936035\n",
      "33000: 2.3074941635131836\n",
      "33100: 2.470954179763794\n",
      "33200: 2.3479018211364746\n",
      "33300: 2.4904611110687256\n",
      "33400: 2.4148526191711426\n",
      "33500: 2.4495112895965576\n",
      "33600: 2.546631097793579\n",
      "33700: 2.2383272647857666\n",
      "33800: 2.2323927879333496\n",
      "33900: 2.268479108810425\n",
      "34000: 2.486997127532959\n",
      "34100: 2.4129605293273926\n",
      "34200: 2.438014030456543\n",
      "34300: 2.6402204036712646\n",
      "34400: 2.437929630279541\n",
      "34500: 2.4777591228485107\n",
      "34600: 2.103403091430664\n",
      "34700: 2.446314573287964\n",
      "34800: 2.4839982986450195\n",
      "34900: 2.623509645462036\n",
      "35000: 2.4503486156463623\n",
      "35100: 2.302185297012329\n",
      "35200: 2.0674855709075928\n",
      "35300: 2.463514566421509\n",
      "35400: 2.0917625427246094\n",
      "35500: 2.5632922649383545\n",
      "35600: 2.4528274536132812\n",
      "35700: 2.6649229526519775\n",
      "35800: 2.3580212593078613\n",
      "35900: 2.3192548751831055\n",
      "36000: 2.4584593772888184\n",
      "36100: 2.7949652671813965\n",
      "36200: 2.270242214202881\n",
      "36300: 2.204893112182617\n",
      "36400: 2.5774710178375244\n",
      "36500: 2.5427799224853516\n",
      "36600: 2.4795408248901367\n",
      "36700: 2.3368079662323\n",
      "36800: 2.4490935802459717\n",
      "36900: 2.380751371383667\n",
      "37000: 2.368673086166382\n",
      "37100: 2.383061647415161\n",
      "37200: 2.03131365776062\n",
      "37300: 2.3261430263519287\n",
      "37400: 2.5896356105804443\n",
      "37500: 2.4540090560913086\n",
      "37600: 2.4439847469329834\n",
      "37700: 2.4630980491638184\n",
      "37800: 2.232649326324463\n",
      "37900: 2.7380306720733643\n",
      "38000: 2.569356679916382\n",
      "38100: 2.2860593795776367\n",
      "38200: 2.4588215351104736\n",
      "38300: 2.2200582027435303\n",
      "38400: 2.292020797729492\n",
      "38500: 2.4877936840057373\n",
      "38600: 2.844865083694458\n",
      "38700: 2.45668888092041\n",
      "38800: 2.292619466781616\n",
      "38900: 2.5298566818237305\n",
      "39000: 2.521493434906006\n",
      "39100: 2.770282506942749\n",
      "39200: 2.548368453979492\n",
      "39300: 2.187628984451294\n",
      "39400: 2.527535915374756\n",
      "39500: 2.447291851043701\n",
      "39600: 2.5718624591827393\n",
      "39700: 2.6216118335723877\n",
      "39800: 2.456977128982544\n",
      "39900: 2.449209451675415\n",
      "40000: 2.705963134765625\n",
      "40100: 2.2564258575439453\n",
      "40200: 2.6679837703704834\n",
      "40300: 2.385114908218384\n",
      "40400: 2.639371395111084\n",
      "40500: 2.6920716762542725\n",
      "40600: 2.787555456161499\n",
      "40700: 2.5320098400115967\n",
      "40800: 2.6010029315948486\n",
      "40900: 2.460399866104126\n",
      "41000: 2.426054000854492\n",
      "41100: 2.3981761932373047\n",
      "41200: 2.5887043476104736\n",
      "41300: 2.5166003704071045\n",
      "41400: 2.3437001705169678\n",
      "41500: 2.309075117111206\n",
      "41600: 2.601536989212036\n",
      "41700: 2.3095247745513916\n",
      "41800: 2.3430044651031494\n",
      "41900: 2.372166633605957\n",
      "42000: 2.3573434352874756\n",
      "42100: 2.359232187271118\n",
      "42200: 2.2880055904388428\n",
      "42300: 2.6453158855438232\n",
      "42400: 2.4248907566070557\n",
      "42500: 2.372180700302124\n",
      "42600: 2.2802047729492188\n",
      "42700: 2.0692012310028076\n",
      "42800: 2.19824481010437\n",
      "42900: 2.3270225524902344\n",
      "43000: 2.5987045764923096\n",
      "43100: 2.453089714050293\n",
      "43200: 2.553093671798706\n",
      "43300: 2.501277208328247\n",
      "43400: 2.307013750076294\n",
      "43500: 2.262728214263916\n",
      "43600: 2.749812364578247\n",
      "43700: 2.392069101333618\n",
      "43800: 2.3491344451904297\n",
      "43900: 2.3421196937561035\n",
      "44000: 2.2255523204803467\n",
      "44100: 2.3791182041168213\n",
      "44200: 2.5453011989593506\n",
      "44300: 2.4349114894866943\n",
      "44400: 2.293015480041504\n",
      "44500: 2.3986318111419678\n",
      "44600: 2.516216993331909\n",
      "44700: 2.6832549571990967\n",
      "44800: 2.3322181701660156\n",
      "44900: 2.3364992141723633\n",
      "45000: 2.446716070175171\n",
      "45100: 2.1305956840515137\n",
      "45200: 2.207935333251953\n",
      "45300: 2.655613422393799\n",
      "45400: 2.4288082122802734\n",
      "45500: 2.374412775039673\n",
      "45600: 2.4884231090545654\n",
      "45700: 2.257690191268921\n",
      "45800: 2.435223340988159\n",
      "45900: 2.73264217376709\n",
      "46000: 2.5578315258026123\n",
      "46100: 2.143183469772339\n",
      "46200: 2.2423038482666016\n",
      "46300: 2.2548489570617676\n",
      "46400: 2.518968343734741\n",
      "46500: 2.7538015842437744\n",
      "46600: 2.4602935314178467\n",
      "46700: 2.482024908065796\n",
      "46800: 2.6660993099212646\n",
      "46900: 2.441248655319214\n",
      "47000: 2.6103410720825195\n",
      "47100: 2.510040283203125\n",
      "47200: 2.3634033203125\n",
      "47300: 2.600295305252075\n",
      "47400: 2.252203941345215\n",
      "47500: 2.1615920066833496\n",
      "47600: 2.4899709224700928\n",
      "47700: 2.107509136199951\n",
      "47800: 2.130647659301758\n",
      "47900: 2.3549392223358154\n",
      "48000: 2.6910157203674316\n",
      "48100: 2.6985983848571777\n",
      "48200: 2.773374319076538\n",
      "48300: 2.4243385791778564\n",
      "48400: 2.757761001586914\n",
      "48500: 2.5453808307647705\n",
      "48600: 2.4589545726776123\n",
      "48700: 2.3933324813842773\n",
      "48800: 2.6535956859588623\n",
      "48900: 2.507920265197754\n",
      "49000: 2.4006121158599854\n",
      "49100: 2.3196423053741455\n",
      "49200: 2.5107262134552\n",
      "49300: 2.2069544792175293\n",
      "49400: 2.4256882667541504\n",
      "49500: 2.4888508319854736\n",
      "49600: 2.3303654193878174\n",
      "49700: 2.156998634338379\n",
      "49800: 2.4334237575531006\n",
      "49900: 2.3861234188079834\n",
      "50000: 2.3213562965393066\n",
      "50100: 2.408493995666504\n",
      "50200: 2.357245445251465\n",
      "50300: 2.4110302925109863\n",
      "50400: 2.5561015605926514\n",
      "50500: 2.3198039531707764\n",
      "50600: 2.507641077041626\n",
      "50700: 2.340071201324463\n",
      "50800: 2.3595783710479736\n",
      "50900: 2.3276894092559814\n",
      "51000: 2.3214316368103027\n",
      "51100: 2.406672477722168\n",
      "51200: 2.579066753387451\n",
      "51300: 2.5766513347625732\n",
      "51400: 2.4408583641052246\n",
      "51500: 2.2003417015075684\n",
      "51600: 2.6827685832977295\n",
      "51700: 2.3224945068359375\n",
      "51800: 2.403061628341675\n",
      "51900: 2.612027168273926\n",
      "52000: 2.0762996673583984\n",
      "52100: 2.5060203075408936\n",
      "52200: 2.4142706394195557\n",
      "52300: 2.3910961151123047\n",
      "52400: 2.2270963191986084\n",
      "52500: 2.231876850128174\n",
      "52600: 2.393817186355591\n",
      "52700: 2.4139926433563232\n",
      "52800: 2.2747902870178223\n",
      "52900: 2.451273202896118\n",
      "53000: 2.646378993988037\n",
      "53100: 2.6366589069366455\n",
      "53200: 2.1837635040283203\n",
      "53300: 2.372343063354492\n",
      "53400: 2.547595262527466\n",
      "53500: 2.5036420822143555\n",
      "53600: 2.5167839527130127\n",
      "53700: 2.461165189743042\n",
      "53800: 2.373786211013794\n",
      "53900: 2.827117443084717\n",
      "54000: 2.5084173679351807\n",
      "54100: 2.4423959255218506\n",
      "54200: 2.1054036617279053\n",
      "54300: 2.4803552627563477\n",
      "54400: 2.1269192695617676\n",
      "54500: 2.4952633380889893\n",
      "54600: 2.3742542266845703\n",
      "54700: 2.3617653846740723\n",
      "54800: 2.4926018714904785\n",
      "54900: 2.1022255420684814\n",
      "55000: 2.4771504402160645\n",
      "55100: 2.452951431274414\n",
      "55200: 2.4614675045013428\n",
      "55300: 2.7714648246765137\n",
      "55400: 2.3716647624969482\n",
      "55500: 2.3909220695495605\n",
      "55600: 2.1740307807922363\n",
      "55700: 2.535698175430298\n",
      "55800: 2.4011056423187256\n",
      "55900: 2.3581135272979736\n",
      "56000: 2.5973193645477295\n",
      "56100: 2.4941558837890625\n",
      "56200: 2.3875069618225098\n",
      "56300: 2.4175474643707275\n",
      "56400: 2.492051601409912\n",
      "56500: 2.3682758808135986\n",
      "56600: 2.4953365325927734\n",
      "56700: 2.1835644245147705\n",
      "56800: 2.7598843574523926\n",
      "56900: 2.6355133056640625\n",
      "57000: 2.200692892074585\n",
      "57100: 2.3810529708862305\n",
      "57200: 2.3417539596557617\n",
      "57300: 2.3581998348236084\n",
      "57400: 2.1584689617156982\n",
      "57500: 2.5121729373931885\n",
      "57600: 2.3634047508239746\n",
      "57700: 2.384147882461548\n",
      "57800: 2.6339306831359863\n",
      "57900: 2.6186139583587646\n",
      "58000: 2.4852969646453857\n",
      "58100: 2.430926561355591\n",
      "58200: 2.7109193801879883\n",
      "58300: 2.344341278076172\n",
      "58400: 2.3015270233154297\n",
      "58500: 2.5211479663848877\n",
      "58600: 2.465359926223755\n",
      "58700: 2.0291285514831543\n",
      "58800: 2.463693141937256\n",
      "58900: 2.1503968238830566\n",
      "59000: 2.254638910293579\n",
      "59100: 2.3545117378234863\n",
      "59200: 2.589963436126709\n",
      "59300: 2.4448790550231934\n",
      "59400: 2.234426259994507\n",
      "59500: 2.2798051834106445\n",
      "59600: 2.344980239868164\n",
      "59700: 2.4653069972991943\n",
      "59800: 2.3705317974090576\n",
      "59900: 2.275486707687378\n",
      "60000: 2.356478452682495\n",
      "60100: 2.5471131801605225\n",
      "60200: 2.6367475986480713\n",
      "60300: 2.5082955360412598\n",
      "60400: 2.5953855514526367\n",
      "60500: 2.314751148223877\n",
      "60600: 2.5407707691192627\n",
      "60700: 2.668914318084717\n",
      "60800: 2.294625759124756\n",
      "60900: 2.4954419136047363\n",
      "61000: 2.3440182209014893\n",
      "61100: 2.4829630851745605\n",
      "61200: 2.330136299133301\n",
      "61300: 2.526228189468384\n",
      "61400: 2.3938844203948975\n",
      "61500: 2.547680139541626\n",
      "61600: 2.4744434356689453\n",
      "61700: 2.0954792499542236\n",
      "61800: 2.388918876647949\n",
      "61900: 2.0971946716308594\n",
      "62000: 2.0799760818481445\n",
      "62100: 2.2380385398864746\n",
      "62200: 2.4405949115753174\n",
      "62300: 2.503751277923584\n",
      "62400: 2.2870490550994873\n",
      "62500: 2.5650627613067627\n",
      "62600: 2.1368653774261475\n",
      "62700: 2.2583248615264893\n",
      "62800: 2.3851146697998047\n",
      "62900: 2.494253396987915\n",
      "63000: 2.4726147651672363\n",
      "63100: 2.314378023147583\n",
      "63200: 2.4938805103302\n",
      "63300: 2.296298027038574\n",
      "63400: 2.4785571098327637\n",
      "63500: 2.4008378982543945\n",
      "63600: 2.4764647483825684\n",
      "63700: 2.780606508255005\n",
      "63800: 2.3697469234466553\n",
      "63900: 2.53045392036438\n",
      "64000: 2.3235697746276855\n",
      "64100: 2.356466054916382\n",
      "64200: 2.3637092113494873\n",
      "64300: 2.6164610385894775\n",
      "64400: 2.3711767196655273\n",
      "64500: 2.3417789936065674\n",
      "64600: 2.2952582836151123\n",
      "64700: 2.431591272354126\n",
      "64800: 2.6720263957977295\n",
      "64900: 2.554682731628418\n",
      "65000: 2.4127039909362793\n",
      "65100: 2.5348517894744873\n",
      "65200: 2.382286310195923\n",
      "65300: 2.3445558547973633\n",
      "65400: 2.3337385654449463\n",
      "65500: 2.565919876098633\n",
      "65600: 2.5028865337371826\n",
      "65700: 2.2582011222839355\n",
      "65800: 2.5571048259735107\n",
      "65900: 2.444647789001465\n",
      "66000: 2.6307294368743896\n",
      "66100: 2.1153128147125244\n",
      "66200: 2.471027374267578\n",
      "66300: 2.428250789642334\n",
      "66400: 2.221248149871826\n",
      "66500: 2.5900261402130127\n",
      "66600: 2.4653546810150146\n",
      "66700: 2.4430794715881348\n",
      "66800: 2.131779193878174\n",
      "66900: 2.308194398880005\n",
      "67000: 2.4597601890563965\n",
      "67100: 2.4482533931732178\n",
      "67200: 2.13983416557312\n",
      "67300: 2.7992348670959473\n",
      "67400: 2.366694211959839\n",
      "67500: 2.378995656967163\n",
      "67600: 2.399111270904541\n",
      "67700: 2.391423463821411\n",
      "67800: 2.6378867626190186\n",
      "67900: 2.554190158843994\n",
      "68000: 2.460137367248535\n",
      "68100: 2.578447103500366\n",
      "68200: 2.504488468170166\n",
      "68300: 2.072528123855591\n",
      "68400: 2.598674774169922\n",
      "68500: 2.497530937194824\n",
      "68600: 2.3686769008636475\n",
      "68700: 2.1140875816345215\n",
      "68800: 2.4820165634155273\n",
      "68900: 2.217134475708008\n",
      "69000: 2.3428268432617188\n",
      "69100: 2.4533402919769287\n",
      "69200: 2.4739952087402344\n",
      "69300: 2.3053414821624756\n",
      "69400: 2.2812018394470215\n",
      "69500: 2.191366672515869\n",
      "69600: 2.680034637451172\n",
      "69700: 2.426711320877075\n",
      "69800: 2.6290080547332764\n",
      "69900: 2.245502471923828\n",
      "70000: 2.4322144985198975\n",
      "70100: 2.6211962699890137\n",
      "70200: 2.212705135345459\n",
      "70300: 2.528224229812622\n",
      "70400: 2.3750410079956055\n",
      "70500: 2.468010187149048\n",
      "70600: 2.3560192584991455\n",
      "70700: 2.3966763019561768\n",
      "70800: 2.282809019088745\n",
      "70900: 2.516585111618042\n",
      "71000: 2.5630340576171875\n",
      "71100: 2.3198161125183105\n",
      "71200: 2.647533416748047\n",
      "71300: 2.191417932510376\n",
      "71400: 2.565366268157959\n",
      "71500: 2.569345474243164\n",
      "71600: 2.45447039604187\n",
      "71700: 2.464726448059082\n",
      "71800: 2.2784056663513184\n",
      "71900: 2.52486252784729\n",
      "72000: 2.313588857650757\n",
      "72100: 2.2174816131591797\n",
      "72200: 2.567915678024292\n",
      "72300: 2.521491765975952\n",
      "72400: 2.5252456665039062\n",
      "72500: 2.3195858001708984\n",
      "72600: 2.285285234451294\n",
      "72700: 2.2804501056671143\n",
      "72800: 2.3676345348358154\n",
      "72900: 2.213714122772217\n",
      "73000: 2.343535900115967\n",
      "73100: 2.5121312141418457\n",
      "73200: 2.5658979415893555\n",
      "73300: 2.218165159225464\n",
      "73400: 2.542954206466675\n",
      "73500: 2.2083144187927246\n",
      "73600: 2.223743438720703\n",
      "73700: 2.2963640689849854\n",
      "73800: 2.2673468589782715\n",
      "73900: 2.2818665504455566\n",
      "74000: 2.6997227668762207\n",
      "74100: 2.1030056476593018\n",
      "74200: 2.51586651802063\n",
      "74300: 2.584986925125122\n",
      "74400: 2.2434937953948975\n",
      "74500: 2.352389097213745\n",
      "74600: 2.541118621826172\n",
      "74700: 2.258162021636963\n",
      "74800: 2.2862212657928467\n",
      "74900: 2.5480778217315674\n",
      "75000: 2.257265329360962\n",
      "75100: 2.226778507232666\n",
      "75200: 2.3141088485717773\n",
      "75300: 2.4541120529174805\n",
      "75400: 2.234022617340088\n",
      "75500: 2.215820789337158\n",
      "75600: 2.654528856277466\n",
      "75700: 2.474161148071289\n",
      "75800: 2.404794692993164\n",
      "75900: 2.3387274742126465\n",
      "76000: 2.3244357109069824\n",
      "76100: 2.7147669792175293\n",
      "76200: 2.5045533180236816\n",
      "76300: 2.5829949378967285\n",
      "76400: 2.427037239074707\n",
      "76500: 2.3126401901245117\n",
      "76600: 2.402630567550659\n",
      "76700: 2.426046848297119\n",
      "76800: 2.3028202056884766\n",
      "76900: 2.4298906326293945\n",
      "77000: 2.289180278778076\n",
      "77100: 2.7553415298461914\n",
      "77200: 2.450261354446411\n",
      "77300: 2.426339626312256\n",
      "77400: 2.32133150100708\n",
      "77500: 2.3338799476623535\n",
      "77600: 2.6401121616363525\n",
      "77700: 2.5184733867645264\n",
      "77800: 2.2337019443511963\n",
      "77900: 2.3079535961151123\n",
      "78000: 2.370014190673828\n",
      "78100: 2.7783315181732178\n",
      "78200: 2.8043041229248047\n",
      "78300: 2.327791929244995\n",
      "78400: 2.51603364944458\n",
      "78500: 2.3221373558044434\n",
      "78600: 2.5325732231140137\n",
      "78700: 2.301539182662964\n",
      "78800: 2.585916519165039\n",
      "78900: 2.3800153732299805\n",
      "79000: 2.3549585342407227\n",
      "79100: 2.537973165512085\n",
      "79200: 2.505058526992798\n",
      "79300: 2.3405990600585938\n",
      "79400: 2.3302011489868164\n",
      "79500: 2.7272932529449463\n",
      "79600: 2.4775614738464355\n",
      "79700: 2.358905553817749\n",
      "79800: 2.2028732299804688\n",
      "79900: 2.381910800933838\n",
      "80000: 2.0459110736846924\n",
      "80100: 2.3724749088287354\n",
      "80200: 2.332021713256836\n",
      "80300: 2.4568185806274414\n",
      "80400: 2.7000157833099365\n",
      "80500: 2.6011159420013428\n",
      "80600: 2.4182891845703125\n",
      "80700: 2.3353042602539062\n",
      "80800: 2.33752703666687\n",
      "80900: 2.3431763648986816\n",
      "81000: 2.268603563308716\n",
      "81100: 2.2899155616760254\n",
      "81200: 2.3557515144348145\n",
      "81300: 2.516627788543701\n",
      "81400: 2.519712448120117\n",
      "81500: 2.6021714210510254\n",
      "81600: 2.263232946395874\n",
      "81700: 2.274458169937134\n",
      "81800: 2.2556490898132324\n",
      "81900: 2.468954086303711\n",
      "82000: 2.4397964477539062\n",
      "82100: 2.325213670730591\n",
      "82200: 2.557149887084961\n",
      "82300: 2.2576401233673096\n",
      "82400: 2.4815752506256104\n",
      "82500: 2.4222593307495117\n",
      "82600: 2.37528920173645\n",
      "82700: 2.307485818862915\n",
      "82800: 2.3020517826080322\n",
      "82900: 2.5630319118499756\n",
      "83000: 2.4011123180389404\n",
      "83100: 2.5142905712127686\n",
      "83200: 2.2504396438598633\n",
      "83300: 2.2415049076080322\n",
      "83400: 2.081498384475708\n",
      "83500: 2.3399572372436523\n",
      "83600: 2.6119775772094727\n",
      "83700: 2.305248498916626\n",
      "83800: 2.493335247039795\n",
      "83900: 2.224555730819702\n",
      "84000: 2.1870827674865723\n",
      "84100: 2.141059160232544\n",
      "84200: 2.332839012145996\n",
      "84300: 2.5353047847747803\n",
      "84400: 2.543344497680664\n",
      "84500: 2.429734945297241\n",
      "84600: 2.3654592037200928\n",
      "84700: 2.261427402496338\n",
      "84800: 2.314786434173584\n",
      "84900: 2.351203680038452\n",
      "85000: 2.4319918155670166\n",
      "85100: 2.6410670280456543\n",
      "85200: 2.2320477962493896\n",
      "85300: 2.535871744155884\n",
      "85400: 2.5757699012756348\n",
      "85500: 2.4892890453338623\n",
      "85600: 2.6049749851226807\n",
      "85700: 2.4571683406829834\n",
      "85800: 2.1928515434265137\n",
      "85900: 2.2822561264038086\n",
      "86000: 2.3738012313842773\n",
      "86100: 2.1405086517333984\n",
      "86200: 2.315845251083374\n",
      "86300: 2.4090747833251953\n",
      "86400: 2.6517672538757324\n",
      "86500: 2.4564154148101807\n",
      "86600: 2.4607863426208496\n",
      "86700: 2.328305721282959\n",
      "86800: 2.7263503074645996\n",
      "86900: 2.7122445106506348\n",
      "87000: 2.4321160316467285\n",
      "87100: 2.37760853767395\n",
      "87200: 2.5247697830200195\n",
      "87300: 2.748478651046753\n",
      "87400: 2.5294229984283447\n",
      "87500: 2.3845326900482178\n",
      "87600: 2.67439341545105\n",
      "87700: 2.381290912628174\n",
      "87800: 2.5920164585113525\n",
      "87900: 2.3555209636688232\n",
      "88000: 2.2968833446502686\n",
      "88100: 2.606870174407959\n",
      "88200: 2.4902145862579346\n",
      "88300: 2.4317407608032227\n",
      "88400: 2.4064207077026367\n",
      "88500: 2.3111155033111572\n",
      "88600: 2.489140748977661\n",
      "88700: 2.336052417755127\n",
      "88800: 2.4150376319885254\n",
      "88900: 2.633758544921875\n",
      "89000: 2.5722503662109375\n",
      "89100: 2.5756125450134277\n",
      "89200: 2.3691234588623047\n",
      "89300: 2.4086809158325195\n",
      "89400: 2.324899435043335\n",
      "89500: 2.4742684364318848\n",
      "89600: 2.2503466606140137\n",
      "89700: 2.431612968444824\n",
      "89800: 2.3631372451782227\n",
      "89900: 2.338463306427002\n",
      "90000: 2.5913331508636475\n",
      "90100: 2.3868539333343506\n",
      "90200: 2.177976131439209\n",
      "90300: 2.4506280422210693\n",
      "90400: 2.340083360671997\n",
      "90500: 2.0504987239837646\n",
      "90600: 2.455677032470703\n",
      "90700: 2.373772382736206\n",
      "90800: 2.36159610748291\n",
      "90900: 2.175767183303833\n",
      "91000: 2.6117563247680664\n",
      "91100: 2.540194272994995\n",
      "91200: 2.4176058769226074\n",
      "91300: 2.6726200580596924\n",
      "91400: 2.4176344871520996\n",
      "91500: 2.5012152194976807\n",
      "91600: 2.0489871501922607\n",
      "91700: 2.318716526031494\n",
      "91800: 2.0243146419525146\n",
      "91900: 2.367077350616455\n",
      "92000: 2.397998332977295\n",
      "92100: 2.1342613697052\n",
      "92200: 2.5050792694091797\n",
      "92300: 2.5822865962982178\n",
      "92400: 2.314619302749634\n",
      "92500: 2.3577916622161865\n",
      "92600: 2.4198098182678223\n",
      "92700: 2.517789602279663\n",
      "92800: 2.466181516647339\n",
      "92900: 2.6321892738342285\n",
      "93000: 2.4704322814941406\n",
      "93100: 2.12416410446167\n",
      "93200: 2.3287670612335205\n",
      "93300: 2.3548877239227295\n",
      "93400: 2.4754509925842285\n",
      "93500: 2.163259744644165\n",
      "93600: 2.3641128540039062\n",
      "93700: 2.5885608196258545\n",
      "93800: 2.6298234462738037\n",
      "93900: 2.5022170543670654\n",
      "94000: 2.42783260345459\n",
      "94100: 2.3932547569274902\n",
      "94200: 2.2681469917297363\n",
      "94300: 2.296027421951294\n",
      "94400: 2.320549726486206\n",
      "94500: 2.465053081512451\n",
      "94600: 2.393894672393799\n",
      "94700: 2.4131414890289307\n",
      "94800: 2.4372706413269043\n",
      "94900: 2.1811492443084717\n",
      "95000: 2.3080053329467773\n",
      "95100: 2.3251054286956787\n",
      "95200: 2.2805016040802\n",
      "95300: 2.3352203369140625\n",
      "95400: 2.397080659866333\n",
      "95500: 2.307623863220215\n",
      "95600: 2.567753553390503\n",
      "95700: 2.226780891418457\n",
      "95800: 2.1963515281677246\n",
      "95900: 2.3180320262908936\n",
      "96000: 2.2803988456726074\n",
      "96100: 2.5499777793884277\n",
      "96200: 2.6597893238067627\n",
      "96300: 2.305481195449829\n",
      "96400: 2.48052716255188\n",
      "96500: 2.3613758087158203\n",
      "96600: 2.4915449619293213\n",
      "96700: 2.294182777404785\n",
      "96800: 2.1707916259765625\n",
      "96900: 2.3541934490203857\n",
      "97000: 2.2925186157226562\n",
      "97100: 2.4887306690216064\n",
      "97200: 2.2640058994293213\n",
      "97300: 2.5116968154907227\n",
      "97400: 2.6456758975982666\n",
      "97500: 2.1139612197875977\n",
      "97600: 2.3880906105041504\n",
      "97700: 2.2450125217437744\n",
      "97800: 2.2251718044281006\n",
      "97900: 2.225374937057495\n",
      "98000: 2.4986095428466797\n",
      "98100: 2.3500351905822754\n",
      "98200: 2.3777756690979004\n",
      "98300: 2.405867099761963\n",
      "98400: 2.2976837158203125\n",
      "98500: 2.693479299545288\n",
      "98600: 2.46596360206604\n",
      "98700: 2.371520519256592\n",
      "98800: 2.3671231269836426\n",
      "98900: 2.4199721813201904\n",
      "99000: 2.4340977668762207\n",
      "99100: 2.1470513343811035\n",
      "99200: 1.965064525604248\n",
      "99300: 2.5551815032958984\n",
      "99400: 2.383453607559204\n",
      "99500: 2.431044340133667\n",
      "99600: 2.346292018890381\n",
      "99700: 2.383343458175659\n",
      "99800: 2.3671250343322754\n",
      "99900: 2.4935567378997803\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (64,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    #lr = lrs[i]\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a4567697-f417-46d4-9d76-125a36743a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1338a874-ff04-41b8-b408-d71fe79f8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think andrej is trying to say that this graph should tell me a good learning\n",
    "# rate somehow, but I don't really get it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3b329178-aba6-4cd4-b2ff-cf54c1e4d53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3984, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "457f2531-e7fa-463c-bff1-84eedc50c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mriaai.\n",
      "yazhiee.\n",
      "medhahah.\n",
      "rethrs.\n",
      "endrar.\n",
      "cazered.\n",
      "eliih.\n",
      "miirea.\n",
      "edeisea.\n",
      "anarae.\n",
      "teimhlte.\n",
      "ciysadh.\n",
      "rilhiries.\n",
      "kinir.\n",
      "jelilxe.\n",
      "pucan.\n",
      "bamacdi.\n",
      "kymile.\n",
      "ehsyraa.\n",
      "mhanlyd.\n",
      "ahilina.\n",
      "yarsui.\n",
      "zalel.\n",
      "juren.\n",
      "crevis.\n",
      "jaoen.\n",
      "pordir.\n",
      "koxoeo.\n",
      "hizpei.\n",
      "jaretey.\n",
      "cehmreo.\n",
      "miilaii.\n",
      "jevondwa.\n",
      "layitta.\n",
      "sazten.\n",
      "goisa.\n",
      "blitbt.\n",
      "xabsa.\n",
      "riu.\n",
      "melirke.\n",
      "arriy.\n",
      "xavia.\n",
      "damacien.\n",
      "cyanio.\n",
      "audtii.\n",
      "banih.\n",
      "vorle.\n",
      "ajalera.\n",
      "moialo.\n",
      "laramii.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5f835fb6-9661-4d4b-957c-3ebff11326d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 12 -- theory: i need a better optimization strategy for a more complex network.\n",
    "#                    maybe it's easier to train a smaller net....\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f6e69a3c-0f01-401d-989e-d7e86cd4d885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182484, 8]) torch.Size([182484])\n",
      "torch.Size([22869, 8]) torch.Size([22869])\n",
      "torch.Size([22793, 8]) torch.Size([22793])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "               # original paper uses a cotext of 3 words\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2806d60c-b9f2-458f-ad73-2ccd72b62033",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 8), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((64, 100), generator=g) # weights\n",
    "b1 = torch.randn(100, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a92dba9f-6c23-460f-9a85-36be4a91f8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9443"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "211f6f52-dc1e-4a69-8ffb-1f97876a3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.330089807510376\n",
      "100: 2.3923168182373047\n",
      "200: 2.391613245010376\n",
      "300: 2.3664283752441406\n",
      "400: 2.610666275024414\n",
      "500: 2.3220608234405518\n",
      "600: 2.3096301555633545\n",
      "700: 2.4452760219573975\n",
      "800: 2.427002191543579\n",
      "900: 2.361023187637329\n",
      "1000: 2.518714189529419\n",
      "1100: 2.3422248363494873\n",
      "1200: 2.4920294284820557\n",
      "1300: 2.4895739555358887\n",
      "1400: 2.3213417530059814\n",
      "1500: 2.6742475032806396\n",
      "1600: 2.726616859436035\n",
      "1700: 2.5693960189819336\n",
      "1800: 2.5881383419036865\n",
      "1900: 2.4109766483306885\n",
      "2000: 2.5260813236236572\n",
      "2100: 2.2356295585632324\n",
      "2200: 2.3765172958374023\n",
      "2300: 2.230626344680786\n",
      "2400: 2.488909959793091\n",
      "2500: 2.41420316696167\n",
      "2600: 2.432661771774292\n",
      "2700: 2.5296168327331543\n",
      "2800: 2.36438250541687\n",
      "2900: 2.498133659362793\n",
      "3000: 2.118522882461548\n",
      "3100: 2.4593193531036377\n",
      "3200: 2.27461314201355\n",
      "3300: 2.607172727584839\n",
      "3400: 2.3483800888061523\n",
      "3500: 2.332271099090576\n",
      "3600: 2.428596258163452\n",
      "3700: 2.4144701957702637\n",
      "3800: 2.518435478210449\n",
      "3900: 2.417273759841919\n",
      "4000: 2.5323400497436523\n",
      "4100: 2.3491640090942383\n",
      "4200: 2.3414604663848877\n",
      "4300: 2.371173858642578\n",
      "4400: 2.3263745307922363\n",
      "4500: 2.632052183151245\n",
      "4600: 2.586824655532837\n",
      "4700: 2.443251848220825\n",
      "4800: 2.346390724182129\n",
      "4900: 2.5249674320220947\n",
      "5000: 2.4399800300598145\n",
      "5100: 2.2457809448242188\n",
      "5200: 2.2952911853790283\n",
      "5300: 2.3314099311828613\n",
      "5400: 2.3062705993652344\n",
      "5500: 2.6012022495269775\n",
      "5600: 2.335108757019043\n",
      "5700: 2.2511632442474365\n",
      "5800: 2.4226362705230713\n",
      "5900: 2.3055367469787598\n",
      "6000: 2.41192889213562\n",
      "6100: 2.2570948600769043\n",
      "6200: 2.629232883453369\n",
      "6300: 2.2355730533599854\n",
      "6400: 2.3446314334869385\n",
      "6500: 2.3901901245117188\n",
      "6600: 2.434232234954834\n",
      "6700: 2.489535331726074\n",
      "6800: 2.465261936187744\n",
      "6900: 2.532926559448242\n",
      "7000: 2.3256068229675293\n",
      "7100: 2.5797154903411865\n",
      "7200: 2.4737579822540283\n",
      "7300: 2.5698726177215576\n",
      "7400: 2.21317982673645\n",
      "7500: 2.38731050491333\n",
      "7600: 2.2307586669921875\n",
      "7700: 2.43448543548584\n",
      "7800: 2.3463666439056396\n",
      "7900: 2.2989230155944824\n",
      "8000: 2.337719202041626\n",
      "8100: 2.373769760131836\n",
      "8200: 2.354567050933838\n",
      "8300: 2.2119760513305664\n",
      "8400: 2.363273859024048\n",
      "8500: 2.3507282733917236\n",
      "8600: 2.421238422393799\n",
      "8700: 2.2794103622436523\n",
      "8800: 2.4582293033599854\n",
      "8900: 2.385178327560425\n",
      "9000: 2.49902606010437\n",
      "9100: 2.333080291748047\n",
      "9200: 2.4303321838378906\n",
      "9300: 2.455000162124634\n",
      "9400: 2.389967441558838\n",
      "9500: 2.411395311355591\n",
      "9600: 2.436673641204834\n",
      "9700: 2.5065135955810547\n",
      "9800: 2.3253371715545654\n",
      "9900: 2.350720167160034\n",
      "10000: 2.396766424179077\n",
      "10100: 2.3794357776641846\n",
      "10200: 2.1799979209899902\n",
      "10300: 2.3424620628356934\n",
      "10400: 2.4553027153015137\n",
      "10500: 2.4579222202301025\n",
      "10600: 2.1905100345611572\n",
      "10700: 2.432523727416992\n",
      "10800: 2.399012565612793\n",
      "10900: 2.3715882301330566\n",
      "11000: 2.592953681945801\n",
      "11100: 2.4788053035736084\n",
      "11200: 2.4983644485473633\n",
      "11300: 2.451002836227417\n",
      "11400: 2.4715356826782227\n",
      "11500: 2.515380620956421\n",
      "11600: 2.374295949935913\n",
      "11700: 2.3361241817474365\n",
      "11800: 2.453608274459839\n",
      "11900: 2.3122353553771973\n",
      "12000: 2.396047830581665\n",
      "12100: 2.4160163402557373\n",
      "12200: 2.3145105838775635\n",
      "12300: 2.6305203437805176\n",
      "12400: 2.292485237121582\n",
      "12500: 2.329517364501953\n",
      "12600: 2.409064292907715\n",
      "12700: 2.4620983600616455\n",
      "12800: 2.3878045082092285\n",
      "12900: 2.3928334712982178\n",
      "13000: 2.3833370208740234\n",
      "13100: 2.539912462234497\n",
      "13200: 2.597771644592285\n",
      "13300: 2.2855279445648193\n",
      "13400: 2.462275266647339\n",
      "13500: 2.4382195472717285\n",
      "13600: 2.227370500564575\n",
      "13700: 2.3771753311157227\n",
      "13800: 2.1767845153808594\n",
      "13900: 2.4139702320098877\n",
      "14000: 2.330793619155884\n",
      "14100: 2.2195827960968018\n",
      "14200: 2.2629411220550537\n",
      "14300: 2.402712345123291\n",
      "14400: 2.493903398513794\n",
      "14500: 2.5507595539093018\n",
      "14600: 2.3666703701019287\n",
      "14700: 2.2571303844451904\n",
      "14800: 2.3950726985931396\n",
      "14900: 2.2872776985168457\n",
      "15000: 2.5509448051452637\n",
      "15100: 2.682732582092285\n",
      "15200: 2.310600519180298\n",
      "15300: 2.197216749191284\n",
      "15400: 2.5978903770446777\n",
      "15500: 2.2736892700195312\n",
      "15600: 2.362222671508789\n",
      "15700: 2.4156148433685303\n",
      "15800: 2.6032583713531494\n",
      "15900: 2.5216660499572754\n",
      "16000: 2.375403642654419\n",
      "16100: 2.2794594764709473\n",
      "16200: 2.3765878677368164\n",
      "16300: 2.5587475299835205\n",
      "16400: 2.506302833557129\n",
      "16500: 2.4177074432373047\n",
      "16600: 2.3786802291870117\n",
      "16700: 2.2561631202697754\n",
      "16800: 2.512099266052246\n",
      "16900: 2.452252149581909\n",
      "17000: 2.4503536224365234\n",
      "17100: 2.5859756469726562\n",
      "17200: 2.54888653755188\n",
      "17300: 2.4286956787109375\n",
      "17400: 2.3563308715820312\n",
      "17500: 2.357145071029663\n",
      "17600: 2.399386167526245\n",
      "17700: 2.538393020629883\n",
      "17800: 2.4825146198272705\n",
      "17900: 2.568716526031494\n",
      "18000: 2.5200695991516113\n",
      "18100: 2.586038112640381\n",
      "18200: 2.280156373977661\n",
      "18300: 2.3195066452026367\n",
      "18400: 2.4519124031066895\n",
      "18500: 2.500901460647583\n",
      "18600: 2.6000425815582275\n",
      "18700: 2.3363819122314453\n",
      "18800: 2.5452842712402344\n",
      "18900: 2.4364116191864014\n",
      "19000: 2.4294540882110596\n",
      "19100: 2.520540952682495\n",
      "19200: 2.442927837371826\n",
      "19300: 2.5420308113098145\n",
      "19400: 2.534534454345703\n",
      "19500: 2.4740188121795654\n",
      "19600: 2.3421895503997803\n",
      "19700: 2.4647297859191895\n",
      "19800: 2.4048502445220947\n",
      "19900: 2.383056163787842\n",
      "20000: 2.443197250366211\n",
      "20100: 2.443016290664673\n",
      "20200: 2.5499277114868164\n",
      "20300: 2.362154722213745\n",
      "20400: 2.5256433486938477\n",
      "20500: 2.411647319793701\n",
      "20600: 2.2816925048828125\n",
      "20700: 2.563009738922119\n",
      "20800: 2.270521402359009\n",
      "20900: 2.5120861530303955\n",
      "21000: 2.435443878173828\n",
      "21100: 2.2545065879821777\n",
      "21200: 2.58907413482666\n",
      "21300: 2.355365753173828\n",
      "21400: 2.550889730453491\n",
      "21500: 2.520657539367676\n",
      "21600: 2.594961404800415\n",
      "21700: 2.381969690322876\n",
      "21800: 2.297943353652954\n",
      "21900: 2.3516738414764404\n",
      "22000: 2.427297353744507\n",
      "22100: 2.4620649814605713\n",
      "22200: 2.3205080032348633\n",
      "22300: 2.324125289916992\n",
      "22400: 2.5088415145874023\n",
      "22500: 2.5577385425567627\n",
      "22600: 2.2961225509643555\n",
      "22700: 2.488691806793213\n",
      "22800: 2.5275163650512695\n",
      "22900: 2.213672161102295\n",
      "23000: 2.40053391456604\n",
      "23100: 2.2781293392181396\n",
      "23200: 2.0631535053253174\n",
      "23300: 2.5158021450042725\n",
      "23400: 2.569087028503418\n",
      "23500: 2.334300994873047\n",
      "23600: 2.545952796936035\n",
      "23700: 2.5501909255981445\n",
      "23800: 2.261284589767456\n",
      "23900: 2.4306423664093018\n",
      "24000: 2.436189651489258\n",
      "24100: 2.237607955932617\n",
      "24200: 2.387324333190918\n",
      "24300: 2.442537307739258\n",
      "24400: 2.735755681991577\n",
      "24500: 2.553169012069702\n",
      "24600: 2.445373773574829\n",
      "24700: 2.3741812705993652\n",
      "24800: 2.298130750656128\n",
      "24900: 2.393399477005005\n",
      "25000: 2.6438465118408203\n",
      "25100: 2.4548702239990234\n",
      "25200: 2.3938424587249756\n",
      "25300: 2.6823320388793945\n",
      "25400: 2.4787776470184326\n",
      "25500: 2.5830419063568115\n",
      "25600: 2.4070541858673096\n",
      "25700: 2.3306386470794678\n",
      "25800: 2.6084353923797607\n",
      "25900: 2.3698582649230957\n",
      "26000: 2.276174545288086\n",
      "26100: 2.197890043258667\n",
      "26200: 2.3077375888824463\n",
      "26300: 2.1777238845825195\n",
      "26400: 2.4076619148254395\n",
      "26500: 2.39245867729187\n",
      "26600: 2.396263360977173\n",
      "26700: 2.4596569538116455\n",
      "26800: 2.40273118019104\n",
      "26900: 2.4238059520721436\n",
      "27000: 2.4740779399871826\n",
      "27100: 2.417431592941284\n",
      "27200: 2.3879802227020264\n",
      "27300: 2.454584836959839\n",
      "27400: 2.505392551422119\n",
      "27500: 2.3649821281433105\n",
      "27600: 2.466930866241455\n",
      "27700: 2.3573617935180664\n",
      "27800: 2.3975296020507812\n",
      "27900: 2.5142619609832764\n",
      "28000: 2.160938262939453\n",
      "28100: 2.4623706340789795\n",
      "28200: 2.2434136867523193\n",
      "28300: 2.2155308723449707\n",
      "28400: 2.4498438835144043\n",
      "28500: 2.458289623260498\n",
      "28600: 2.339414596557617\n",
      "28700: 2.2692666053771973\n",
      "28800: 2.4175121784210205\n",
      "28900: 2.248654842376709\n",
      "29000: 2.53135085105896\n",
      "29100: 2.4195802211761475\n",
      "29200: 2.4662318229675293\n",
      "29300: 2.432136297225952\n",
      "29400: 2.356032371520996\n",
      "29500: 2.450148344039917\n",
      "29600: 2.6046950817108154\n",
      "29700: 2.39616322517395\n",
      "29800: 2.3408560752868652\n",
      "29900: 2.329618215560913\n",
      "30000: 2.275182008743286\n",
      "30100: 2.481489896774292\n",
      "30200: 2.2309443950653076\n",
      "30300: 2.491795539855957\n",
      "30400: 2.390960216522217\n",
      "30500: 2.580716371536255\n",
      "30600: 2.4590516090393066\n",
      "30700: 2.313901901245117\n",
      "30800: 2.550200939178467\n",
      "30900: 2.558408498764038\n",
      "31000: 2.383030414581299\n",
      "31100: 2.389127016067505\n",
      "31200: 2.5151569843292236\n",
      "31300: 2.40256667137146\n",
      "31400: 2.275355577468872\n",
      "31500: 2.4337995052337646\n",
      "31600: 2.282930850982666\n",
      "31700: 2.358370542526245\n",
      "31800: 2.352386951446533\n",
      "31900: 2.3131608963012695\n",
      "32000: 2.586932897567749\n",
      "32100: 2.4923055171966553\n",
      "32200: 2.3704922199249268\n",
      "32300: 2.552461862564087\n",
      "32400: 2.370162010192871\n",
      "32500: 2.2692363262176514\n",
      "32600: 2.6257317066192627\n",
      "32700: 2.569530487060547\n",
      "32800: 2.462923526763916\n",
      "32900: 2.351459264755249\n",
      "33000: 2.438657522201538\n",
      "33100: 2.5203256607055664\n",
      "33200: 2.3655142784118652\n",
      "33300: 2.4451065063476562\n",
      "33400: 2.4563026428222656\n",
      "33500: 2.3475217819213867\n",
      "33600: 2.373044490814209\n",
      "33700: 2.4287970066070557\n",
      "33800: 2.2888224124908447\n",
      "33900: 2.3912322521209717\n",
      "34000: 2.430969715118408\n",
      "34100: 2.5081326961517334\n",
      "34200: 2.481510877609253\n",
      "34300: 2.5614895820617676\n",
      "34400: 2.44343900680542\n",
      "34500: 2.5693647861480713\n",
      "34600: 2.3290412425994873\n",
      "34700: 2.506727933883667\n",
      "34800: 2.3784635066986084\n",
      "34900: 2.5034759044647217\n",
      "35000: 2.4928877353668213\n",
      "35100: 2.434596538543701\n",
      "35200: 2.4548466205596924\n",
      "35300: 2.438977003097534\n",
      "35400: 2.5441267490386963\n",
      "35500: 2.350759744644165\n",
      "35600: 2.4790914058685303\n",
      "35700: 2.586913824081421\n",
      "35800: 2.2334747314453125\n",
      "35900: 2.4396018981933594\n",
      "36000: 2.402364730834961\n",
      "36100: 2.482482433319092\n",
      "36200: 2.3389029502868652\n",
      "36300: 2.440218210220337\n",
      "36400: 2.4868085384368896\n",
      "36500: 2.5275635719299316\n",
      "36600: 2.314108371734619\n",
      "36700: 2.3139822483062744\n",
      "36800: 2.453690528869629\n",
      "36900: 2.413813829421997\n",
      "37000: 2.448204517364502\n",
      "37100: 2.3481383323669434\n",
      "37200: 2.3927414417266846\n",
      "37300: 2.73417067527771\n",
      "37400: 2.671142101287842\n",
      "37500: 2.4776718616485596\n",
      "37600: 2.479072093963623\n",
      "37700: 2.2699496746063232\n",
      "37800: 2.431227922439575\n",
      "37900: 2.248842716217041\n",
      "38000: 2.4069783687591553\n",
      "38100: 2.345982313156128\n",
      "38200: 2.4606878757476807\n",
      "38300: 2.4602785110473633\n",
      "38400: 2.3407788276672363\n",
      "38500: 2.4385461807250977\n",
      "38600: 2.5662307739257812\n",
      "38700: 2.2421951293945312\n",
      "38800: 2.4753341674804688\n",
      "38900: 2.6185302734375\n",
      "39000: 2.510946273803711\n",
      "39100: 2.4162099361419678\n",
      "39200: 2.3756320476531982\n",
      "39300: 2.5037052631378174\n",
      "39400: 2.369110584259033\n",
      "39500: 2.539252758026123\n",
      "39600: 2.377927541732788\n",
      "39700: 2.376127243041992\n",
      "39800: 2.3806703090667725\n",
      "39900: 2.3853068351745605\n",
      "40000: 2.340994119644165\n",
      "40100: 2.5008206367492676\n",
      "40200: 2.3939552307128906\n",
      "40300: 2.371114492416382\n",
      "40400: 2.542135238647461\n",
      "40500: 2.4963793754577637\n",
      "40600: 2.272411346435547\n",
      "40700: 2.6549417972564697\n",
      "40800: 2.546898126602173\n",
      "40900: 2.3948628902435303\n",
      "41000: 2.608825206756592\n",
      "41100: 2.3642466068267822\n",
      "41200: 2.267784833908081\n",
      "41300: 2.395477056503296\n",
      "41400: 2.620636463165283\n",
      "41500: 2.4005861282348633\n",
      "41600: 2.2230639457702637\n",
      "41700: 2.52919864654541\n",
      "41800: 2.333127975463867\n",
      "41900: 2.426752805709839\n",
      "42000: 2.4659245014190674\n",
      "42100: 2.3179197311401367\n",
      "42200: 2.3779642581939697\n",
      "42300: 2.422825813293457\n",
      "42400: 2.2747697830200195\n",
      "42500: 2.447425603866577\n",
      "42600: 2.362718105316162\n",
      "42700: 2.415804862976074\n",
      "42800: 2.6104114055633545\n",
      "42900: 2.4562556743621826\n",
      "43000: 2.49310040473938\n",
      "43100: 2.4103002548217773\n",
      "43200: 2.4042410850524902\n",
      "43300: 2.548105001449585\n",
      "43400: 2.389531135559082\n",
      "43500: 2.230818748474121\n",
      "43600: 2.332817554473877\n",
      "43700: 2.252793073654175\n",
      "43800: 2.2836644649505615\n",
      "43900: 2.4429752826690674\n",
      "44000: 2.5160186290740967\n",
      "44100: 2.3501861095428467\n",
      "44200: 2.402390241622925\n",
      "44300: 2.202564001083374\n",
      "44400: 2.5189197063446045\n",
      "44500: 2.380129814147949\n",
      "44600: 2.5971310138702393\n",
      "44700: 2.437594175338745\n",
      "44800: 2.5769362449645996\n",
      "44900: 2.3713901042938232\n",
      "45000: 2.412728786468506\n",
      "45100: 2.496523380279541\n",
      "45200: 2.3925509452819824\n",
      "45300: 2.3928093910217285\n",
      "45400: 2.41119647026062\n",
      "45500: 2.3597631454467773\n",
      "45600: 2.4128479957580566\n",
      "45700: 2.4675183296203613\n",
      "45800: 2.4793312549591064\n",
      "45900: 2.296220302581787\n",
      "46000: 2.1582493782043457\n",
      "46100: 2.5410704612731934\n",
      "46200: 2.4576351642608643\n",
      "46300: 2.3692514896392822\n",
      "46400: 2.6550393104553223\n",
      "46500: 2.341991662979126\n",
      "46600: 2.3131296634674072\n",
      "46700: 2.6054441928863525\n",
      "46800: 2.4615583419799805\n",
      "46900: 2.4502053260803223\n",
      "47000: 2.4455726146698\n",
      "47100: 2.4839930534362793\n",
      "47200: 2.472571849822998\n",
      "47300: 2.2325382232666016\n",
      "47400: 2.4278411865234375\n",
      "47500: 2.435073137283325\n",
      "47600: 2.453166961669922\n",
      "47700: 2.3855702877044678\n",
      "47800: 2.643071174621582\n",
      "47900: 2.346195936203003\n",
      "48000: 2.4413273334503174\n",
      "48100: 2.3898885250091553\n",
      "48200: 2.5688607692718506\n",
      "48300: 2.2207093238830566\n",
      "48400: 2.2298707962036133\n",
      "48500: 2.448066234588623\n",
      "48600: 2.2888660430908203\n",
      "48700: 2.3660943508148193\n",
      "48800: 2.5012764930725098\n",
      "48900: 2.3826358318328857\n",
      "49000: 2.523273468017578\n",
      "49100: 2.2001333236694336\n",
      "49200: 2.4206933975219727\n",
      "49300: 2.3083815574645996\n",
      "49400: 2.5145516395568848\n",
      "49500: 2.408198595046997\n",
      "49600: 2.378300428390503\n",
      "49700: 2.329296350479126\n",
      "49800: 2.413914203643799\n",
      "49900: 2.250002145767212\n",
      "50000: 2.3567066192626953\n",
      "50100: 2.4040989875793457\n",
      "50200: 2.414698362350464\n",
      "50300: 2.516867160797119\n",
      "50400: 2.3495869636535645\n",
      "50500: 2.2808334827423096\n",
      "50600: 2.3229568004608154\n",
      "50700: 2.473560094833374\n",
      "50800: 2.4520113468170166\n",
      "50900: 2.5044426918029785\n",
      "51000: 2.4546403884887695\n",
      "51100: 2.403722047805786\n",
      "51200: 2.570854902267456\n",
      "51300: 2.414289712905884\n",
      "51400: 2.3471672534942627\n",
      "51500: 2.589704990386963\n",
      "51600: 2.3926596641540527\n",
      "51700: 2.407985210418701\n",
      "51800: 2.4769437313079834\n",
      "51900: 2.4528541564941406\n",
      "52000: 2.167910575866699\n",
      "52100: 2.318024158477783\n",
      "52200: 2.6012423038482666\n",
      "52300: 2.3958168029785156\n",
      "52400: 2.444664478302002\n",
      "52500: 2.473219633102417\n",
      "52600: 2.3334031105041504\n",
      "52700: 2.2936246395111084\n",
      "52800: 2.6059703826904297\n",
      "52900: 2.409539222717285\n",
      "53000: 2.611786127090454\n",
      "53100: 2.4151155948638916\n",
      "53200: 2.2489964962005615\n",
      "53300: 2.3350203037261963\n",
      "53400: 2.3425188064575195\n",
      "53500: 2.422553539276123\n",
      "53600: 2.4943687915802\n",
      "53700: 2.1569321155548096\n",
      "53800: 2.5207347869873047\n",
      "53900: 2.367645263671875\n",
      "54000: 2.436929702758789\n",
      "54100: 2.4417994022369385\n",
      "54200: 2.4341065883636475\n",
      "54300: 2.512626886367798\n",
      "54400: 2.362318277359009\n",
      "54500: 2.432166576385498\n",
      "54600: 2.376539945602417\n",
      "54700: 2.3473711013793945\n",
      "54800: 2.561701536178589\n",
      "54900: 2.2579281330108643\n",
      "55000: 2.2331643104553223\n",
      "55100: 2.364462375640869\n",
      "55200: 2.47871732711792\n",
      "55300: 2.506796360015869\n",
      "55400: 2.528958559036255\n",
      "55500: 2.2653396129608154\n",
      "55600: 2.4449703693389893\n",
      "55700: 2.5962953567504883\n",
      "55800: 2.2578115463256836\n",
      "55900: 2.2952144145965576\n",
      "56000: 2.4234514236450195\n",
      "56100: 2.2559752464294434\n",
      "56200: 2.5227482318878174\n",
      "56300: 2.398453950881958\n",
      "56400: 2.4968886375427246\n",
      "56500: 2.4433817863464355\n",
      "56600: 2.4621403217315674\n",
      "56700: 2.448843240737915\n",
      "56800: 2.494826078414917\n",
      "56900: 2.495237350463867\n",
      "57000: 2.3492166996002197\n",
      "57100: 2.4323692321777344\n",
      "57200: 2.553651809692383\n",
      "57300: 2.3652102947235107\n",
      "57400: 2.4678292274475098\n",
      "57500: 2.3153326511383057\n",
      "57600: 2.2465996742248535\n",
      "57700: 2.1494123935699463\n",
      "57800: 2.412569761276245\n",
      "57900: 2.328707695007324\n",
      "58000: 2.386932611465454\n",
      "58100: 2.4062552452087402\n",
      "58200: 2.402698278427124\n",
      "58300: 2.498720407485962\n",
      "58400: 2.378805160522461\n",
      "58500: 2.3525888919830322\n",
      "58600: 2.4009063243865967\n",
      "58700: 2.5763020515441895\n",
      "58800: 2.4601712226867676\n",
      "58900: 2.4078927040100098\n",
      "59000: 2.3954219818115234\n",
      "59100: 2.312924385070801\n",
      "59200: 2.2201273441314697\n",
      "59300: 2.3964786529541016\n",
      "59400: 2.4644694328308105\n",
      "59500: 2.5311574935913086\n",
      "59600: 2.4289863109588623\n",
      "59700: 2.347499370574951\n",
      "59800: 2.5310022830963135\n",
      "59900: 2.4454846382141113\n",
      "60000: 2.333658456802368\n",
      "60100: 2.469583511352539\n",
      "60200: 2.442992925643921\n",
      "60300: 2.2250118255615234\n",
      "60400: 2.382370710372925\n",
      "60500: 2.305257558822632\n",
      "60600: 2.318343162536621\n",
      "60700: 2.4667677879333496\n",
      "60800: 2.463555097579956\n",
      "60900: 2.454068183898926\n",
      "61000: 2.5395402908325195\n",
      "61100: 2.381591796875\n",
      "61200: 2.5224392414093018\n",
      "61300: 2.420008897781372\n",
      "61400: 2.590268850326538\n",
      "61500: 2.2438251972198486\n",
      "61600: 2.646664619445801\n",
      "61700: 2.258504867553711\n",
      "61800: 2.4519073963165283\n",
      "61900: 2.425910234451294\n",
      "62000: 2.422898769378662\n",
      "62100: 2.2009081840515137\n",
      "62200: 2.3561360836029053\n",
      "62300: 2.510706901550293\n",
      "62400: 2.3181827068328857\n",
      "62500: 2.407548666000366\n",
      "62600: 2.335453987121582\n",
      "62700: 2.5973429679870605\n",
      "62800: 2.575174570083618\n",
      "62900: 2.429370880126953\n",
      "63000: 2.2910029888153076\n",
      "63100: 2.337430477142334\n",
      "63200: 2.566427707672119\n",
      "63300: 2.429161310195923\n",
      "63400: 2.272566080093384\n",
      "63500: 2.280971050262451\n",
      "63600: 2.4135212898254395\n",
      "63700: 2.340709686279297\n",
      "63800: 2.277264356613159\n",
      "63900: 2.2669928073883057\n",
      "64000: 2.5325422286987305\n",
      "64100: 2.5027921199798584\n",
      "64200: 2.583735227584839\n",
      "64300: 2.2582132816314697\n",
      "64400: 2.4340667724609375\n",
      "64500: 2.4045779705047607\n",
      "64600: 2.6273996829986572\n",
      "64700: 2.266315221786499\n",
      "64800: 2.3561065196990967\n",
      "64900: 2.122875928878784\n",
      "65000: 2.512286901473999\n",
      "65100: 2.532153844833374\n",
      "65200: 2.4755537509918213\n",
      "65300: 2.450740098953247\n",
      "65400: 2.153648614883423\n",
      "65500: 2.3084335327148438\n",
      "65600: 2.4612810611724854\n",
      "65700: 2.3095760345458984\n",
      "65800: 2.335254669189453\n",
      "65900: 2.604294776916504\n",
      "66000: 2.2832603454589844\n",
      "66100: 2.5964508056640625\n",
      "66200: 2.460812568664551\n",
      "66300: 2.43693470954895\n",
      "66400: 2.4013102054595947\n",
      "66500: 2.504879951477051\n",
      "66600: 2.4054629802703857\n",
      "66700: 2.3888840675354004\n",
      "66800: 2.4236462116241455\n",
      "66900: 2.409945487976074\n",
      "67000: 2.3703713417053223\n",
      "67100: 2.492443084716797\n",
      "67200: 2.4470391273498535\n",
      "67300: 2.4287490844726562\n",
      "67400: 2.487977981567383\n",
      "67500: 2.405266046524048\n",
      "67600: 2.5751562118530273\n",
      "67700: 2.4051525592803955\n",
      "67800: 2.3398847579956055\n",
      "67900: 2.523336410522461\n",
      "68000: 2.4398715496063232\n",
      "68100: 2.510470390319824\n",
      "68200: 2.3932294845581055\n",
      "68300: 2.4014194011688232\n",
      "68400: 2.4575071334838867\n",
      "68500: 2.2094390392303467\n",
      "68600: 2.311840534210205\n",
      "68700: 2.4226317405700684\n",
      "68800: 2.4317128658294678\n",
      "68900: 2.404498338699341\n",
      "69000: 2.3920140266418457\n",
      "69100: 2.3871920108795166\n",
      "69200: 2.3518075942993164\n",
      "69300: 2.421306610107422\n",
      "69400: 2.4364445209503174\n",
      "69500: 2.263744354248047\n",
      "69600: 2.4299912452697754\n",
      "69700: 2.6366026401519775\n",
      "69800: 2.355480670928955\n",
      "69900: 2.573556423187256\n",
      "70000: 2.3247201442718506\n",
      "70100: 2.488035202026367\n",
      "70200: 2.251019239425659\n",
      "70300: 2.3116536140441895\n",
      "70400: 2.2596380710601807\n",
      "70500: 2.4141197204589844\n",
      "70600: 2.2953591346740723\n",
      "70700: 2.450495719909668\n",
      "70800: 2.2312426567077637\n",
      "70900: 2.438838243484497\n",
      "71000: 2.4370293617248535\n",
      "71100: 2.3961737155914307\n",
      "71200: 2.3298726081848145\n",
      "71300: 2.3696391582489014\n",
      "71400: 2.3266308307647705\n",
      "71500: 2.456158399581909\n",
      "71600: 2.341728448867798\n",
      "71700: 2.324775457382202\n",
      "71800: 2.5245556831359863\n",
      "71900: 2.3046905994415283\n",
      "72000: 2.4781816005706787\n",
      "72100: 2.5698442459106445\n",
      "72200: 2.241658926010132\n",
      "72300: 2.6122493743896484\n",
      "72400: 2.3496110439300537\n",
      "72500: 2.4219603538513184\n",
      "72600: 2.3004798889160156\n",
      "72700: 2.412687301635742\n",
      "72800: 2.5433428287506104\n",
      "72900: 2.4003589153289795\n",
      "73000: 2.4495272636413574\n",
      "73100: 2.2882418632507324\n",
      "73200: 2.618865966796875\n",
      "73300: 2.446967124938965\n",
      "73400: 2.4168460369110107\n",
      "73500: 2.4741313457489014\n",
      "73600: 2.484335422515869\n",
      "73700: 2.4992008209228516\n",
      "73800: 2.422008991241455\n",
      "73900: 2.36741304397583\n",
      "74000: 2.2930305004119873\n",
      "74100: 2.4626667499542236\n",
      "74200: 2.351123332977295\n",
      "74300: 2.397738456726074\n",
      "74400: 2.431213855743408\n",
      "74500: 2.4058680534362793\n",
      "74600: 2.4840328693389893\n",
      "74700: 2.4994518756866455\n",
      "74800: 2.2609894275665283\n",
      "74900: 2.5322718620300293\n",
      "75000: 2.5806167125701904\n",
      "75100: 2.5199971199035645\n",
      "75200: 2.334590435028076\n",
      "75300: 2.3175830841064453\n",
      "75400: 2.351206064224243\n",
      "75500: 2.3859972953796387\n",
      "75600: 2.4853920936584473\n",
      "75700: 2.4574899673461914\n",
      "75800: 2.274282932281494\n",
      "75900: 2.464386224746704\n",
      "76000: 2.455247163772583\n",
      "76100: 2.5008535385131836\n",
      "76200: 2.3052990436553955\n",
      "76300: 2.3706912994384766\n",
      "76400: 2.62113356590271\n",
      "76500: 2.334475517272949\n",
      "76600: 2.2788965702056885\n",
      "76700: 2.550661325454712\n",
      "76800: 2.2981550693511963\n",
      "76900: 2.4266741275787354\n",
      "77000: 2.4872078895568848\n",
      "77100: 2.7844064235687256\n",
      "77200: 2.386490821838379\n",
      "77300: 2.6299943923950195\n",
      "77400: 2.3087596893310547\n",
      "77500: 2.556582450866699\n",
      "77600: 2.3703830242156982\n",
      "77700: 2.397742509841919\n",
      "77800: 2.2301604747772217\n",
      "77900: 2.2943685054779053\n",
      "78000: 2.3753089904785156\n",
      "78100: 2.381284475326538\n",
      "78200: 2.4478039741516113\n",
      "78300: 2.4680750370025635\n",
      "78400: 2.3720285892486572\n",
      "78500: 2.5070431232452393\n",
      "78600: 2.4316518306732178\n",
      "78700: 2.460333824157715\n",
      "78800: 2.412630558013916\n",
      "78900: 2.4512593746185303\n",
      "79000: 2.5245180130004883\n",
      "79100: 2.533201217651367\n",
      "79200: 2.659594774246216\n",
      "79300: 2.464662551879883\n",
      "79400: 2.310286045074463\n",
      "79500: 2.51088809967041\n",
      "79600: 2.3283445835113525\n",
      "79700: 2.2288479804992676\n",
      "79800: 2.4716360569000244\n",
      "79900: 2.3733627796173096\n",
      "80000: 2.51275372505188\n",
      "80100: 2.341794729232788\n",
      "80200: 2.4741828441619873\n",
      "80300: 2.271749973297119\n",
      "80400: 2.51137113571167\n",
      "80500: 2.608262777328491\n",
      "80600: 2.469729423522949\n",
      "80700: 2.458303451538086\n",
      "80800: 2.5198142528533936\n",
      "80900: 2.608109474182129\n",
      "81000: 2.4359312057495117\n",
      "81100: 2.5244317054748535\n",
      "81200: 2.558320999145508\n",
      "81300: 2.404100179672241\n",
      "81400: 2.587827682495117\n",
      "81500: 2.3218791484832764\n",
      "81600: 2.3785948753356934\n",
      "81700: 2.434776782989502\n",
      "81800: 2.415173292160034\n",
      "81900: 2.389636993408203\n",
      "82000: 2.764425277709961\n",
      "82100: 2.385568380355835\n",
      "82200: 2.350466251373291\n",
      "82300: 2.4783077239990234\n",
      "82400: 2.4694981575012207\n",
      "82500: 2.2518248558044434\n",
      "82600: 2.5029735565185547\n",
      "82700: 2.394374370574951\n",
      "82800: 2.6130166053771973\n",
      "82900: 2.3632853031158447\n",
      "83000: 2.5837533473968506\n",
      "83100: 2.4719631671905518\n",
      "83200: 2.4413228034973145\n",
      "83300: 2.6117892265319824\n",
      "83400: 2.42299222946167\n",
      "83500: 2.6457467079162598\n",
      "83600: 2.53033185005188\n",
      "83700: 2.4621822834014893\n",
      "83800: 2.578364133834839\n",
      "83900: 2.496926784515381\n",
      "84000: 2.429077625274658\n",
      "84100: 2.4171323776245117\n",
      "84200: 2.454042911529541\n",
      "84300: 2.539776563644409\n",
      "84400: 2.5204033851623535\n",
      "84500: 2.399137020111084\n",
      "84600: 2.4717838764190674\n",
      "84700: 2.3923561573028564\n",
      "84800: 2.378159761428833\n",
      "84900: 2.369565725326538\n",
      "85000: 2.3954215049743652\n",
      "85100: 2.666863441467285\n",
      "85200: 2.283590078353882\n",
      "85300: 2.3162436485290527\n",
      "85400: 2.1778645515441895\n",
      "85500: 2.19913911819458\n",
      "85600: 2.367928981781006\n",
      "85700: 2.4618563652038574\n",
      "85800: 2.370649814605713\n",
      "85900: 2.5343000888824463\n",
      "86000: 2.3897674083709717\n",
      "86100: 2.4204628467559814\n",
      "86200: 2.1811625957489014\n",
      "86300: 2.5239548683166504\n",
      "86400: 2.4019510746002197\n",
      "86500: 2.439936637878418\n",
      "86600: 2.5196218490600586\n",
      "86700: 2.3748369216918945\n",
      "86800: 2.314068555831909\n",
      "86900: 2.688358783721924\n",
      "87000: 2.519270896911621\n",
      "87100: 2.598600149154663\n",
      "87200: 2.365888833999634\n",
      "87300: 2.3244411945343018\n",
      "87400: 2.400998830795288\n",
      "87500: 2.338009834289551\n",
      "87600: 2.5680794715881348\n",
      "87700: 2.3333263397216797\n",
      "87800: 2.479160785675049\n",
      "87900: 2.3432934284210205\n",
      "88000: 2.296025276184082\n",
      "88100: 2.5427327156066895\n",
      "88200: 2.294053792953491\n",
      "88300: 2.340139389038086\n",
      "88400: 2.4751617908477783\n",
      "88500: 2.4008307456970215\n",
      "88600: 2.3184444904327393\n",
      "88700: 2.411012887954712\n",
      "88800: 2.5073180198669434\n",
      "88900: 2.4112396240234375\n",
      "89000: 2.374213218688965\n",
      "89100: 2.4447197914123535\n",
      "89200: 2.593670606613159\n",
      "89300: 2.519239902496338\n",
      "89400: 2.370997667312622\n",
      "89500: 2.311296224594116\n",
      "89600: 2.3574469089508057\n",
      "89700: 2.4879846572875977\n",
      "89800: 2.5367398262023926\n",
      "89900: 2.362285852432251\n",
      "90000: 2.298996925354004\n",
      "90100: 2.387925148010254\n",
      "90200: 2.415884017944336\n",
      "90300: 2.355794668197632\n",
      "90400: 2.3950514793395996\n",
      "90500: 2.473670721054077\n",
      "90600: 2.7004756927490234\n",
      "90700: 2.344583034515381\n",
      "90800: 2.5544698238372803\n",
      "90900: 2.43221116065979\n",
      "91000: 2.4969611167907715\n",
      "91100: 2.6237893104553223\n",
      "91200: 2.3828539848327637\n",
      "91300: 2.4402053356170654\n",
      "91400: 2.5577285289764404\n",
      "91500: 2.519817352294922\n",
      "91600: 2.473928451538086\n",
      "91700: 2.4052674770355225\n",
      "91800: 2.4180121421813965\n",
      "91900: 2.384523391723633\n",
      "92000: 2.35394024848938\n",
      "92100: 2.5169711112976074\n",
      "92200: 2.5250818729400635\n",
      "92300: 2.488724708557129\n",
      "92400: 2.4693524837493896\n",
      "92500: 2.2793776988983154\n",
      "92600: 2.3359687328338623\n",
      "92700: 2.4661927223205566\n",
      "92800: 2.470104932785034\n",
      "92900: 2.4166982173919678\n",
      "93000: 2.4422945976257324\n",
      "93100: 2.3768558502197266\n",
      "93200: 2.339550018310547\n",
      "93300: 2.3246772289276123\n",
      "93400: 2.6732077598571777\n",
      "93500: 2.705176591873169\n",
      "93600: 2.4804575443267822\n",
      "93700: 2.3138980865478516\n",
      "93800: 2.4063432216644287\n",
      "93900: 2.461348056793213\n",
      "94000: 2.5098061561584473\n",
      "94100: 2.2475907802581787\n",
      "94200: 2.2547833919525146\n",
      "94300: 2.3790037631988525\n",
      "94400: 2.512766122817993\n",
      "94500: 2.45410418510437\n",
      "94600: 2.5448713302612305\n",
      "94700: 2.2599287033081055\n",
      "94800: 2.3818416595458984\n",
      "94900: 2.4965462684631348\n",
      "95000: 2.4303269386291504\n",
      "95100: 2.5272023677825928\n",
      "95200: 2.3594682216644287\n",
      "95300: 2.6024038791656494\n",
      "95400: 2.4049036502838135\n",
      "95500: 2.5043060779571533\n",
      "95600: 2.341104507446289\n",
      "95700: 2.447816848754883\n",
      "95800: 2.3542110919952393\n",
      "95900: 2.5145087242126465\n",
      "96000: 2.4847989082336426\n",
      "96100: 2.1616644859313965\n",
      "96200: 2.4897243976593018\n",
      "96300: 2.445293426513672\n",
      "96400: 2.3269946575164795\n",
      "96500: 2.429677963256836\n",
      "96600: 2.3932244777679443\n",
      "96700: 2.4416873455047607\n",
      "96800: 2.5720772743225098\n",
      "96900: 2.416494846343994\n",
      "97000: 2.3260703086853027\n",
      "97100: 2.549197196960449\n",
      "97200: 2.380781650543213\n",
      "97300: 2.566049814224243\n",
      "97400: 2.5171689987182617\n",
      "97500: 2.4368531703948975\n",
      "97600: 2.468385696411133\n",
      "97700: 2.481788158416748\n",
      "97800: 2.4294967651367188\n",
      "97900: 2.312314510345459\n",
      "98000: 2.5251219272613525\n",
      "98100: 2.3735313415527344\n",
      "98200: 2.283643960952759\n",
      "98300: 2.432697057723999\n",
      "98400: 2.4517362117767334\n",
      "98500: 2.305330753326416\n",
      "98600: 2.3521759510040283\n",
      "98700: 2.408080577850342\n",
      "98800: 2.4636125564575195\n",
      "98900: 2.3705477714538574\n",
      "99000: 2.659341335296631\n",
      "99100: 2.4005351066589355\n",
      "99200: 2.1436405181884766\n",
      "99300: 2.5653204917907715\n",
      "99400: 2.4498746395111084\n",
      "99500: 2.4249095916748047\n",
      "99600: 2.3020637035369873\n",
      "99700: 2.493849754333496\n",
      "99800: 2.2132503986358643\n",
      "99900: 2.3218166828155518\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (128,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    #lr = lrs[i]\n",
    "    lr = 0.001\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b23661b0-48cf-4f65-81b3-30fe71f367bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4194, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 64) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf453d-835c-4d19-9910-3fcd59701d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# iter 12 -- i seemed to be having better results when I was using less context and embeddings... let's try that again\n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2d95a58e-90ac-472c-80c6-8166d622efa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182691, 3]) torch.Size([182691])\n",
      "torch.Size([22793, 3]) torch.Size([22793])\n",
      "torch.Size([22662, 3]) torch.Size([22662])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "               # original paper uses a cotext of 3 words\n",
    "               # original paper uses a cotext of 3 words\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop first character and append, rolling window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2ebb5892-a650-4279-ad58-cd744fb0f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1) # for reproducibility\n",
    "C = torch.randn((27, 3), generator=g)\n",
    "# hidden layer\n",
    "W1 = torch.randn((9, 300), generator=g) # weights\n",
    "b1 = torch.randn(300, generator=g) # biases\n",
    "# output layer (right?)\n",
    "W2 = torch.randn((300, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cd77809e-8b5e-4d6c-90a1-982e429030ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11208"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "99528403-178e-43d7-93da-0a894352bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.3351354598999023\n",
      "500: 2.3748114109039307\n",
      "1000: 2.2989559173583984\n",
      "1500: 2.2898166179656982\n",
      "2000: 2.4200313091278076\n",
      "2500: 2.2524056434631348\n",
      "3000: 2.406329393386841\n",
      "3500: 2.2602272033691406\n",
      "4000: 2.2859275341033936\n",
      "4500: 2.2646021842956543\n",
      "5000: 2.306941509246826\n",
      "5500: 2.3091988563537598\n",
      "6000: 2.27148699760437\n",
      "6500: 2.273379325866699\n",
      "7000: 2.3045904636383057\n",
      "7500: 2.2338345050811768\n",
      "8000: 2.3146235942840576\n",
      "8500: 2.3298723697662354\n",
      "9000: 2.3640639781951904\n",
      "9500: 2.31756329536438\n",
      "10000: 2.3269405364990234\n",
      "10500: 2.2810139656066895\n",
      "11000: 2.304314374923706\n",
      "11500: 2.3256990909576416\n",
      "12000: 2.2709662914276123\n",
      "12500: 2.359128713607788\n",
      "13000: 2.371206045150757\n",
      "13500: 2.3234236240386963\n",
      "14000: 2.331359386444092\n",
      "14500: 2.3027188777923584\n",
      "15000: 2.3617441654205322\n",
      "15500: 2.3140342235565186\n",
      "16000: 2.336979389190674\n",
      "16500: 2.3278074264526367\n",
      "17000: 2.320267915725708\n",
      "17500: 2.2733700275421143\n",
      "18000: 2.339160680770874\n",
      "18500: 2.3235976696014404\n",
      "19000: 2.331841468811035\n",
      "19500: 2.2719674110412598\n",
      "20000: 2.2891197204589844\n",
      "20500: 2.3111329078674316\n",
      "21000: 2.302053213119507\n",
      "21500: 2.376530170440674\n",
      "22000: 2.272529363632202\n",
      "22500: 2.3034846782684326\n",
      "23000: 2.3534650802612305\n",
      "23500: 2.3047444820404053\n",
      "24000: 2.321072578430176\n",
      "24500: 2.249457597732544\n",
      "25000: 2.2698757648468018\n",
      "25500: 2.3506038188934326\n",
      "26000: 2.2360806465148926\n",
      "26500: 2.3091742992401123\n",
      "27000: 2.3638522624969482\n",
      "27500: 2.3075222969055176\n",
      "28000: 2.328728437423706\n",
      "28500: 2.3045449256896973\n",
      "29000: 2.341449737548828\n",
      "29500: 2.266369104385376\n",
      "30000: 2.3235671520233154\n",
      "30500: 2.316359043121338\n",
      "31000: 2.3351426124572754\n",
      "31500: 2.2832484245300293\n",
      "32000: 2.2768290042877197\n",
      "32500: 2.355135679244995\n",
      "33000: 2.2926042079925537\n",
      "33500: 2.2975785732269287\n",
      "34000: 2.376671314239502\n",
      "34500: 2.2346956729888916\n",
      "35000: 2.324726104736328\n",
      "35500: 2.317869186401367\n",
      "36000: 2.313258171081543\n",
      "36500: 2.4281625747680664\n",
      "37000: 2.357701301574707\n",
      "37500: 2.3266189098358154\n",
      "38000: 2.33908748626709\n",
      "38500: 2.2937510013580322\n",
      "39000: 2.375105142593384\n",
      "39500: 2.2620480060577393\n",
      "40000: 2.329179525375366\n",
      "40500: 2.32369065284729\n",
      "41000: 2.3513998985290527\n",
      "41500: 2.3089072704315186\n",
      "42000: 2.330801486968994\n",
      "42500: 2.350036144256592\n",
      "43000: 2.2983736991882324\n",
      "43500: 2.3133468627929688\n",
      "44000: 2.3241195678710938\n",
      "44500: 2.255892038345337\n",
      "45000: 2.3016228675842285\n",
      "45500: 2.3233585357666016\n",
      "46000: 2.325551748275757\n",
      "46500: 2.372206211090088\n",
      "47000: 2.310167074203491\n",
      "47500: 2.3037338256835938\n",
      "48000: 2.346653699874878\n",
      "48500: 2.3027830123901367\n",
      "49000: 2.2997922897338867\n",
      "49500: 2.3298306465148926\n",
      "50000: 2.2824337482452393\n",
      "50500: 2.3164963722229004\n",
      "51000: 2.274167060852051\n",
      "51500: 2.235097885131836\n",
      "52000: 2.2491164207458496\n",
      "52500: 2.337482213973999\n",
      "53000: 2.3301804065704346\n",
      "53500: 2.303328037261963\n",
      "54000: 2.3443684577941895\n",
      "54500: 2.3162362575531006\n",
      "55000: 2.309004545211792\n",
      "55500: 2.3196122646331787\n",
      "56000: 2.312800884246826\n",
      "56500: 2.3176426887512207\n",
      "57000: 2.2973828315734863\n",
      "57500: 2.3375535011291504\n",
      "58000: 2.3440144062042236\n",
      "58500: 2.331935167312622\n",
      "59000: 2.337554454803467\n",
      "59500: 2.301692008972168\n",
      "60000: 2.33099627494812\n",
      "60500: 2.3650741577148438\n",
      "61000: 2.3412766456604004\n",
      "61500: 2.298276662826538\n",
      "62000: 2.3145241737365723\n",
      "62500: 2.3587796688079834\n",
      "63000: 2.296271562576294\n",
      "63500: 2.296096086502075\n",
      "64000: 2.298527479171753\n",
      "64500: 2.3301241397857666\n",
      "65000: 2.204556941986084\n",
      "65500: 2.336242437362671\n",
      "66000: 2.3124890327453613\n",
      "66500: 2.3110125064849854\n",
      "67000: 2.3044755458831787\n",
      "67500: 2.351148843765259\n",
      "68000: 2.2929575443267822\n",
      "68500: 2.3332273960113525\n",
      "69000: 2.3241515159606934\n",
      "69500: 2.345379114151001\n",
      "70000: 2.3261966705322266\n",
      "70500: 2.325871229171753\n",
      "71000: 2.30802845954895\n",
      "71500: 2.3066012859344482\n",
      "72000: 2.3219664096832275\n",
      "72500: 2.367448329925537\n",
      "73000: 2.3115181922912598\n",
      "73500: 2.34407377243042\n",
      "74000: 2.3571369647979736\n",
      "74500: 2.294437885284424\n",
      "75000: 2.3544716835021973\n",
      "75500: 2.2820308208465576\n",
      "76000: 2.3100292682647705\n",
      "76500: 2.3548378944396973\n",
      "77000: 2.2988922595977783\n",
      "77500: 2.329601764678955\n",
      "78000: 2.4036288261413574\n",
      "78500: 2.391524314880371\n",
      "79000: 2.2414724826812744\n",
      "79500: 2.3549327850341797\n",
      "80000: 2.245620012283325\n",
      "80500: 2.3561413288116455\n",
      "81000: 2.3128507137298584\n",
      "81500: 2.27864146232605\n",
      "82000: 2.3025312423706055\n",
      "82500: 2.328601121902466\n",
      "83000: 2.349747896194458\n",
      "83500: 2.356732130050659\n",
      "84000: 2.352938175201416\n",
      "84500: 2.305445909500122\n",
      "85000: 2.326282501220703\n",
      "85500: 2.271533966064453\n",
      "86000: 2.325575113296509\n",
      "86500: 2.330580711364746\n",
      "87000: 2.2628238201141357\n",
      "87500: 2.2988088130950928\n",
      "88000: 2.3310153484344482\n",
      "88500: 2.3116040229797363\n",
      "89000: 2.3161985874176025\n",
      "89500: 2.2775402069091797\n",
      "90000: 2.336449384689331\n",
      "90500: 2.370560646057129\n",
      "91000: 2.3044421672821045\n",
      "91500: 2.3240113258361816\n",
      "92000: 2.348811149597168\n",
      "92500: 2.3041563034057617\n",
      "93000: 2.347486972808838\n",
      "93500: 2.347320795059204\n",
      "94000: 2.3396871089935303\n",
      "94500: 2.281559705734253\n",
      "95000: 2.2911860942840576\n",
      "95500: 2.3047282695770264\n",
      "96000: 2.3452370166778564\n",
      "96500: 2.304847002029419\n",
      "97000: 2.363255023956299\n",
      "97500: 2.2793188095092773\n",
      "98000: 2.3816592693328857\n",
      "98500: 2.306594133377075\n",
      "99000: 2.2483747005462646\n",
      "99500: 2.246438503265381\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):   \n",
    "    \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (1000,))\n",
    "\n",
    "    # forward pass    \n",
    "    emb = C[Xtr[ix]]\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 9) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # ALSO INDEXING INTO BATCH HERE\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(f'{i}: {loss.item()}')\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    #lr = lrs[i]\n",
    "    lr = 0.001\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    #lri.append(lre[i])\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7c91354e-884d-494f-b642-8915f57a3f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3239, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]    \n",
    "h = torch.tanh(emb.view(-1, 9) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "007ec36d-2c38-45b2-a855-eb715b10cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mria.\n",
      "mayah.\n",
      "see.\n",
      "med.\n",
      "ryah.\n",
      "rethra.\n",
      "ejdrne.\n",
      "caderedielin.\n",
      "shi.\n",
      "jen.\n",
      "ede.\n",
      "sostanar.\n",
      "kayzioh.\n",
      "karin.\n",
      "shubergihimiel.\n",
      "kin.\n",
      "renlynnnn.\n",
      "pacfoubamyed.\n",
      "riyah.\n",
      "faeh.\n",
      "ylah.\n",
      "myskoyah.\n",
      "halnan.\n",
      "yansun.\n",
      "zakelven.\n",
      "ely.\n",
      "revis.\n",
      "jaonn.\n",
      "pordin.\n",
      "kyko.\n",
      "obhizptila.\n",
      "amueygh.\n",
      "huraon.\n",
      "ialairi.\n",
      "evparwella.\n",
      "ortarashten.\n",
      "goisa.\n",
      "alin.\n",
      "fexvissin.\n",
      "damelirketann.\n",
      "ylxan.\n",
      "tud.\n",
      "jactenic.\n",
      "atin.\n",
      "aud.\n",
      "aive.\n",
      "dihavirley.\n",
      "jayepaneniagrylina.\n",
      "iimisg.\n",
      "onret.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2 ** 31 - 1 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6ba1-f78c-48d5-b9d0-6db1f9d9bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
